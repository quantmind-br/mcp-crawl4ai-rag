# ===============================
# MCP SERVER CONFIGURATION
# ===============================
TRANSPORT=sse
HOST=0.0.0.0
PORT=8051

# ===============================
# AI MODELS
# ===============================

# Chat Model (for summaries, contextual embeddings)
# Examples: gpt-4o-mini, gpt-3.5-turbo, claude-3-haiku
CHAT_MODEL=gpt-4o-mini
CHAT_API_KEY=
CHAT_API_BASE=

# Embeddings Model (for vector search)
# 
# SUPPORTED PROVIDERS:
# 
# OpenAI (default):
# - text-embedding-3-small (1536 dims, $0.02/1M tokens)
# - text-embedding-3-large (3072 dims, $0.13/1M tokens)
# - text-embedding-ada-002 (1536 dims, $0.10/1M tokens)
#
# DeepInfra (cost-effective alternative):
# - Qwen/Qwen3-Embedding-0.6B (1024 dims, ~$0.01/1M tokens)
# - BAAI/bge-large-en-v1.5 (1024 dims)
# - BAAI/bge-small-en-v1.5 (384 dims)
# - sentence-transformers/all-MiniLM-L6-v2 (384 dims)
#
# RECOMMENDED: Qwen/Qwen3-Embedding-0.6B for best price/performance
EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDINGS_API_KEY=
EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai

# Embedding dimensions (auto-detected from model if not specified)
# 
# AUTO-DETECTION SUPPORTED MODELS:
# - text-embedding-3-small: 1536
# - text-embedding-3-large: 3072
# - Qwen/Qwen3-Embedding-0.6B: 1024
# - BAAI/bge-large-en-v1.5: 1024
# - BAAI/bge-small-en-v1.5: 384
# - sentence-transformers/all-MiniLM-L6-v2: 384
#
# MANUAL OVERRIDE: Set to specific value if needed
# WARNING: Wrong dimensions will trigger automatic collection recreation!
EMBEDDINGS_DIMENSIONS=

# ===============================
# CONFIGURATION EXAMPLES
# ===============================
#
# Example 1: DeepInfra Qwen3 Setup (RECOMMENDED - Cost Effective)
# EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
# EMBEDDINGS_API_KEY=your-deepinfra-api-key
# EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai
# EMBEDDINGS_DIMENSIONS=  # Auto-detected as 1024
#
# Example 2: OpenAI Setup (Higher cost, proven reliability)
# EMBEDDINGS_MODEL=text-embedding-3-small
# EMBEDDINGS_API_KEY=sk-proj-your-openai-key
# EMBEDDINGS_API_BASE=  # Uses OpenAI default
# EMBEDDINGS_DIMENSIONS=  # Auto-detected as 1536
#
# Example 3: Mixed Provider Setup (DeepInfra primary, OpenAI fallback)
# EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
# EMBEDDINGS_API_KEY=your-deepinfra-key
# EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai
# EMBEDDINGS_FALLBACK_MODEL=text-embedding-3-small
# EMBEDDINGS_FALLBACK_API_KEY=sk-proj-your-openai-key
# EMBEDDINGS_FALLBACK_API_BASE=https://api.openai.com/v1
#
# Example 4: Custom Dimensions Override
# EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
# EMBEDDINGS_API_KEY=your-deepinfra-key
# EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai
# EMBEDDINGS_DIMENSIONS=1024  # Explicit override (normally auto-detected)

# Fallback Models
CHAT_FALLBACK_MODEL=gpt-4o-mini
EMBEDDINGS_FALLBACK_MODEL=text-embedding-3-small

# Fallback API Configuration (optional - inherits from primary if not set)
# 
# PURPOSE: Enable true API provider failover for resilience and flexibility
# 
# INHERITANCE BEHAVIOR:
# - If CHAT_FALLBACK_API_KEY is not set, inherits CHAT_API_KEY
# - If CHAT_FALLBACK_API_BASE is not set, inherits CHAT_API_BASE
# - Same inheritance pattern applies to EMBEDDINGS_FALLBACK_* variables
#
# USE CASES:
# 1. Resilience: Primary API down, automatic failover to different provider
# 2. Cost optimization: Premium primary provider, cheaper fallback provider
# 3. Rate limiting: Fallback to unrestricted provider when primary is rate limited
# 4. Regional compliance: Different providers for different geographical requirements
#
# CONFIGURATION EXAMPLES:
#
# Example 1: Mixed providers (Primary: OpenRouter, Fallback: OpenAI)
# CHAT_API_KEY=sk-or-v1-your-openrouter-key
# CHAT_API_BASE=https://openrouter.ai/api/v1
# CHAT_FALLBACK_API_KEY=sk-proj-your-openai-key
# CHAT_FALLBACK_API_BASE=https://api.openai.com/v1
#
# Example 2: Inheritance (Fallback uses same provider with inherited config)
# CHAT_API_KEY=your-primary-key
# CHAT_API_BASE=https://api.yourprovider.com/v1
# CHAT_FALLBACK_MODEL=gpt-3.5-turbo  # Only model differs, API config inherited
#
# Example 3: Azure primary, OpenAI fallback
# CHAT_API_KEY=your-azure-key
# CHAT_API_BASE=https://your-resource.openai.azure.com/
# CHAT_FALLBACK_API_KEY=sk-proj-your-openai-key
# CHAT_FALLBACK_API_BASE=https://api.openai.com/v1
#
# TROUBLESHOOTING:
# - If both primary and fallback fail: Check API keys and network connectivity
# - Inheritance not working: Verify primary configuration is set correctly
# - Wrong provider used: Check that fallback base URL is explicitly set for different providers
#
# Optional: Set only if you want different API configuration for fallback
CHAT_FALLBACK_API_KEY=
CHAT_FALLBACK_API_BASE=
EMBEDDINGS_FALLBACK_API_KEY=
EMBEDDINGS_FALLBACK_API_BASE=

# ===============================
# REDIS CACHE CONFIGURATION
# ===============================

# Redis Connection
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_USERNAME=
REDIS_SSL=false

# Redis Performance Settings
REDIS_CONNECTION_TIMEOUT=5
REDIS_SOCKET_TIMEOUT=5
REDIS_MAX_CONNECTIONS=20
REDIS_HEALTH_CHECK_INTERVAL=30

# Cache Behavior
USE_REDIS_CACHE=false
REDIS_EMBEDDING_TTL=86400
REDIS_CIRCUIT_BREAKER_FAILURES=5
REDIS_CIRCUIT_BREAKER_TIMEOUT=60

# ===============================
# RAG FEATURES
# ===============================

# Contextual Embeddings: Enhance chunks with document context for better accuracy
# Performance impact: +30% token usage, +15-25% accuracy
USE_CONTEXTUAL_EMBEDDINGS=false

# Hybrid Search: Combine semantic (dense) + keyword (sparse) search using FastBM25
# NEW: Native Qdrant implementation with Reciprocal Rank Fusion (RRF)
# Performance: <200ms search (vs >500ms client-side), +20-40% accuracy
# Requires: fastembed>=0.4.0 (auto-installed), collection migration
USE_HYBRID_SEARCH=false

# Automatic Collection Migration for Hybrid Search
# IMPORTANT: Enables automatic migration from legacy to hybrid schema
# WARNING: Migration will DELETE existing data (backup created automatically)
# Safety: Disabled by default, set to 'true' to enable automatic migration
AUTO_MIGRATE_COLLECTIONS=false

# Agentic RAG: Extract and index code examples separately
# Use for programming/technical documentation with dedicated code search
USE_AGENTIC_RAG=false

# Reranking: Re-order search results using cross-encoder models
# Performance: +50-100ms per query, +10-15% accuracy
USE_RERANKING=false

# Knowledge Graph: Neo4j integration for AI hallucination detection
# Advanced feature for code analysis and validation
USE_KNOWLEDGE_GRAPH=false

# ===============================
# HYBRID SEARCH CONFIGURATION
# ===============================
# Advanced settings for hybrid search (requires USE_HYBRID_SEARCH=true)

# Sparse Vector (BM25) Configuration
# FastBM25 encoder settings via fastembed library
# These are automatically optimized but can be customized if needed

# RRF (Reciprocal Rank Fusion) Parameters
# Controls how dense and sparse search results are combined
# RRF_K: Higher values reduce rank position impact (default: 60)
# DENSE_WEIGHT: Weight for semantic search (0.0-1.0, default: 0.5)
# Example: 0.7 = 70% semantic, 30% keyword search
# RRF_K=60
# DENSE_WEIGHT=0.5

# Collection Migration Settings
# BACKUP_RETENTION_DAYS: How long to keep collection backups (default: 7)
# MAX_MIGRATION_RETRIES: Number of retry attempts for failed migrations (default: 3)
# MIGRATION_BATCH_SIZE: Batch size for data migration (default: 1000)
# BACKUP_RETENTION_DAYS=7
# MAX_MIGRATION_RETRIES=3
# MIGRATION_BATCH_SIZE=1000

# ===============================
# RERANKING CONFIGURATION
# ===============================
# Enhanced reranking settings (requires USE_RERANKING=true)

# CrossEncoder model selection
# Popular models for different use cases:
# - cross-encoder/ms-marco-MiniLM-L-6-v2: Balanced performance and speed (default)
# - cross-encoder/ms-marco-MiniLM-L-12-v2: Higher accuracy, slower
# - cross-encoder/ms-marco-TinyBERT-L-2-v2: Fastest, lower accuracy
# - cross-encoder/stsb-distilroberta-base: For semantic similarity tasks
RERANKING_MODEL_NAME=cross-encoder/ms-marco-MiniLM-L-6-v2

# Model warming configuration
# Number of dummy predictions during startup to improve first request latency
# Higher values = better warmup, slower startup. Set to 0 to disable.
RERANKING_WARMUP_SAMPLES=5

# ===============================
# GPU ACCELERATION
# ===============================
# Options: auto|cuda|mps|cpu
USE_GPU_ACCELERATION=auto
# Options: float32|float16|bfloat16
GPU_PRECISION=float32
GPU_DEVICE_INDEX=0
GPU_MEMORY_FRACTION=0.8

# ===============================
# DATABASES
# ===============================

# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Neo4j Knowledge Graph
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123

# ===============================
# HTTP CLIENT CONFIGURATION
# ===============================
# ConnectionResetError fix settings for Windows stability
# These are automatically applied but can be overridden here

# Enable HTTP/2 for better performance (true/false)
HTTPX_HTTP2=true

# Maximum total HTTP connections for crawling operations
# Higher values = faster crawling, more resource usage
# Default: 200 (optimized for performance)
HTTPCORE_MAX_CONNECTIONS=200

# Maximum keepalive connections to reuse
# Higher values = better connection reuse, less overhead
# Default: 50 (balanced performance)
HTTPCORE_MAX_KEEPALIVE_CONNECTIONS=50

# Connection keepalive expiry time in seconds
# Longer values = better connection reuse, more memory usage
# Default: 30.0 (optimized for stability)
HTTPCORE_KEEPALIVE_EXPIRY=30.0