# The transport for the MCP server - either 'sse' or 'stdio' (defaults to sse if left empty)
TRANSPORT=

# Host to bind to if using sse as the transport (leave empty if using stdio)
# Set this to 0.0.0.0 if using Docker, otherwise set to localhost (if using uv)
HOST=

# Port to listen on if using sse as the transport (leave empty if using stdio)
PORT=

# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# This is for backward compatibility - new deployments should use CHAT_API_KEY and EMBEDDINGS_API_KEY
OPENAI_API_KEY=

# === CONFIGURAÇÃO DO MODELO CHAT ===
# Configure o LLM usado para resumos, embeddings contextuais e análise de código
# Suporta qualquer API compatível com OpenAI (OpenAI, Azure OpenAI, LocalAI, Ollama, etc.)

# O modelo de chat para usar em resumos e embeddings contextuais
# Exemplos: gpt-4o-mini, gpt-3.5-turbo, claude-3-haiku, llama3-8b-instruct
CHAT_MODEL=

# Chave da API para o serviço do modelo de chat
# Usa OPENAI_API_KEY como fallback se não especificado (compatibilidade retroativa)
CHAT_API_KEY=

# URL base para a API do modelo de chat
# Exemplos: 
#   OpenAI: https://api.openai.com/v1 (padrão)
#   Azure OpenAI: https://your-resource.openai.azure.com/
#   LocalAI: http://localhost:8080/v1
#   Ollama: http://localhost:11434/v1
CHAT_API_BASE=

# === CONFIGURAÇÃO DO MODELO EMBEDDINGS ===
# Configure o modelo de embeddings para busca vetorial e similaridade semântica
# Pode usar um provedor diferente do modelo de chat

# O modelo de embeddings para usar na busca vetorial
# Exemplos: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
EMBEDDINGS_MODEL=text-embedding-3-small

# Chave da API para o serviço de embeddings
# Usa OPENAI_API_KEY como fallback se não especificado (compatibilidade retroativa)
EMBEDDINGS_API_KEY=

# URL base para a API de embeddings
# Exemplos:
#   OpenAI: https://api.openai.com/v1 (padrão)
#   Azure OpenAI: https://your-resource.openai.azure.com/
#   LocalAI: http://localhost:8080/v1
EMBEDDINGS_API_BASE=

# === COMPATIBILIDADE RETROATIVA ===
# DEPRECATED: Use CHAT_MODEL ao invés de MODEL_CHOICE
# Esta variável é mantida para compatibilidade retroativa e será removida numa versão futura
MODEL_CHOICE=

# RAG strategies - set these to "true" or "false" (default to "false")
# USE_CONTEXTUAL_EMBEDDINGS: Enhances embeddings with contextual information for better retrieval
USE_CONTEXTUAL_EMBEDDINGS=false

# USE_HYBRID_SEARCH: Combines vector similarity search with keyword search for better results
USE_HYBRID_SEARCH=false

# USE_AGENTIC_RAG: Enables code example extraction, storage, and specialized code search functionality
USE_AGENTIC_RAG=false

# USE_RERANKING: Applies cross-encoder reranking to improve search result relevance
USE_RERANKING=false

# GPU Acceleration Configuration for CrossEncoder Reranking
# These settings only apply when USE_RERANKING=true

# USE_GPU_ACCELERATION: Enable GPU acceleration for CrossEncoder model
# Options: auto (detect best device), true/cuda (force CUDA), mps (Apple Silicon), false/cpu (CPU only)
USE_GPU_ACCELERATION=auto

# GPU_PRECISION: Precision for GPU inference (reduces memory usage, may slightly impact accuracy)
# Options: float32 (full precision), float16 (half precision), bfloat16 (brain float)
GPU_PRECISION=float32

# GPU_DEVICE_INDEX: GPU index for multi-GPU systems (0 = first GPU)
GPU_DEVICE_INDEX=0

# GPU_MEMORY_FRACTION: Fraction of GPU memory to use (0.1 to 1.0)
GPU_MEMORY_FRACTION=0.8

# USE_KNOWLEDGE_GRAPH: Enables AI hallucination detection and repository parsing tools using Neo4j
# If you set this to true, you must also set the Neo4j environment variables below.
USE_KNOWLEDGE_GRAPH=false

# Qdrant Configuration for Vector Database
# Qdrant is used as the vector database for storing and searching document embeddings
# These settings point to the local Qdrant instance running in Docker

# Qdrant host - use localhost when running MCP server with uv
# If running the MCP server in Docker, change to host.docker.internal
QDRANT_HOST=localhost

# Qdrant REST API port (default: 6333)
QDRANT_PORT=6333

# Neo4j Configuration for Knowledge Graph Tools
# These are required for the AI hallucination detection and repository parsing tools
# Leave empty to disable knowledge graph functionality

# Neo4j connection URI - use bolt://localhost:7687 for local, neo4j:// for cloud instances
# IMPORTANT: If running the MCP server through Docker, change localhost to host.docker.internal
NEO4J_URI=bolt://localhost:7687

# Neo4j username (usually 'neo4j' for default installations)
NEO4J_USER=neo4j

# Neo4j password for your database instance
# Default password for local Docker setup (matches docker-compose.yaml)
NEO4J_PASSWORD=password123