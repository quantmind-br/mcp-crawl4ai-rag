# ===============================
# MCP SERVER CONFIGURATION
# ===============================
TRANSPORT=sse
HOST=0.0.0.0
PORT=8051

# ===============================
# AI MODELS
# ===============================

# Chat Model (for summaries, contextual embeddings)
# Examples: gpt-4o-mini, gpt-3.5-turbo, claude-3-haiku, gemini/gemini-2.0-flash
CHAT_MODEL=gpt-4o-mini
CHAT_API_KEY=
CHAT_API_BASE=

# Embeddings Model (for vector search)
# 
# SUPPORTED PROVIDERS:
# 
# OpenAI (default):
# - text-embedding-3-small (1536 dims, $0.02/1M tokens)
# - text-embedding-3-large (3072 dims, $0.13/1M tokens)
# - text-embedding-ada-002 (1536 dims, $0.10/1M tokens)
#
# DeepInfra (cost-effective alternative):
# - Qwen/Qwen3-Embedding-0.6B (1024 dims, ~$0.01/1M tokens)
# - BAAI/bge-large-en-v1.5 (1024 dims)
# - BAAI/bge-small-en-v1.5 (384 dims)
# - sentence-transformers/all-MiniLM-L6-v2 (384 dims)
#
# RECOMMENDED: Qwen/Qwen3-Embedding-0.6B for best price/performance
EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDINGS_API_KEY=
EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai

# Embedding dimensions (auto-detected from model if not specified)
# 
# AUTO-DETECTION SUPPORTED MODELS:
# - text-embedding-3-small: 1536
# - text-embedding-3-large: 3072
# - Qwen/Qwen3-Embedding-0.6B: 1024
# - BAAI/bge-large-en-v1.5: 1024
# - BAAI/bge-small-en-v1.5: 384
# - sentence-transformers/all-MiniLM-L6-v2: 384
#
# MANUAL OVERRIDE: Set to specific value if needed
# WARNING: Wrong dimensions will trigger automatic collection recreation!
EMBEDDINGS_DIMENSIONS=

# Fallback Models
CHAT_FALLBACK_MODEL=gpt-4o-mini
EMBEDDINGS_FALLBACK_MODEL=text-embedding-3-small

# Optional fallback API configuration
CHAT_FALLBACK_API_KEY=
CHAT_FALLBACK_API_BASE=
EMBEDDINGS_FALLBACK_API_KEY=
EMBEDDINGS_FALLBACK_API_BASE=


# Advanced Redis Configuration (Optional - for fine-tuning)
REDIS_CONNECTION_TIMEOUT=5
REDIS_SOCKET_TIMEOUT=5
REDIS_MAX_CONNECTIONS=20
REDIS_HEALTH_CHECK_INTERVAL=30
REDIS_EMBEDDING_TTL=86400

# Circuit breaker settings for Redis resilience
REDIS_CIRCUIT_BREAKER_FAILURES=5
REDIS_CIRCUIT_BREAKER_TIMEOUT=60

# ===============================
# RAG FEATURES
# ===============================

# Contextual Embeddings: Enhance chunks with document context for better accuracy
# Performance impact: +30% token usage, +15-25% accuracy
USE_CONTEXTUAL_EMBEDDINGS=false

# Hybrid Search: Combine semantic (dense) + keyword (sparse) search using FastBM25
# NEW: Native Qdrant implementation with Reciprocal Rank Fusion (RRF)
# Performance: <200ms search (vs >500ms client-side), +20-40% accuracy
# Requires: fastembed>=0.4.0 (auto-installed), collection migration
USE_HYBRID_SEARCH=false

# Agentic RAG: Extract and index code examples separately
# Use for programming/technical documentation with dedicated code search
USE_AGENTIC_RAG=false

# Reranking: Re-order search results using cross-encoder models
# Performance: +50-100ms per query, +10-15% accuracy
USE_RERANKING=false

# Knowledge Graph: Neo4j integration for AI hallucination detection
# Advanced feature for code analysis and validation
USE_KNOWLEDGE_GRAPH=false

# Reranking model (when USE_RERANKING=true)
RERANKING_MODEL_NAME=cross-encoder/ms-marco-MiniLM-L-6-v2

# Reranking warmup samples (higher for better accuracy)
RERANKING_WARMUP_SAMPLES=5

# GPU settings (for reranking)
USE_GPU_ACCELERATION=auto

# ===============================
# GPU CONFIGURATION (Optional - for reranking)
# ===============================
# GPU device index (0 for first GPU, 1 for second, etc.)
GPU_DEVICE_INDEX=0

# Fraction of GPU memory to use (0.8 = 80%)
GPU_MEMORY_FRACTION=0.8

# Model precision for GPU operations
# Options: float32 (more memory, higher precision), float16 (less memory), bfloat16 (balance)
GPU_PRECISION=float32

# GPU usage preference (auto, cuda, mps, cpu)
GPU_PREFERENCE=auto

# ===============================
# DATABASES
# ===============================
# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333
# Neo4j Knowledge Graph
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123

# Redis Cache
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_USERNAME=
REDIS_SSL=false

# ===============================
# REDIS CACHE SETTINGS
# ===============================
USE_REDIS_CACHE=false

# ===============================
# PERFORMANCE OPTIMIZATION
# ===============================

# ProcessPoolExecutor workers for CPU-bound parsing tasks
# Recommended: Number of CPU cores available for parsing
# Enterprise hardware example: 32 for Dual Xeon setups
CPU_WORKERS=4

# ThreadPoolExecutor workers for I/O-bound operations
# Recommended: 2-3x CPU_WORKERS for optimal I/O concurrency
# Enterprise hardware example: 80 for high-memory systems
IO_WORKERS=10

# Batch size for Qdrant vector insertion operations
# Optimal range: 500-1000 for standard setups, 2000+ for enterprise hardware
BATCH_SIZE_QDRANT=500

# Batch size for Neo4j UNWIND operations
# Optimal range: 5000+ for bulk operations with minimal network round trips
# Enterprise hardware example: 20000+ for high-memory systems
BATCH_SIZE_NEO4J=5000

# Batch size for embeddings API calls
# OpenAI API limit: 1000-2000, DeepInfra supports higher (5000+)
# Adjust based on your API provider limits
BATCH_SIZE_EMBEDDINGS=1000

# Batch size for concurrent file processing
# Controls how many files are processed in each batch
# Enterprise hardware example: 100+ for abundant RAM
BATCH_SIZE_FILE_PROCESSING=10

# Maximum concurrent parsing operations
# Limits simultaneous CPU-intensive parsing tasks
# Enterprise hardware example: 35 for high-core systems
MAX_CONCURRENT_PARSING=8

# Chunk size for embedding generation
# Controls how text is split for embedding generation
# Enterprise example: 500 for better context with high-memory configs
EMBEDDING_CHUNK_SIZE=100

# ===============================
# HTTP OPTIMIZATION (Optional)
# ===============================
# HTTP/2 support for better API performance
HTTPX_HTTP2=true

# HTTP connection limits
# Standard: 20-50, Enterprise: 100-200+
HTTPCORE_MAX_CONNECTIONS=100
HTTPCORE_KEEPALIVE_EXPIRY=30

# ===============================
# SYSTEM CONFIGURATION (Optional)
# ===============================
# Logging level (DEBUG for development, INFO for production, WARNING/ERROR for minimal logs)
LOG_LEVEL=INFO

# ===============================
# EXAMPLE CONFIGURATIONS BY HARDWARE TYPE
# ===============================
#
# DEVELOPMENT / SMALL SCALE:
# CPU_WORKERS=4, IO_WORKERS=10, BATCH_SIZE_QDRANT=500
# BATCH_SIZE_EMBEDDINGS=1000, REDIS_MAX_CONNECTIONS=20
#
# PRODUCTION / MEDIUM SCALE:
# CPU_WORKERS=8, IO_WORKERS=20, BATCH_SIZE_QDRANT=1000
# BATCH_SIZE_EMBEDDINGS=2000, REDIS_MAX_CONNECTIONS=50
#
# ENTERPRISE / LARGE SCALE:
# CPU_WORKERS=32, IO_WORKERS=80, BATCH_SIZE_QDRANT=2000
# BATCH_SIZE_EMBEDDINGS=5000, REDIS_MAX_CONNECTIONS=100
# BATCH_SIZE_NEO4J=20000, EMBEDDING_CHUNK_SIZE=500

# ===============================
# MCP TOOLS TIMEOUT CONFIGURATION
# ===============================
# Controls maximum execution time for MCP tools to prevent client disconnections
# Values are in seconds - adjust based on your expected operation complexity

# Quick operations: Simple queries, data retrieval, status checks
MCP_QUICK_TIMEOUT=60

# Medium operations: RAG queries, analysis tasks, script validation
MCP_MEDIUM_TIMEOUT=300

# Long operations: Single page crawls, repository parsing, complex analysis
MCP_LONG_TIMEOUT=1800

# Very long operations: Multi-page crawls, full repository indexing, bulk processing
MCP_VERY_LONG_TIMEOUT=3600