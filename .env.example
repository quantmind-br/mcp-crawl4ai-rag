# ===============================
# MCP SERVER CONFIGURATION
# ===============================
TRANSPORT=sse
HOST=0.0.0.0
PORT=8051

# ===============================
# AI MODELS
# ===============================

# Chat Model (for summaries, contextual embeddings)
# Examples: gpt-4o-mini, gpt-3.5-turbo, claude-3-haiku
CHAT_MODEL=gpt-4o-mini
CHAT_API_KEY=
CHAT_API_BASE=

# Embeddings Model (for vector search)
# 
# SUPPORTED PROVIDERS:
# 
# OpenAI (default):
# - text-embedding-3-small (1536 dims, $0.02/1M tokens)
# - text-embedding-3-large (3072 dims, $0.13/1M tokens)
# - text-embedding-ada-002 (1536 dims, $0.10/1M tokens)
#
# DeepInfra (cost-effective alternative):
# - Qwen/Qwen3-Embedding-0.6B (1024 dims, ~$0.01/1M tokens)
# - BAAI/bge-large-en-v1.5 (1024 dims)
# - BAAI/bge-small-en-v1.5 (384 dims)
# - sentence-transformers/all-MiniLM-L6-v2 (384 dims)
#
# RECOMMENDED: Qwen/Qwen3-Embedding-0.6B for best price/performance
EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDINGS_API_KEY=
EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai

# Embedding dimensions (auto-detected from model if not specified)
# 
# AUTO-DETECTION SUPPORTED MODELS:
# - text-embedding-3-small: 1536
# - text-embedding-3-large: 3072
# - Qwen/Qwen3-Embedding-0.6B: 1024
# - BAAI/bge-large-en-v1.5: 1024
# - BAAI/bge-small-en-v1.5: 384
# - sentence-transformers/all-MiniLM-L6-v2: 384
#
# MANUAL OVERRIDE: Set to specific value if needed
# WARNING: Wrong dimensions will trigger automatic collection recreation!
EMBEDDINGS_DIMENSIONS=

# Fallback Models
CHAT_FALLBACK_MODEL=gpt-4o-mini
EMBEDDINGS_FALLBACK_MODEL=text-embedding-3-small

# Optional fallback API configuration
CHAT_FALLBACK_API_KEY=
CHAT_FALLBACK_API_BASE=
EMBEDDINGS_FALLBACK_API_KEY=
EMBEDDINGS_FALLBACK_API_BASE=

# ===============================
# REDIS CACHE CONFIGURATION
# ===============================

# Redis Connection
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_USERNAME=
REDIS_SSL=false

# Cache Settings
USE_REDIS_CACHE=false

# ===============================
# RAG FEATURES
# ===============================

# Contextual Embeddings: Enhance chunks with document context for better accuracy
# Performance impact: +30% token usage, +15-25% accuracy
USE_CONTEXTUAL_EMBEDDINGS=false

# Hybrid Search: Combine semantic (dense) + keyword (sparse) search using FastBM25
# NEW: Native Qdrant implementation with Reciprocal Rank Fusion (RRF)
# Performance: <200ms search (vs >500ms client-side), +20-40% accuracy
# Requires: fastembed>=0.4.0 (auto-installed), collection migration
USE_HYBRID_SEARCH=false

# Agentic RAG: Extract and index code examples separately
# Use for programming/technical documentation with dedicated code search
USE_AGENTIC_RAG=false

# Reranking: Re-order search results using cross-encoder models
# Performance: +50-100ms per query, +10-15% accuracy
USE_RERANKING=false

# Knowledge Graph: Neo4j integration for AI hallucination detection
# Advanced feature for code analysis and validation
USE_KNOWLEDGE_GRAPH=false

# Reranking model (when USE_RERANKING=true)
RERANKING_MODEL_NAME=cross-encoder/ms-marco-MiniLM-L-6-v2

# GPU settings (for reranking)
USE_GPU_ACCELERATION=auto

# ===============================
# DATABASES
# ===============================

# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Neo4j Knowledge Graph
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123

