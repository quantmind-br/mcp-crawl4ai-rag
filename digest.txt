Directory structure:
└── mcp-crawl4ai-rag/
    ├── README.md
    ├── CLAUDE.md
    ├── CRUSH.md
    ├── docker-compose.yaml
    ├── Dockerfile
    ├── LICENSE
    ├── pyproject.toml
    ├── run_server.py
    ├── setup.bat
    ├── start.bat
    ├── test_redis_cache.py
    ├── .env.example
    ├── backups/
    │   └── code_examples_backup_20250802_234528.json
    ├── knowledge_graphs/
    │   ├── ai_hallucination_detector.py
    │   ├── ai_script_analyzer.py
    │   ├── hallucination_reporter.py
    │   ├── knowledge_graph_validator.py
    │   ├── parse_repo_into_neo4j.py
    │   └── query_knowledge_graph.py
    ├── PRPs/
    │   ├── README.md
    │   ├── ai_docs/
    │   │   ├── github_cloning_best_practices.md
    │   │   ├── python_ast_docstring_extraction.md
    │   │   └── typescript_jsdoc_parsing.md
    │   ├── scripts/
    │   │   └── prp_runner.py
    │   └── templates/
    │       ├── prp_base.md
    │       ├── prp_base_typescript.md
    │       ├── prp_planning.md
    │       ├── prp_spec.md
    │       └── prp_task.md
    ├── scripts/
    │   ├── README.md
    │   ├── benchmark_embedding_cache.py
    │   ├── clean_qdrant.py
    │   └── fix_qdrant_dimensions.py
    ├── src/
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── crawl4ai_mcp.py
    │   ├── device_manager.py
    │   ├── embedding_cache.py
    │   ├── embedding_config.py
    │   ├── event_loop_fix.py
    │   ├── qdrant_wrapper.py
    │   ├── utils.py
    │   └── utils/
    │       ├── __init__.py
    │       ├── github_processor.py
    │       └── validation.py
    ├── tests/
    │   ├── __init__.py
    │   ├── conftest.py
    │   ├── integration_test.py
    │   ├── performance_benchmark.py
    │   ├── test_backward_compatibility.py
    │   ├── test_deepinfra_config.py
    │   ├── test_deepinfra_integration.py
    │   ├── test_device_manager.py
    │   ├── test_embedding_cache.py
    │   ├── test_event_loop_fix.py
    │   ├── test_fallback_api_config.py
    │   ├── test_flexible_api_config.py
    │   ├── test_github_processor.py
    │   ├── test_gpu_integration.py
    │   ├── test_integration_docker.py
    │   ├── test_mcp_basic.py
    │   ├── test_mcp_server.py
    │   ├── test_performance_validation.py
    │   ├── test_qdrant_optimization.py
    │   ├── test_qdrant_wrapper.py
    │   ├── test_redis_integration.py
    │   ├── test_reranking_enhanced.py
    │   ├── test_script.py
    │   ├── test_smart_crawl_github.py
    │   ├── test_smart_crawl_github_integration.py
    │   └── test_utils_integration.py
    └── .gemini/
        └── commands/
            ├── execute-prp.toml
            └── generate-prp.toml

================================================
FILE: README.md
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x8f in position 1452: character maps to <undefined>


================================================
FILE: CLAUDE.md
================================================
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Architecture Overview

This is a **Crawl4AI RAG MCP Server** that provides AI agents and coding assistants with advanced web crawling and RAG capabilities through the Model Context Protocol (MCP). The system integrates Crawl4AI for web scraping, Qdrant for vector storage, and optionally Neo4j for knowledge graph-based hallucination detection.

### Core Components
- **MCP Server**: `src/crawl4ai_mcp.py` - Main FastMCP server with tools for crawling and RAG
- **Vector Database**: Qdrant client wrapper (`src/qdrant_wrapper.py`) for document and code storage
- **Web Crawler**: Crawl4AI integration with smart URL detection and parallel processing
- **Knowledge Graph**: Neo4j integration (`knowledge_graphs/`) for AI hallucination detection
- **Utils**: Helper functions (`src/utils.py`) for embeddings, chunking, and API clients

### Key Technologies
- **MCP**: Model Context Protocol for AI agent integration
- **Crawl4AI**: Web crawling with automatic content extraction
- **Qdrant**: Vector database for semantic search
- **Neo4j**: Graph database for code analysis (optional)
- **OpenAI/DeepInfra**: APIs for embeddings and chat completions
- **PyTorch**: GPU acceleration for reranking (optional)

## Development Commands

### Setup and Installation
```bash
# Install dependencies
uv sync

# Setup Docker services (Qdrant + Neo4j)
setup.bat  # Windows
# or: docker-compose up -d

# Start MCP server
start.bat  # Windows
# or: uv run -m src
```

### Testing
```bash
# Run all tests
uv run pytest

# Run specific test categories
uv run pytest tests/test_mcp_basic.py          # Basic MCP functionality
uv run pytest tests/test_qdrant_wrapper.py     # Vector database tests
uv run pytest tests/test_deepinfra_config.py   # API configuration tests
uv run pytest tests/test_github_processor.py   # Knowledge graph tests

# Performance benchmarks
uv run pytest tests/performance_benchmark.py

# Integration tests (requires Docker services)
uv run pytest tests/integration_test.py
```

### Development Scripts
```bash
# Clean Qdrant database
uv run python scripts/clean_qdrant.py

# Fix Qdrant dimensions for existing collections
uv run python scripts/fix_qdrant_dimensions.py

# Knowledge graph tools
uv run python knowledge_graphs/parse_repo_into_neo4j.py <repo_url>
uv run python knowledge_graphs/ai_hallucination_detector.py <script_path>
```

## Configuration Architecture

The system uses a flexible multi-provider API configuration supporting:

### Modern Configuration (Recommended)
- `CHAT_MODEL` + `CHAT_API_KEY` + `CHAT_API_BASE` - For chat/completion operations
- `EMBEDDINGS_MODEL` + `EMBEDDINGS_API_KEY` + `EMBEDDINGS_API_BASE` - For vector embeddings
- `EMBEDDINGS_DIMENSIONS` - Explicit dimension override

### Legacy Configuration (Still Supported)
- `OPENAI_API_KEY` - Fallback for both chat and embeddings
- `MODEL_CHOICE` - Deprecated, use `CHAT_MODEL` instead

### RAG Strategy Flags
All default to `false`, enable as needed:
- `USE_CONTEXTUAL_EMBEDDINGS` - Enhanced context for chunks
- `USE_HYBRID_SEARCH` - Keyword + semantic search
- `USE_AGENTIC_RAG` - Specialized code example extraction
- `USE_RERANKING` - Cross-encoder result reordering
- `USE_KNOWLEDGE_GRAPH` - AI hallucination detection

### Reranking Configuration

The reranking system uses Cross-encoder models for semantic result reordering. When `USE_RERANKING=true`, the following configuration options are available:

#### Core Reranking Settings
- `USE_RERANKING` - Enable/disable reranking (default: `false`)
- `RERANKING_MODEL_NAME` - CrossEncoder model to use (default: `cross-encoder/ms-marco-MiniLM-L-6-v2`)
- `RERANKING_WARMUP_SAMPLES` - Number of dummy predictions during startup (default: `5`)

#### Device and Performance Settings
- `GPU_DEVICE_INDEX` - GPU device index for reranking (default: `0`)
- `GPU_PRECISION` - Model precision on GPU: `float32`, `float16`, `bfloat16` (default: `float32`)
- `USE_GPU_ACCELERATION` - Device preference: `auto`, `cuda`, `mps`, `cpu` (default: `auto`)

#### Model Options
Popular CrossEncoder models for different use cases:
- `cross-encoder/ms-marco-MiniLM-L-6-v2` - Balanced performance and speed (default)
- `cross-encoder/ms-marco-MiniLM-L-12-v2` - Higher accuracy, slower
- `cross-encoder/ms-marco-TinyBERT-L-2-v2` - Fastest, lower accuracy
- `cross-encoder/stsb-distilroberta-base` - For semantic similarity tasks

#### Health Monitoring
The system includes comprehensive health checks for reranking functionality:
- Model loading validation
- Device allocation verification
- Inference capability testing
- Performance metrics collection

Use the `health_check_reranking` MCP tool to validate your reranking setup.

## MCP Tools

### Core Tools (Always Available)
- `crawl_single_page` - Crawl individual webpage
- `smart_crawl_url` - Intelligent crawling (sitemaps, recursive)
- `get_available_sources` - List indexed sources
- `perform_rag_query` - Semantic search with source filtering

### Conditional Tools
- `search_code_examples` - Code-specific search (requires `USE_AGENTIC_RAG=true`)
- `parse_github_repository` - Index repo structure (requires `USE_KNOWLEDGE_GRAPH=true`)
- `check_ai_script_hallucinations` - Validate AI code (requires `USE_KNOWLEDGE_GRAPH=true`)
- `query_knowledge_graph` - Explore knowledge graph (requires `USE_KNOWLEDGE_GRAPH=true`)

## Model Configuration
- O modelo padrÃ£o que estamos usando na aplicaÃ§Ã£o Ã© o gpt-4o-mini. NÃ£o altere o modelo em qualquer arquivo sem a expressa instruÃ§Ã£o do usuÃ¡rio.

## Code Architecture Patterns

### Error Handling
- Use `tenacity` for retrying failed operations (especially API calls)
- Implement graceful fallbacks (GPU â†’ CPU, contextual â†’ basic embeddings)
- Log errors with context using the configured logger

### Async Patterns
- All MCP tools are async functions using `@mcp.tool()` decorator
- Web crawling uses `AsyncWebCrawler` with parallel processing
- Database operations are async with connection pooling

### Device Management
- `src/device_manager.py` handles GPU detection and memory management
- Automatic fallback from CUDA â†’ MPS â†’ CPU
- Memory cleanup after GPU operations

### Configuration Loading
- Environment variables loaded via `dotenv`
- Backward compatibility for legacy configurations
- Dynamic embedding dimensions based on model choice

## Important Implementation Notes

### Vector Dimensions
- Embedding dimensions are auto-detected from model choice
- Override with `EMBEDDINGS_DIMENSIONS` if needed
- Collections recreated if dimensions mismatch

### Windows Compatibility
- `src/event_loop_fix.py` handles Windows ConnectionResetError issues
- Batch scripts (`.bat`) for setup and startup
- PyTorch CUDA wheels specified for Windows in `pyproject.toml`

### Testing Strategy
- Mock external services (OpenAI, Qdrant) in unit tests
- Integration tests require actual Docker services
- Performance benchmarks measure crawling and search speed

## Coding Guidelines
- NÃ£o utilize emojis no cÃ³digo
- Use type hints for all function parameters and return values
- Follow async/await patterns for I/O operations
- Implement proper error handling with retries for external services


================================================
FILE: CRUSH.md
================================================
# CRUSH.md

Build/lint/test
- Env: Python 3.12+ with uv. Create venv: uv venv; activate; install deps: uv sync; editable install: uv pip install -e .; crawler setup: uv run crawl4ai-setup
- Run server: uv run -m src; stdio: TRANSPORT=stdio uv run -m src; SSE: uv run src/crawl4ai_mcp.py; health at /health
- Docker: docker build -t mcp/crawl4ai-rag --build-arg PORT=8051 .; docker run --env-file .env -p 8051:8051 mcp/crawl4ai-rag; compose: docker-compose up -d
- Tests: uv run pytest -q; single file: uv run pytest tests/test_qdrant_wrapper.py -q; single test: uv run pytest tests/test_qdrant_wrapper.py::TestQdrantClientWrapper::test_init_default_config -q
- Lint/format/typecheck: uv run ruff check .; uv run ruff format .; uv run mypy .
- Utilities: uv run python scripts/clean_qdrant.py; uv run python scripts/fix_qdrant_dimensions.py; KG tools: uv run python knowledge_graphs/ai_hallucination_detector.py <script.py>

Code style
- Imports: group stdlib/third-party/local; one per line; avoid deep relatives; minimal __all__; keep MCP tool imports local to tools
- Formatting: 120-char soft limit; use ruff format (black-compatible); trailing commas on multiline; one blank line between groups
- Types: annotate public funcs; use Optional/Dict/List/Tuple; prefer dataclasses for simple structs; MCP tools return JSON-serializable dicts/strings
- Naming: snake_case for funcs/vars; PascalCase for classes; UPPER_CASE for constants/env; private helpers prefixed with _
- Errors: never log secrets; catch broad exceptions only at tool boundaries; return {"success": false, "error": "..."}; use logging not prints (except CLI)
- Env/config: load .env via dotenv; support CHAT_/EMBEDDINGS_ with fallbacks; legacy OPENAI_* still supported; allow EMBEDDINGS_DIMENSIONS override
- RAG defaults: embeddings text-embedding-3-small; vector DB QdrantClientWrapper; optional hybrid search; rerank via CrossEncoder when USE_RERANKING=true
- Testing: mock Qdrant/OpenAI; deterministic tests; unit tests avoid network/GPU; run via uv for consistent env
- Concurrency: async/await everywhere; @mcp.tool for MCP; avoid blocking I/O in handlers; validate/sanitize URLs/paths; device_manager handles GPU fallback
- Security: do not commit/print API keys; guard file/network access behind flags; sanitize inputs; avoid writing to arbitrary paths

Cursor/Copilot
- No Cursor (.cursor/rules, .cursorrules) or Copilot (.github/copilot-instructions.md) rules found; if added, mirror constraints here and follow strictly

Notes
- Default chat model: gpt-4o-mini (do not change without explicit instruction)



================================================
FILE: docker-compose.yaml
================================================
services:
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    container_name: mcp-qdrant
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      - QDRANT__SERVICE__ENABLE_CORS=true
      - QDRANT__SERVICE__MAX_REQUEST_SIZE_MB=32
    # Healthcheck removed - service is functional but container lacks health check tools
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1'
        reservations:
          memory: 1G

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    container_name: mcp-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
    command: >
      redis-server
      --appendonly yes
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M

  neo4j:
    image: neo4j:5.26.1
    restart: unless-stopped
    container_name: mcp-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    environment:
      # Authentication
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD:-password123}
      
      # Plugins
      - NEO4J_PLUGINS=["apoc"]
      
      # Network Configuration (Modern Neo4j 5.x settings)
      - NEO4J_server_default__listen__address=0.0.0.0
      - NEO4J_server_bolt_listen__address=:7687
      - NEO4J_server_http_listen__address=:7474
      
      # Security Settings
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*
      
      # Performance Configuration (Modern Neo4j 5.x settings)
      - NEO4J_server_memory_heap_initial__size=512M
      - NEO4J_server_memory_heap_max__size=1G
      - NEO4J_server_memory_pagecache_size=512M
    healthcheck:
      test: ["CMD", "cypher-shell", "--username", "neo4j", "--password", "${NEO4J_PASSWORD:-password123}", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1'
        reservations:
          memory: 1G

volumes:
  qdrant_data:
    driver: local
  redis_data:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local

networks:
  default:
    name: mcp-network
    driver: bridge


================================================
FILE: Dockerfile
================================================
FROM python:3.12-slim

ARG PORT=8051

WORKDIR /app

# Install uv
RUN pip install uv

# Copy the MCP server files
COPY . .

# Install packages directly to the system (no virtual environment)
# Combining commands to reduce Docker layers
RUN uv pip install --system -e . && \
    crawl4ai-setup

EXPOSE ${PORT}

# Command to run the MCP server
CMD ["python", "src/crawl4ai_mcp.py"]



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 Cole Medin

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[project]
name = "crawl4ai-mcp"
version = "0.1.0"
description = "MCP server for integrating web crawling and RAG into AI agents and AI coding assistants"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "crawl4ai==0.6.2",
    "mcp==1.7.1",
    "qdrant-client>=1.12.0",
    "openai==1.71.0",
    "dotenv==0.9.9",
    "neo4j>=5.28.1",
    "tenacity>=8.0.0",
    "pytest>=8.4.1",
    "requests>=2.32.3",
    "sentence-transformers>=5.0.0",
    "redis>=5.0.0,<6.0.0",
    # PyTorch CUDA - specified as direct URLs
    "torch @ https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-win_amd64.whl",
    "torchvision @ https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-win_amd64.whl",
    "torchaudio @ https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-win_amd64.whl",
]

[dependency-groups]
dev = [
    "ruff>=0.12.7",
]




================================================
FILE: run_server.py
================================================
#!/usr/bin/env python3
"""
Simple entry point for the Crawl4AI MCP server.
This avoids relative import issues by running from the project root.
"""
import sys
import asyncio
from pathlib import Path

# Add src directory to Python path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

# Now we can import directly without relative imports
from crawl4ai_mcp import main

if __name__ == "__main__":
    asyncio.run(main())


================================================
FILE: setup.bat
================================================
@echo off
chcp 65001 >nul
setlocal EnableExtensions EnableDelayedExpansion

REM setup.bat - Initialize Docker stack for Crawl4AI MCP RAG application

echo ========================================
echo   Crawl4AI MCP RAG - Docker Setup
echo ========================================
echo.

echo [1/4] Verificando Docker...
docker --version >nul 2>&1
if %errorlevel% neq 0 (
    echo ERRO: Docker nao instalado ou inativo.
    pause
    exit /b 1
)
echo OK Docker disponivel

echo [2/4] Verificando docker-compose.yaml...
if not exist "docker-compose.yaml" (
    echo ERRO: docker-compose.yaml nao encontrado.
    pause
    exit /b 1
)
echo OK docker-compose.yaml OK

echo [3/4] Parando containers existentes...
docker-compose down --remove-orphans --volumes 2>nul
echo OK Limpeza concluida

echo [4/4] Iniciando servicos Docker...
echo Iniciando Qdrant, Neo4j e Redis...
docker-compose up -d

timeout /t 5 /nobreak >nul
echo.
echo Verificando status...
docker-compose ps

echo.
echo Aguardando servicos inicializarem...

:wait_qdrant
echo Verificando Qdrant...
curl -s http://localhost:6333/health >nul 2>&1
if %errorlevel% neq 0 (
    echo Qdrant ainda iniciando...
    timeout /t 3 /nobreak >nul
    goto wait_qdrant
)
echo OK Qdrant pronto em http://localhost:6333

echo Verificando Neo4j...
timeout /t 10 /nobreak >nul
netstat -an | find "7474" >nul
if %errorlevel% neq 0 (
    echo Neo4j pode ainda estar iniciando...
    timeout /t 5 /nobreak >nul
)
echo OK Neo4j disponivel em http://localhost:7474

echo Verificando Redis...
timeout /t 3 /nobreak >nul
docker exec mcp-redis redis-cli ping >nul 2>&1
if %errorlevel% neq 0 (
    echo Redis pode ainda estar iniciando...
    timeout /t 3 /nobreak >nul
)
echo OK Redis disponivel em localhost:6379

echo.
echo ========================================
echo       Setup concluido!
echo ========================================

echo Servicos iniciados:
echo   Qdrant: http://localhost:6333
echo   Neo4j:  http://localhost:7474 (user: neo4j, pass: password)
echo   Redis:  localhost:6379

echo.
echo Comandos uteis:
echo   docker-compose logs
echo   docker-compose logs qdrant
echo   docker-compose logs neo4j
echo   docker-compose logs redis
echo   docker-compose down

endlocal
pause



================================================
FILE: start.bat
================================================
@echo off
chcp 65001 >nul
setlocal EnableExtensions EnableDelayedExpansion

REM start.bat - Start Crawl4AI MCP RAG server

echo ========================================
echo   Crawl4AI MCP RAG - Server Startup
echo ========================================
echo.

echo [1/6] Checking uv installation...
uv --version >nul 2>&1
if %errorlevel% neq 0 (
    echo ERRO: uv nao esta instalado.
    echo Instale com: pip install uv
    pause
    exit /b 1
)
echo OK uv disponivel

echo [2/6] Verificando arquivo .env...
if not exist ".env" (
    echo AVISO: .env nao encontrado. Copiando .env.example...
    if exist ".env.example" (
        copy ".env.example" ".env" >nul
        echo OK .env criado a partir de .env.example
    ) else (
        echo ERRO: .env.example ausente.
        pause
        exit /b 1
    )
) else (
    echo OK .env encontrado
)

echo [3/6] Verificando servicos Docker...
echo Verificando Qdrant...
curl -s http://localhost:6333/health >nul 2>&1
if %errorlevel% neq 0 (
    echo AVISO: Qdrant indisponivel em localhost:6333
    echo Execute 'setup.bat' antes para iniciar os servicos.
    set /p "continue=Continuar assim mesmo? (s/N): "
    if /i not "%continue%"=="s" (
        echo Inicio cancelado.
        pause
        exit /b 1
    )
) else (
    echo OK Qdrant acessivel
)

echo Verificando Neo4j...
netstat -an | find "7474" >nul 2>&1
if %errorlevel% neq 0 (
    echo AVISO: Neo4j nao acessivel em localhost:7474
) else (
    echo OK Neo4j acessivel
)

echo Verificando Redis...
netstat -an | find "6379" >nul 2>&1
if %errorlevel% neq 0 (
    echo AVISO: Redis nao acessivel em localhost:6379
    echo Cache de embeddings sera desabilitado.
) else (
    echo OK Redis acessivel
)

echo [4/6] Instalando dependencias...
uv sync
if %errorlevel% neq 0 (
    echo ERRO: Falha ao instalar dependencias.
    pause
    exit /b 1
)
echo OK Dependencias instaladas

echo [5/6] Verificando arquivos do servidor...
if not exist "src\crawl4ai_mcp.py" (
    echo ERRO: src\crawl4ai_mcp.py nao encontrado.
    pause
    exit /b 1
)
echo OK Arquivos OK

echo [6/7] Verificando se a porta 8051 esta em uso...
set "PID="
for /f "tokens=5" %%a in ('netstat -aon ^| findstr ":8051"') do (
    if not defined PID set "PID=%%a"
)

if defined PID (
    if "%PID%" NEQ "0" (
        echo A porta 8051 esta sendo usada pelo processo com PID %PID%.
        echo Encerrando o processo...
        taskkill /F /PID %PID% >nul
        if !errorlevel! equ 0 (
            echo OK Processo encerrado com sucesso.
        ) else (
            echo AVISO: Nao foi possivel encerrar o processo com PID %PID%.
        )
    )
) else (
    echo OK Porta 8051 esta livre.
)


echo [7/7] Iniciando servidor MCP...
uv run -m src
if %errorlevel% neq 0 (
    echo Tentando alternativa...
    uv run run_server.py
)
set exit_code=%errorlevel%

echo.
if %exit_code% neq 0 (
    echo ========================================
    echo     ERRO Servidor finalizado com erro
    echo ========================================
    echo Codigo: %exit_code%
) else (
    echo ========================================
    echo      OK Servidor finalizado
    echo ========================================
)

endlocal
pause



================================================
FILE: test_redis_cache.py
================================================
#!/usr/bin/env python3
"""
Script para testar o cache Redis de embeddings na prÃ¡tica.
Testa cache hits, misses, TTL e performance.
"""

import time
import sys
import os
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from dotenv import load_dotenv
from embedding_cache import EmbeddingCache
import redis

load_dotenv()

def test_redis_cache():
    """Teste completo do cache Redis de embeddings."""
    
    print("=== TESTE COMPLETO DO CACHE REDIS ===\n")
    
    # 1. Verificar configuraÃ§Ã£o
    print("1. CONFIGURACAO:")
    print(f"   USE_REDIS_CACHE: {os.getenv('USE_REDIS_CACHE')}")
    print(f"   REDIS_HOST: {os.getenv('REDIS_HOST')}")
    print(f"   REDIS_PORT: {os.getenv('REDIS_PORT')}")
    print(f"   REDIS_EMBEDDING_TTL: {os.getenv('REDIS_EMBEDDING_TTL')}s")
    print()
    
    # 2. Inicializar cache
    try:
        cache = EmbeddingCache()
        print("2. INICIALIZACAO:")
        print("   [OK] EmbeddingCache inicializado")
        
        # Health check
        health = cache.health_check()
        print(f"   Status: {health['status']}")
        print(f"   Conexao: {health['connection_test']}")
        print(f"   Memoria: {health['memory_usage']}")
        print()
        
    except Exception as e:
        print(f"   [ERROR] Falha na inicializacao: {e}")
        return False
    
    # 3. Teste de Cache Miss (primeira busca)
    print("3. TESTE CACHE MISS:")
    test_text = "Este Ã© um texto de teste para cache de embeddings"
    test_key = f"test_embedding_{hash(test_text)}"
    
    start_time = time.time()
    result_miss = cache.get(test_key)
    miss_time = time.time() - start_time
    
    print(f"   Texto: '{test_text[:50]}...'")
    print(f"   Resultado (deve ser None): {result_miss}")
    print(f"   Tempo: {miss_time*1000:.2f}ms")
    print()
    
    # 4. Simular criaÃ§Ã£o de embedding e cache
    print("4. TESTE CACHE SET:")
    fake_embedding = [0.1, 0.2, 0.3, 0.4, 0.5] * 200  # 1000 dimensions
    
    start_time = time.time()
    cache.set(test_key, fake_embedding, ttl=300)  # 5 minutos
    set_time = time.time() - start_time
    
    print(f"   Embedding simulado (1000 dims): {fake_embedding[:5]}...")
    print(f"   Cache SET completado")
    print(f"   Tempo: {set_time*1000:.2f}ms")
    print()
    
    # 5. Teste de Cache Hit (segunda busca)
    print("5. TESTE CACHE HIT:")
    
    start_time = time.time()
    result_hit = cache.get(test_key)
    hit_time = time.time() - start_time
    
    print(f"   Resultado encontrado: {result_hit is not None}")
    print(f"   Dimensoes: {len(result_hit) if result_hit else 0}")
    print(f"   Primeiros valores: {result_hit[:5] if result_hit else None}")
    print(f"   Tempo: {hit_time*1000:.2f}ms")
    print(f"   Speedup: {miss_time/hit_time:.1f}x mais rapido")
    print()
    
    # 6. Teste de Performance (mÃºltiplas operaÃ§Ãµes)
    print("6. TESTE DE PERFORMANCE:")
    
    # Cache mÃºltiplos embeddings
    embeddings_test = {}
    for i in range(10):
        key = f"perf_test_{i}"
        embedding = [i * 0.1] * 1024  # 1024 dims
        embeddings_test[key] = embedding
    
    # Teste SET em lote
    start_time = time.time()
    for key, embedding in embeddings_test.items():
        cache.set(key, embedding, ttl=300)
    batch_set_time = time.time() - start_time
    
    print(f"   SET 10 embeddings: {batch_set_time*1000:.2f}ms")
    
    # Teste GET em lote
    start_time = time.time()
    hits = 0
    for key in embeddings_test.keys():
        result = cache.get(key)
        if result is not None:
            hits += 1
    batch_get_time = time.time() - start_time
    
    print(f"   GET 10 embeddings: {batch_get_time*1000:.2f}ms")
    print(f"   Cache hits: {hits}/10")
    print(f"   Avg GET time: {batch_get_time/10*1000:.2f}ms per embedding")
    print()
    
    # 7. Teste de TTL (Time To Live)
    print("7. TESTE TTL (Time To Live):")
    
    ttl_key = "ttl_test"
    cache.set(ttl_key, [1, 2, 3], ttl=2)  # 2 segundos apenas
    
    # Verificar imediatamente
    immediate_result = cache.get(ttl_key)
    print(f"   Imediatamente: {immediate_result is not None}")
    
    # Aguardar expiraÃ§Ã£o
    print("   Aguardando expiracao (3s)...")
    time.sleep(3)
    
    expired_result = cache.get(ttl_key)
    print(f"   Apos expiracao: {expired_result is not None}")
    print()
    
    # 8. EstatÃ­sticas do Redis
    print("8. ESTATISTICAS REDIS:")
    try:
        r = redis.Redis(
            host=os.getenv('REDIS_HOST', 'localhost'),
            port=int(os.getenv('REDIS_PORT', '6379')),
            db=int(os.getenv('REDIS_DB', '0'))
        )
        
        info = r.info()
        stats = r.info('stats')
        
        print(f"   Total commands processed: {stats.get('total_commands_processed', 'N/A')}")
        print(f"   Cache hits: {stats.get('keyspace_hits', 0)}")
        print(f"   Cache misses: {stats.get('keyspace_misses', 0)}")
        
        hit_ratio = 0
        if stats.get('keyspace_hits', 0) + stats.get('keyspace_misses', 0) > 0:
            hit_ratio = stats['keyspace_hits'] / (stats['keyspace_hits'] + stats['keyspace_misses']) * 100
        
        print(f"   Hit ratio: {hit_ratio:.1f}%")
        print(f"   Memory used: {info['used_memory_human']}")
        print(f"   Keys in DB: {info.get('db0', {}).get('keys', 0) if 'db0' in info else 0}")
        
    except Exception as e:
        print(f"   [ERROR] Falha ao obter estatisticas: {e}")
    
    print()
    
    # 9. Limpeza
    print("9. LIMPEZA:")
    cleanup_keys = [test_key, ttl_key] + list(embeddings_test.keys())
    for key in cleanup_keys:
        cache.delete(key)
    print(f"   {len(cleanup_keys)} chaves de teste removidas")
    print()
    
    print("=== TESTE CONCLUIDO ===")
    print("[OK] Cache Redis funcionando corretamente!")
    return True

if __name__ == "__main__":
    success = test_redis_cache()
    sys.exit(0 if success else 1)


================================================
FILE: .env.example
================================================
# ===============================
# MCP SERVER CONFIGURATION
# ===============================
TRANSPORT=sse
HOST=0.0.0.0
PORT=8051

# ===============================
# AI MODELS
# ===============================

# Chat Model (for summaries, contextual embeddings)
# Examples: gpt-4o-mini, gpt-3.5-turbo, claude-3-haiku
CHAT_MODEL=gpt-4o-mini
CHAT_API_KEY=
CHAT_API_BASE=

# Embeddings Model (for vector search)
# 
# SUPPORTED PROVIDERS:
# 
# OpenAI (default):
# - text-embedding-3-small (1536 dims, $0.02/1M tokens)
# - text-embedding-3-large (3072 dims, $0.13/1M tokens)
# - text-embedding-ada-002 (1536 dims, $0.10/1M tokens)
#
# DeepInfra (cost-effective alternative):
# - Qwen/Qwen3-Embedding-0.6B (1024 dims, ~$0.01/1M tokens)
# - BAAI/bge-large-en-v1.5 (1024 dims)
# - BAAI/bge-small-en-v1.5 (384 dims)
# - sentence-transformers/all-MiniLM-L6-v2 (384 dims)
#
# RECOMMENDED: Qwen/Qwen3-Embedding-0.6B for best price/performance
EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
EMBEDDINGS_API_KEY=
EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai

# Embedding dimensions (auto-detected from model if not specified)
# 
# AUTO-DETECTION SUPPORTED MODELS:
# - text-embedding-3-small: 1536
# - text-embedding-3-large: 3072
# - Qwen/Qwen3-Embedding-0.6B: 1024
# - BAAI/bge-large-en-v1.5: 1024
# - BAAI/bge-small-en-v1.5: 384
# - sentence-transformers/all-MiniLM-L6-v2: 384
#
# MANUAL OVERRIDE: Set to specific value if needed
# WARNING: Wrong dimensions will trigger automatic collection recreation!
EMBEDDINGS_DIMENSIONS=

# ===============================
# CONFIGURATION EXAMPLES
# ===============================
#
# Example 1: DeepInfra Qwen3 Setup (RECOMMENDED - Cost Effective)
# EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
# EMBEDDINGS_API_KEY=your-deepinfra-api-key
# EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai
# EMBEDDINGS_DIMENSIONS=  # Auto-detected as 1024
#
# Example 2: OpenAI Setup (Higher cost, proven reliability)
# EMBEDDINGS_MODEL=text-embedding-3-small
# EMBEDDINGS_API_KEY=sk-proj-your-openai-key
# EMBEDDINGS_API_BASE=  # Uses OpenAI default
# EMBEDDINGS_DIMENSIONS=  # Auto-detected as 1536
#
# Example 3: Mixed Provider Setup (DeepInfra primary, OpenAI fallback)
# EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
# EMBEDDINGS_API_KEY=your-deepinfra-key
# EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai
# EMBEDDINGS_FALLBACK_MODEL=text-embedding-3-small
# EMBEDDINGS_FALLBACK_API_KEY=sk-proj-your-openai-key
# EMBEDDINGS_FALLBACK_API_BASE=https://api.openai.com/v1
#
# Example 4: Custom Dimensions Override
# EMBEDDINGS_MODEL=Qwen/Qwen3-Embedding-0.6B
# EMBEDDINGS_API_KEY=your-deepinfra-key
# EMBEDDINGS_API_BASE=https://api.deepinfra.com/v1/openai
# EMBEDDINGS_DIMENSIONS=1024  # Explicit override (normally auto-detected)

# Fallback Models
CHAT_FALLBACK_MODEL=gpt-4o-mini
EMBEDDINGS_FALLBACK_MODEL=text-embedding-3-small

# Fallback API Configuration (optional - inherits from primary if not set)
# 
# PURPOSE: Enable true API provider failover for resilience and flexibility
# 
# INHERITANCE BEHAVIOR:
# - If CHAT_FALLBACK_API_KEY is not set, inherits CHAT_API_KEY
# - If CHAT_FALLBACK_API_BASE is not set, inherits CHAT_API_BASE
# - Same inheritance pattern applies to EMBEDDINGS_FALLBACK_* variables
#
# USE CASES:
# 1. Resilience: Primary API down, automatic failover to different provider
# 2. Cost optimization: Premium primary provider, cheaper fallback provider
# 3. Rate limiting: Fallback to unrestricted provider when primary is rate limited
# 4. Regional compliance: Different providers for different geographical requirements
#
# CONFIGURATION EXAMPLES:
#
# Example 1: Mixed providers (Primary: OpenRouter, Fallback: OpenAI)
# CHAT_API_KEY=sk-or-v1-your-openrouter-key
# CHAT_API_BASE=https://openrouter.ai/api/v1
# CHAT_FALLBACK_API_KEY=sk-proj-your-openai-key
# CHAT_FALLBACK_API_BASE=https://api.openai.com/v1
#
# Example 2: Inheritance (Fallback uses same provider with inherited config)
# CHAT_API_KEY=your-primary-key
# CHAT_API_BASE=https://api.yourprovider.com/v1
# CHAT_FALLBACK_MODEL=gpt-3.5-turbo  # Only model differs, API config inherited
#
# Example 3: Azure primary, OpenAI fallback
# CHAT_API_KEY=your-azure-key
# CHAT_API_BASE=https://your-resource.openai.azure.com/
# CHAT_FALLBACK_API_KEY=sk-proj-your-openai-key
# CHAT_FALLBACK_API_BASE=https://api.openai.com/v1
#
# TROUBLESHOOTING:
# - If both primary and fallback fail: Check API keys and network connectivity
# - Inheritance not working: Verify primary configuration is set correctly
# - Wrong provider used: Check that fallback base URL is explicitly set for different providers
#
# Optional: Set only if you want different API configuration for fallback
CHAT_FALLBACK_API_KEY=
CHAT_FALLBACK_API_BASE=
EMBEDDINGS_FALLBACK_API_KEY=
EMBEDDINGS_FALLBACK_API_BASE=

# ===============================
# REDIS CACHE CONFIGURATION
# ===============================

# Redis Connection
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_USERNAME=
REDIS_SSL=false

# Redis Performance Settings
REDIS_CONNECTION_TIMEOUT=5
REDIS_SOCKET_TIMEOUT=5
REDIS_MAX_CONNECTIONS=20
REDIS_HEALTH_CHECK_INTERVAL=30

# Cache Behavior
USE_REDIS_CACHE=false
REDIS_EMBEDDING_TTL=86400
REDIS_CIRCUIT_BREAKER_FAILURES=5
REDIS_CIRCUIT_BREAKER_TIMEOUT=60

# ===============================
# RAG FEATURES
# ===============================
USE_CONTEXTUAL_EMBEDDINGS=false
USE_HYBRID_SEARCH=false
USE_AGENTIC_RAG=false
USE_RERANKING=false
USE_KNOWLEDGE_GRAPH=false

# ===============================
# RERANKING CONFIGURATION
# ===============================
# Enhanced reranking settings (requires USE_RERANKING=true)

# CrossEncoder model selection
# Popular models for different use cases:
# - cross-encoder/ms-marco-MiniLM-L-6-v2: Balanced performance and speed (default)
# - cross-encoder/ms-marco-MiniLM-L-12-v2: Higher accuracy, slower
# - cross-encoder/ms-marco-TinyBERT-L-2-v2: Fastest, lower accuracy
# - cross-encoder/stsb-distilroberta-base: For semantic similarity tasks
RERANKING_MODEL_NAME=cross-encoder/ms-marco-MiniLM-L-6-v2

# Model warming configuration
# Number of dummy predictions during startup to improve first request latency
# Higher values = better warmup, slower startup. Set to 0 to disable.
RERANKING_WARMUP_SAMPLES=5

# ===============================
# GPU ACCELERATION
# ===============================
# Options: auto|cuda|mps|cpu
USE_GPU_ACCELERATION=auto
# Options: float32|float16|bfloat16
GPU_PRECISION=float32
GPU_DEVICE_INDEX=0
GPU_MEMORY_FRACTION=0.8

# ===============================
# DATABASES
# ===============================

# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Neo4j Knowledge Graph
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123

# ===============================
# HTTP CLIENT CONFIGURATION
# ===============================
# ConnectionResetError fix settings for Windows stability
# These are automatically applied but can be overridden here

# Enable HTTP/2 for better performance (true/false)
HTTPX_HTTP2=true

# Maximum total HTTP connections for crawling operations
# Higher values = faster crawling, more resource usage
# Default: 200 (optimized for performance)
HTTPCORE_MAX_CONNECTIONS=200

# Maximum keepalive connections to reuse
# Higher values = better connection reuse, less overhead
# Default: 50 (balanced performance)
HTTPCORE_MAX_KEEPALIVE_CONNECTIONS=50

# Connection keepalive expiry time in seconds
# Longer values = better connection reuse, more memory usage
# Default: 30.0 (optimized for stability)
HTTPCORE_KEEPALIVE_EXPIRY=30.0


================================================
FILE: backups/code_examples_backup_20250802_234528.json
================================================
{
  "name": "code_examples",
  "vectors_count": 0,
  "config": "params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None) hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None) optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=10000, flush_interval_sec=5, max_optimization_threads=None) wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0) quantization_config=None strict_mode_config=StrictModeConfigOutput(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, max_points_count=None, filter_max_conditions=None, condition_max_size=None, multivector_config=None, sparse_config=None)",
  "status": "green"
}


================================================
FILE: knowledge_graphs/ai_hallucination_detector.py
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x9d in position 7819: character maps to <undefined>


================================================
FILE: knowledge_graphs/ai_script_analyzer.py
================================================
"""
AI Script Analyzer

Parses Python scripts generated by AI coding assistants using AST to extract:
- Import statements and their usage
- Class instantiations and method calls  
- Function calls with parameters
- Attribute access patterns
- Variable type tracking
"""

import ast
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


@dataclass
class ImportInfo:
    """Information about an import statement"""
    module: str
    name: str
    alias: Optional[str] = None
    is_from_import: bool = False
    line_number: int = 0


@dataclass
class MethodCall:
    """Information about a method call"""
    object_name: str
    method_name: str
    args: List[str]
    kwargs: Dict[str, str]
    line_number: int
    object_type: Optional[str] = None  # Inferred class type


@dataclass
class AttributeAccess:
    """Information about attribute access"""
    object_name: str
    attribute_name: str
    line_number: int
    object_type: Optional[str] = None  # Inferred class type


@dataclass
class FunctionCall:
    """Information about a function call"""
    function_name: str
    args: List[str]
    kwargs: Dict[str, str]
    line_number: int
    full_name: Optional[str] = None  # Module.function_name


@dataclass
class ClassInstantiation:
    """Information about class instantiation"""
    variable_name: str
    class_name: str
    args: List[str]
    kwargs: Dict[str, str]
    line_number: int
    full_class_name: Optional[str] = None  # Module.ClassName


@dataclass
class AnalysisResult:
    """Complete analysis results for a Python script"""
    file_path: str
    imports: List[ImportInfo] = field(default_factory=list)
    class_instantiations: List[ClassInstantiation] = field(default_factory=list)
    method_calls: List[MethodCall] = field(default_factory=list)
    attribute_accesses: List[AttributeAccess] = field(default_factory=list)
    function_calls: List[FunctionCall] = field(default_factory=list)
    variable_types: Dict[str, str] = field(default_factory=dict)  # variable_name -> class_type
    errors: List[str] = field(default_factory=list)


class AIScriptAnalyzer:
    """Analyzes AI-generated Python scripts for validation against knowledge graph"""
    
    def __init__(self):
        self.import_map: Dict[str, str] = {}  # alias -> actual_module_name
        self.variable_types: Dict[str, str] = {}  # variable_name -> class_type
        self.context_manager_vars: Dict[str, Tuple[int, int, str]] = {}  # var_name -> (start_line, end_line, type)
        
    def analyze_script(self, script_path: str) -> AnalysisResult:
        """Analyze a Python script and extract all relevant information"""
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            result = AnalysisResult(file_path=script_path)
            
            # Reset state for new analysis
            self.import_map.clear()
            self.variable_types.clear()
            self.context_manager_vars.clear()
            
            # Track processed nodes to avoid duplicates
            self.processed_calls = set()
            self.method_call_attributes = set()
            
            # First pass: collect imports and build import map
            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    self._extract_imports(node, result)
            
            # Second pass: analyze usage patterns
            for node in ast.walk(tree):
                self._analyze_node(node, result)
            
            # Set inferred types on method calls and attribute accesses
            self._infer_object_types(result)
            
            result.variable_types = self.variable_types.copy()
            
            return result
            
        except Exception as e:
            error_msg = f"Failed to analyze script {script_path}: {str(e)}"
            logger.error(error_msg)
            result = AnalysisResult(file_path=script_path)
            result.errors.append(error_msg)
            return result
    
    def _extract_imports(self, node: ast.AST, result: AnalysisResult):
        """Extract import information and build import mapping"""
        line_num = getattr(node, 'lineno', 0)
        
        if isinstance(node, ast.Import):
            for alias in node.names:
                import_name = alias.name
                alias_name = alias.asname or import_name
                
                result.imports.append(ImportInfo(
                    module=import_name,
                    name=import_name,
                    alias=alias.asname,
                    is_from_import=False,
                    line_number=line_num
                ))
                
                self.import_map[alias_name] = import_name
                
        elif isinstance(node, ast.ImportFrom):
            module = node.module or ""
            for alias in node.names:
                import_name = alias.name
                alias_name = alias.asname or import_name
                
                result.imports.append(ImportInfo(
                    module=module,
                    name=import_name,
                    alias=alias.asname,
                    is_from_import=True,
                    line_number=line_num
                ))
                
                # Map alias to full module.name
                if module:
                    full_name = f"{module}.{import_name}"
                    self.import_map[alias_name] = full_name
                else:
                    self.import_map[alias_name] = import_name
    
    def _analyze_node(self, node: ast.AST, result: AnalysisResult):
        """Analyze individual AST nodes for usage patterns"""
        line_num = getattr(node, 'lineno', 0)
        
        # Assignments (class instantiations and method call results)
        if isinstance(node, ast.Assign):
            if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):
                if isinstance(node.value, ast.Call):
                    # Check if it's a class instantiation or method call
                    if isinstance(node.value.func, ast.Name):
                        # Direct function/class call
                        self._extract_class_instantiation(node, result)
                        # Mark this call as processed to avoid duplicate processing
                        self.processed_calls.add(id(node.value))
                    elif isinstance(node.value.func, ast.Attribute):
                        # Method call - track the variable assignment for type inference
                        var_name = node.targets[0].id
                        self._track_method_result_assignment(node.value, var_name)
                        # Still process the method call
                        self._extract_method_call(node.value, result)
                        self.processed_calls.add(id(node.value))
        
        # AsyncWith statements (context managers)
        elif isinstance(node, ast.AsyncWith):
            self._handle_async_with(node, result)
        elif isinstance(node, ast.With):
            self._handle_with(node, result)
        
        # Method calls and function calls
        elif isinstance(node, ast.Call):
            # Skip if this call was already processed as part of an assignment
            if id(node) in self.processed_calls:
                return
                
            if isinstance(node.func, ast.Attribute):
                self._extract_method_call(node, result)
                # Mark this attribute as used in method call to avoid duplicate processing
                self.method_call_attributes.add(id(node.func))
            elif isinstance(node.func, ast.Name):
                # Check if this is likely a class instantiation (based on imported classes)
                func_name = node.func.id
                full_name = self._resolve_full_name(func_name)
                
                # If this is a known imported class, treat as class instantiation
                if self._is_likely_class_instantiation(func_name, full_name):
                    self._extract_nested_class_instantiation(node, result)
                else:
                    self._extract_function_call(node, result)
        
        # Attribute access (not in call context)
        elif isinstance(node, ast.Attribute):
            # Skip if this attribute was already processed as part of a method call
            if id(node) in self.method_call_attributes:
                return
            self._extract_attribute_access(node, result)
    
    def _extract_class_instantiation(self, node: ast.Assign, result: AnalysisResult):
        """Extract class instantiation from assignment"""
        target = node.targets[0]
        call = node.value
        line_num = getattr(node, 'lineno', 0)
        
        if isinstance(target, ast.Name) and isinstance(call, ast.Call):
            var_name = target.id
            class_name = self._get_name_from_call(call.func)
            
            if class_name:
                args = [self._get_arg_representation(arg) for arg in call.args]
                kwargs = {
                    kw.arg: self._get_arg_representation(kw.value) 
                    for kw in call.keywords if kw.arg
                }
                
                # Resolve full class name using import map
                full_class_name = self._resolve_full_name(class_name)
                
                instantiation = ClassInstantiation(
                    variable_name=var_name,
                    class_name=class_name,
                    args=args,
                    kwargs=kwargs,
                    line_number=line_num,
                    full_class_name=full_class_name
                )
                
                result.class_instantiations.append(instantiation)
                
                # Track variable type for later method call analysis
                self.variable_types[var_name] = full_class_name or class_name
    
    def _extract_method_call(self, node: ast.Call, result: AnalysisResult):
        """Extract method call information"""
        if isinstance(node.func, ast.Attribute):
            line_num = getattr(node, 'lineno', 0)
            
            # Get object and method names
            obj_name = self._get_name_from_node(node.func.value)
            method_name = node.func.attr
            
            if obj_name and method_name:
                args = [self._get_arg_representation(arg) for arg in node.args]
                kwargs = {
                    kw.arg: self._get_arg_representation(kw.value) 
                    for kw in node.keywords if kw.arg
                }
                
                method_call = MethodCall(
                    object_name=obj_name,
                    method_name=method_name,
                    args=args,
                    kwargs=kwargs,
                    line_number=line_num,
                    object_type=self.variable_types.get(obj_name)
                )
                
                result.method_calls.append(method_call)
    
    def _extract_function_call(self, node: ast.Call, result: AnalysisResult):
        """Extract function call information"""
        if isinstance(node.func, ast.Name):
            line_num = getattr(node, 'lineno', 0)
            func_name = node.func.id
            
            args = [self._get_arg_representation(arg) for arg in node.args]
            kwargs = {
                kw.arg: self._get_arg_representation(kw.value) 
                for kw in node.keywords if kw.arg
            }
            
            # Resolve full function name using import map
            full_func_name = self._resolve_full_name(func_name)
            
            function_call = FunctionCall(
                function_name=func_name,
                args=args,
                kwargs=kwargs,
                line_number=line_num,
                full_name=full_func_name
            )
            
            result.function_calls.append(function_call)
    
    def _extract_attribute_access(self, node: ast.Attribute, result: AnalysisResult):
        """Extract attribute access information"""
        line_num = getattr(node, 'lineno', 0)
        
        obj_name = self._get_name_from_node(node.value)
        attr_name = node.attr
        
        if obj_name and attr_name:
            attribute_access = AttributeAccess(
                object_name=obj_name,
                attribute_name=attr_name,
                line_number=line_num,
                object_type=self.variable_types.get(obj_name)
            )
            
            result.attribute_accesses.append(attribute_access)
    
    def _infer_object_types(self, result: AnalysisResult):
        """Update object types for method calls and attribute accesses"""
        for method_call in result.method_calls:
            if not method_call.object_type:
                # First check context manager variables
                obj_type = self._get_context_aware_type(method_call.object_name, method_call.line_number)
                if obj_type:
                    method_call.object_type = obj_type
                else:
                    method_call.object_type = self.variable_types.get(method_call.object_name)
        
        for attr_access in result.attribute_accesses:
            if not attr_access.object_type:
                # First check context manager variables
                obj_type = self._get_context_aware_type(attr_access.object_name, attr_access.line_number)
                if obj_type:
                    attr_access.object_type = obj_type
                else:
                    attr_access.object_type = self.variable_types.get(attr_access.object_name)
    
    def _get_context_aware_type(self, var_name: str, line_number: int) -> Optional[str]:
        """Get the type of a variable considering its context (e.g., async with scope)"""
        if var_name in self.context_manager_vars:
            start_line, end_line, var_type = self.context_manager_vars[var_name]
            if start_line <= line_number <= end_line:
                return var_type
        return None
    
    def _get_name_from_call(self, node: ast.AST) -> Optional[str]:
        """Get the name from a call node (for class instantiation)"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            value_name = self._get_name_from_node(node.value)
            if value_name:
                return f"{value_name}.{node.attr}"
        return None
    
    def _get_name_from_node(self, node: ast.AST) -> Optional[str]:
        """Get string representation of a node (for object names)"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            value_name = self._get_name_from_node(node.value)
            if value_name:
                return f"{value_name}.{node.attr}"
        return None
    
    def _get_arg_representation(self, node: ast.AST) -> str:
        """Get string representation of an argument"""
        if isinstance(node, ast.Constant):
            return repr(node.value)
        elif isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return self._get_name_from_node(node) or "<?>"
        elif isinstance(node, ast.Call):
            func_name = self._get_name_from_call(node.func)
            return f"{func_name}(...)" if func_name else "call(...)"
        else:
            return f"<{type(node).__name__}>"
    
    def _is_likely_class_instantiation(self, func_name: str, full_name: Optional[str]) -> bool:
        """Determine if a function call is likely a class instantiation"""
        # Check if it's a known imported class (classes typically start with uppercase)
        if func_name and func_name[0].isupper():
            return True
        
        # Check if the full name suggests a class (contains known class patterns)
        if full_name:
            # Common class patterns in module names
            class_patterns = [
                'Model', 'Provider', 'Client', 'Agent', 'Manager', 'Handler',
                'Builder', 'Factory', 'Service', 'Controller', 'Processor'
            ]
            return any(pattern in full_name for pattern in class_patterns)
        
        return False
    
    def _extract_nested_class_instantiation(self, node: ast.Call, result: AnalysisResult):
        """Extract class instantiation that's not in direct assignment (e.g., as parameter)"""
        line_num = getattr(node, 'lineno', 0)
        
        if isinstance(node.func, ast.Name):
            class_name = node.func.id
            
            args = [self._get_arg_representation(arg) for arg in node.args]
            kwargs = {
                kw.arg: self._get_arg_representation(kw.value) 
                for kw in node.keywords if kw.arg
            }
            
            # Resolve full class name using import map
            full_class_name = self._resolve_full_name(class_name)
            
            # Use a synthetic variable name since this isn't assigned to a variable
            var_name = f"<{class_name.lower()}_instance>"
            
            instantiation = ClassInstantiation(
                variable_name=var_name,
                class_name=class_name,
                args=args,
                kwargs=kwargs,
                line_number=line_num,
                full_class_name=full_class_name
            )
            
            result.class_instantiations.append(instantiation)
    
    def _track_method_result_assignment(self, call_node: ast.Call, var_name: str):
        """Track when a variable is assigned the result of a method call"""
        if isinstance(call_node.func, ast.Attribute):
            # For now, we'll use a generic type hint for method results
            # In a more sophisticated system, we could look up the return type
            self.variable_types[var_name] = "method_result"
    
    def _handle_async_with(self, node: ast.AsyncWith, result: AnalysisResult):
        """Handle async with statements and track context manager variables"""
        for item in node.items:
            if item.optional_vars and isinstance(item.optional_vars, ast.Name):
                var_name = item.optional_vars.id
                
                # If the context manager is a method call, track the result type
                if isinstance(item.context_expr, ast.Call) and isinstance(item.context_expr.func, ast.Attribute):
                    # Extract and process the method call
                    self._extract_method_call(item.context_expr, result)
                    self.processed_calls.add(id(item.context_expr))
                    
                    # Track context manager scope for pydantic_ai run_stream calls
                    obj_name = self._get_name_from_node(item.context_expr.func.value)
                    method_name = item.context_expr.func.attr
                    
                    if (obj_name and obj_name in self.variable_types and 
                        'pydantic_ai' in str(self.variable_types[obj_name]) and 
                        method_name == 'run_stream'):
                        
                        # Calculate the scope of this async with block
                        start_line = getattr(node, 'lineno', 0)
                        end_line = getattr(node, 'end_lineno', start_line + 50)  # fallback estimate
                        
                        # For run_stream, the return type is specifically StreamedRunResult
                        # This is the actual return type, not a generic placeholder
                        self.context_manager_vars[var_name] = (start_line, end_line, "pydantic_ai.StreamedRunResult")
    
    def _handle_with(self, node: ast.With, result: AnalysisResult):
        """Handle regular with statements and track context manager variables"""
        for item in node.items:
            if item.optional_vars and isinstance(item.optional_vars, ast.Name):
                var_name = item.optional_vars.id
                
                # If the context manager is a method call, track the result type
                if isinstance(item.context_expr, ast.Call) and isinstance(item.context_expr.func, ast.Attribute):
                    # Extract and process the method call
                    self._extract_method_call(item.context_expr, result)
                    self.processed_calls.add(id(item.context_expr))
                    
                    # Track basic type information
                    self.variable_types[var_name] = "context_manager_result"
    
    def _resolve_full_name(self, name: str) -> Optional[str]:
        """Resolve a name to its full module.name using import map"""
        # Check if it's a direct import mapping
        if name in self.import_map:
            return self.import_map[name]
        
        # Check if it's a dotted name with first part in import map
        parts = name.split('.')
        if len(parts) > 1 and parts[0] in self.import_map:
            base_module = self.import_map[parts[0]]
            return f"{base_module}.{'.'.join(parts[1:])}"
        
        return None


def analyze_ai_script(script_path: str) -> AnalysisResult:
    """Convenience function to analyze a single AI-generated script"""
    analyzer = AIScriptAnalyzer()
    return analyzer.analyze_script(script_path)


if __name__ == "__main__":
    # Example usage
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: python ai_script_analyzer.py <script_path>")
        sys.exit(1)
    
    script_path = sys.argv[1]
    result = analyze_ai_script(script_path)
    
    print(f"Analysis Results for: {result.file_path}")
    print(f"Imports: {len(result.imports)}")
    print(f"Class Instantiations: {len(result.class_instantiations)}")
    print(f"Method Calls: {len(result.method_calls)}")
    print(f"Function Calls: {len(result.function_calls)}")
    print(f"Attribute Accesses: {len(result.attribute_accesses)}")
    
    if result.errors:
        print(f"Errors: {result.errors}")


================================================
FILE: knowledge_graphs/hallucination_reporter.py
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x9d in position 21323: character maps to <undefined>


================================================
FILE: knowledge_graphs/knowledge_graph_validator.py
================================================
"""
Knowledge Graph Validator

Validates AI-generated code against Neo4j knowledge graph containing
repository information. Checks imports, methods, attributes, and parameters.
"""

import asyncio
import logging
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
from neo4j import AsyncGraphDatabase

# Global semaphore to prevent concurrent Neo4j initialization
_neo4j_init_semaphore = asyncio.Semaphore(1)

from ai_script_analyzer import (
    AnalysisResult, ImportInfo, MethodCall, AttributeAccess, 
    FunctionCall, ClassInstantiation
)

logger = logging.getLogger(__name__)


class ValidationStatus(Enum):
    VALID = "VALID"
    INVALID = "INVALID" 
    UNCERTAIN = "UNCERTAIN"
    NOT_FOUND = "NOT_FOUND"


@dataclass
class ValidationResult:
    """Result of validating a single element"""
    status: ValidationStatus
    confidence: float  # 0.0 to 1.0
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    suggestions: List[str] = field(default_factory=list)


@dataclass
class ImportValidation:
    """Validation result for an import"""
    import_info: ImportInfo
    validation: ValidationResult
    available_classes: List[str] = field(default_factory=list)
    available_functions: List[str] = field(default_factory=list)


@dataclass
class MethodValidation:
    """Validation result for a method call"""
    method_call: MethodCall
    validation: ValidationResult
    expected_params: List[str] = field(default_factory=list)
    actual_params: List[str] = field(default_factory=list)
    parameter_validation: ValidationResult = None


@dataclass
class AttributeValidation:
    """Validation result for attribute access"""
    attribute_access: AttributeAccess
    validation: ValidationResult
    expected_type: Optional[str] = None


@dataclass
class FunctionValidation:
    """Validation result for function call"""
    function_call: FunctionCall
    validation: ValidationResult
    expected_params: List[str] = field(default_factory=list)
    actual_params: List[str] = field(default_factory=list)
    parameter_validation: ValidationResult = None


@dataclass
class ClassValidation:
    """Validation result for class instantiation"""
    class_instantiation: ClassInstantiation
    validation: ValidationResult
    constructor_params: List[str] = field(default_factory=list)
    parameter_validation: ValidationResult = None


@dataclass
class ScriptValidationResult:
    """Complete validation results for a script"""
    script_path: str
    analysis_result: AnalysisResult
    import_validations: List[ImportValidation] = field(default_factory=list)
    class_validations: List[ClassValidation] = field(default_factory=list)
    method_validations: List[MethodValidation] = field(default_factory=list)
    attribute_validations: List[AttributeValidation] = field(default_factory=list)
    function_validations: List[FunctionValidation] = field(default_factory=list)
    overall_confidence: float = 0.0
    hallucinations_detected: List[Dict[str, Any]] = field(default_factory=list)


class KnowledgeGraphValidator:
    """Validates code against Neo4j knowledge graph"""
    
    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):
        self.neo4j_uri = neo4j_uri
        self.neo4j_user = neo4j_user
        self.neo4j_password = neo4j_password
        self.driver = None
        
        # Cache for performance
        self.module_cache: Dict[str, List[str]] = {}
        self.class_cache: Dict[str, Dict[str, Any]] = {}
        self.method_cache: Dict[str, List[Dict[str, Any]]] = {}
        self.repo_cache: Dict[str, str] = {}  # module_name -> repo_name
        self.knowledge_graph_modules: Set[str] = set()  # Track modules in knowledge graph
    
    async def initialize(self):
        """Initialize Neo4j connection with deadlock prevention"""
        # Use semaphore to prevent concurrent initialization causing deadlocks
        async with _neo4j_init_semaphore:
            try:
                self.driver = AsyncGraphDatabase.driver(
                    self.neo4j_uri, 
                    auth=(self.neo4j_user, self.neo4j_password)
                )
                
                # Test connection
                async with self.driver.session() as session:
                    await session.run("RETURN 1")
                
                logger.info("Knowledge graph validator initialized")
                
            except Exception as e:
                logger.error(f"Failed to initialize Knowledge graph validator: {e}")
                if self.driver:
                    await self.driver.close()
                    self.driver = None
                raise
    
    async def close(self):
        """Close Neo4j connection"""
        if self.driver:
            await self.driver.close()
    
    async def validate_script(self, analysis_result: AnalysisResult) -> ScriptValidationResult:
        """Validate entire script analysis against knowledge graph"""
        result = ScriptValidationResult(
            script_path=analysis_result.file_path,
            analysis_result=analysis_result
        )
        
        # Validate imports first (builds context for other validations)
        result.import_validations = await self._validate_imports(analysis_result.imports)
        
        # Validate class instantiations
        result.class_validations = await self._validate_class_instantiations(
            analysis_result.class_instantiations
        )
        
        # Validate method calls
        result.method_validations = await self._validate_method_calls(
            analysis_result.method_calls
        )
        
        # Validate attribute accesses
        result.attribute_validations = await self._validate_attribute_accesses(
            analysis_result.attribute_accesses
        )
        
        # Validate function calls
        result.function_validations = await self._validate_function_calls(
            analysis_result.function_calls
        )
        
        # Calculate overall confidence and detect hallucinations
        result.overall_confidence = self._calculate_overall_confidence(result)
        result.hallucinations_detected = self._detect_hallucinations(result)
        
        return result
    
    async def _validate_imports(self, imports: List[ImportInfo]) -> List[ImportValidation]:
        """Validate all imports against knowledge graph"""
        validations = []
        
        for import_info in imports:
            validation = await self._validate_single_import(import_info)
            validations.append(validation)
        
        return validations
    
    async def _validate_single_import(self, import_info: ImportInfo) -> ImportValidation:
        """Validate a single import"""
        # Determine module to search for
        search_module = import_info.module if import_info.is_from_import else import_info.name
        
        # Check cache first
        if search_module in self.module_cache:
            available_files = self.module_cache[search_module]
        else:
            # Query Neo4j for matching modules
            available_files = await self._find_modules(search_module)
            self.module_cache[search_module] = available_files
        
        if available_files:
            # Get available classes and functions from the module
            classes, functions = await self._get_module_contents(search_module)
            
            # Track this module as being in the knowledge graph
            self.knowledge_graph_modules.add(search_module)
            
            # Also track the base module for "from X.Y.Z import ..." patterns
            if '.' in search_module:
                base_module = search_module.split('.')[0]
                self.knowledge_graph_modules.add(base_module)
            
            validation = ValidationResult(
                status=ValidationStatus.VALID,
                confidence=0.9,
                message=f"Module '{search_module}' found in knowledge graph",
                details={"matched_files": available_files, "in_knowledge_graph": True}
            )
            
            return ImportValidation(
                import_info=import_info,
                validation=validation,
                available_classes=classes,
                available_functions=functions
            )
        else:
            # External library - mark as such but don't treat as error
            validation = ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.8,  # High confidence it's external, not an error
                message=f"Module '{search_module}' is external (not in knowledge graph)",
                details={"could_be_external": True, "in_knowledge_graph": False}
            )
            
            return ImportValidation(
                import_info=import_info,
                validation=validation
            )
    
    async def _validate_class_instantiations(self, instantiations: List[ClassInstantiation]) -> List[ClassValidation]:
        """Validate class instantiations"""
        validations = []
        
        for instantiation in instantiations:
            validation = await self._validate_single_class_instantiation(instantiation)
            validations.append(validation)
        
        return validations
    
    async def _validate_single_class_instantiation(self, instantiation: ClassInstantiation) -> ClassValidation:
        """Validate a single class instantiation"""
        class_name = instantiation.full_class_name or instantiation.class_name
        
        # Skip validation for classes not from knowledge graph
        if not self._is_from_knowledge_graph(class_name):
            validation = ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.8,
                message=f"Skipping validation: '{class_name}' is not from knowledge graph"
            )
            return ClassValidation(
                class_instantiation=instantiation,
                validation=validation
            )
        
        # Find class in knowledge graph
        class_info = await self._find_class(class_name)
        
        if not class_info:
            validation = ValidationResult(
                status=ValidationStatus.NOT_FOUND,
                confidence=0.2,
                message=f"Class '{class_name}' not found in knowledge graph"
            )
            return ClassValidation(
                class_instantiation=instantiation,
                validation=validation
            )
        
        # Check constructor parameters (look for __init__ method)
        init_method = await self._find_method(class_name, "__init__")
        
        if init_method:
            param_validation = self._validate_parameters(
                expected_params=init_method.get('params_list', []),
                provided_args=instantiation.args,
                provided_kwargs=instantiation.kwargs
            )
        else:
            param_validation = ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.5,
                message="Constructor parameters not found"
            )
        
        # Use parameter validation result if it failed
        if param_validation.status == ValidationStatus.INVALID:
            validation = ValidationResult(
                status=ValidationStatus.INVALID,
                confidence=param_validation.confidence,
                message=f"Class '{class_name}' found but has invalid constructor parameters: {param_validation.message}",
                suggestions=param_validation.suggestions
            )
        else:
            validation = ValidationResult(
                status=ValidationStatus.VALID,
                confidence=0.8,
                message=f"Class '{class_name}' found in knowledge graph"
            )
        
        return ClassValidation(
            class_instantiation=instantiation,
            validation=validation,
            parameter_validation=param_validation
        )
    
    async def _validate_method_calls(self, method_calls: List[MethodCall]) -> List[MethodValidation]:
        """Validate method calls"""
        validations = []
        
        for method_call in method_calls:
            validation = await self._validate_single_method_call(method_call)
            validations.append(validation)
        
        return validations
    
    async def _validate_single_method_call(self, method_call: MethodCall) -> MethodValidation:
        """Validate a single method call"""
        class_type = method_call.object_type
        
        if not class_type:
            validation = ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.3,
                message=f"Cannot determine object type for '{method_call.object_name}'"
            )
            return MethodValidation(
                method_call=method_call,
                validation=validation
            )
        
        # Skip validation for classes not from knowledge graph
        if not self._is_from_knowledge_graph(class_type):
            validation = ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.8,
                message=f"Skipping validation: '{class_type}' is not from knowledge graph"
            )
            return MethodValidation(
                method_call=method_call,
                validation=validation
            )
        
        # Find method in knowledge graph
        method_info = await self._find_method(class_type, method_call.method_name)
        
        if not method_info:
            # Check for similar method names
            similar_methods = await self._find_similar_methods(class_type, method_call.method_name)
            
            validation = ValidationResult(
                status=ValidationStatus.NOT_FOUND,
                confidence=0.1,
                message=f"Method '{method_call.method_name}' not found on class '{class_type}'",
                suggestions=similar_methods
            )
            return MethodValidation(
                method_call=method_call,
                validation=validation
            )
        
        # Validate parameters
        expected_params = method_info.get('params_list', [])
        param_validation = self._validate_parameters(
            expected_params=expected_params,
            provided_args=method_call.args,
            provided_kwargs=method_call.kwargs
        )
        
        # Use parameter validation result if it failed
        if param_validation.status == ValidationStatus.INVALID:
            validation = ValidationResult(
                status=ValidationStatus.INVALID,
                confidence=param_validation.confidence,
                message=f"Method '{method_call.method_name}' found but has invalid parameters: {param_validation.message}",
                suggestions=param_validation.suggestions
            )
        else:
            validation = ValidationResult(
                status=ValidationStatus.VALID,
                confidence=0.9,
                message=f"Method '{method_call.method_name}' found on class '{class_type}'"
            )
        
        return MethodValidation(
            method_call=method_call,
            validation=validation,
            expected_params=expected_params,
            actual_params=method_call.args + list(method_call.kwargs.keys()),
            parameter_validation=param_validation
        )
    
    async def _validate_attribute_accesses(self, attribute_accesses: List[AttributeAccess]) -> List[AttributeValidation]:
        """Validate attribute accesses"""
        validations = []
        
        for attr_access in attribute_accesses:
            validation = await self._validate_single_attribute_access(attr_access)
            validations.append(validation)
        
        return validations
    
    async def _validate_single_attribute_access(self, attr_access: AttributeAccess) -> AttributeValidation:
        """Validate a single attribute access"""
        class_type = attr_access.object_type
        
        if not class_type:
            validation = ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.3,
                message=f"Cannot determine object type for '{attr_access.object_name}'"
            )
            return AttributeValidation(
                attribute_access=attr_access,
                validation=validation
            )
        
        # Skip validation for classes not from knowledge graph
        if not self._is_from_knowledge_graph(class_type):
            validation = ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.8,
                message=f"Skipping validation: '{class_type}' is not from knowledge graph"
            )
            return AttributeValidation(
                attribute_access=attr_access,
                validation=validation
            )
        
        # Find attribute in knowledge graph
        attr_info = await self._find_attribute(class_type, attr_access.attribute_name)
        
        if not attr_info:
            # If not found as attribute, check if it's a method (for decorators like @agent.tool)
            method_info = await self._find_method(class_type, attr_access.attribute_name)
            
            if method_info:
                validation = ValidationResult(
                    status=ValidationStatus.VALID,
                    confidence=0.8,
                    message=f"'{attr_access.attribute_name}' found as method on class '{class_type}' (likely used as decorator)"
                )
                return AttributeValidation(
                    attribute_access=attr_access,
                    validation=validation,
                    expected_type="method"
                )
            
            validation = ValidationResult(
                status=ValidationStatus.NOT_FOUND,
                confidence=0.2,
                message=f"'{attr_access.attribute_name}' not found on class '{class_type}'"
            )
            return AttributeValidation(
                attribute_access=attr_access,
                validation=validation
            )
        
        validation = ValidationResult(
            status=ValidationStatus.VALID,
            confidence=0.8,
            message=f"Attribute '{attr_access.attribute_name}' found on class '{class_type}'"
        )
        
        return AttributeValidation(
            attribute_access=attr_access,
            validation=validation,
            expected_type=attr_info.get('type')
        )
    
    async def _validate_function_calls(self, function_calls: List[FunctionCall]) -> List[FunctionValidation]:
        """Validate function calls"""
        validations = []
        
        for func_call in function_calls:
            validation = await self._validate_single_function_call(func_call)
            validations.append(validation)
        
        return validations
    
    async def _validate_single_function_call(self, func_call: FunctionCall) -> FunctionValidation:
        """Validate a single function call"""
        func_name = func_call.full_name or func_call.function_name
        
        # Skip validation for functions not from knowledge graph
        if func_call.full_name and not self._is_from_knowledge_graph(func_call.full_name):
            validation = ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.8,
                message=f"Skipping validation: '{func_name}' is not from knowledge graph"
            )
            return FunctionValidation(
                function_call=func_call,
                validation=validation
            )
        
        # Find function in knowledge graph
        func_info = await self._find_function(func_name)
        
        if not func_info:
            validation = ValidationResult(
                status=ValidationStatus.NOT_FOUND,
                confidence=0.2,
                message=f"Function '{func_name}' not found in knowledge graph"
            )
            return FunctionValidation(
                function_call=func_call,
                validation=validation
            )
        
        # Validate parameters
        expected_params = func_info.get('params_list', [])
        param_validation = self._validate_parameters(
            expected_params=expected_params,
            provided_args=func_call.args,
            provided_kwargs=func_call.kwargs
        )
        
        # Use parameter validation result if it failed
        if param_validation.status == ValidationStatus.INVALID:
            validation = ValidationResult(
                status=ValidationStatus.INVALID,
                confidence=param_validation.confidence,
                message=f"Function '{func_name}' found but has invalid parameters: {param_validation.message}",
                suggestions=param_validation.suggestions
            )
        else:
            validation = ValidationResult(
                status=ValidationStatus.VALID,
                confidence=0.8,
                message=f"Function '{func_name}' found in knowledge graph"
            )
        
        return FunctionValidation(
            function_call=func_call,
            validation=validation,
            expected_params=expected_params,
            actual_params=func_call.args + list(func_call.kwargs.keys()),
            parameter_validation=param_validation
        )
    
    def _validate_parameters(self, expected_params: List[str], provided_args: List[str], 
                           provided_kwargs: Dict[str, str]) -> ValidationResult:
        """Validate function/method parameters with comprehensive support"""
        if not expected_params:
            return ValidationResult(
                status=ValidationStatus.UNCERTAIN,
                confidence=0.5,
                message="Parameter information not available"
            )
        
        # Parse expected parameters - handle detailed format
        required_positional = []
        optional_positional = []
        keyword_only_required = []
        keyword_only_optional = []
        has_varargs = False
        has_varkwargs = False
        
        for param in expected_params:
            # Handle detailed format: "[keyword_only] name:type=default" or "name:type"
            param_clean = param.strip()
            
            # Check for parameter kind prefix
            kind = 'positional'
            if param_clean.startswith('['):
                end_bracket = param_clean.find(']')
                if end_bracket > 0:
                    kind = param_clean[1:end_bracket]
                    param_clean = param_clean[end_bracket+1:].strip()
            
            # Check for varargs/varkwargs
            if param_clean.startswith('*') and not param_clean.startswith('**'):
                has_varargs = True
                continue
            elif param_clean.startswith('**'):
                has_varkwargs = True
                continue
            
            # Parse name and check if optional
            if ':' in param_clean:
                param_name = param_clean.split(':')[0]
                is_optional = '=' in param_clean
                
                if kind == 'keyword_only':
                    if is_optional:
                        keyword_only_optional.append(param_name)
                    else:
                        keyword_only_required.append(param_name)
                else:  # positional
                    if is_optional:
                        optional_positional.append(param_name)
                    else:
                        required_positional.append(param_name)
        
        # Count provided parameters
        provided_positional_count = len(provided_args)
        provided_keyword_names = set(provided_kwargs.keys())
        
        # Validate positional arguments
        min_required_positional = len(required_positional)
        max_allowed_positional = len(required_positional) + len(optional_positional)
        
        if not has_varargs and provided_positional_count > max_allowed_positional:
            return ValidationResult(
                status=ValidationStatus.INVALID,
                confidence=0.8,
                message=f"Too many positional arguments: provided {provided_positional_count}, max allowed {max_allowed_positional}"
            )
        
        if provided_positional_count < min_required_positional:
            return ValidationResult(
                status=ValidationStatus.INVALID,
                confidence=0.8,
                message=f"Too few positional arguments: provided {provided_positional_count}, required {min_required_positional}"
            )
        
        # Validate keyword arguments
        all_valid_kwarg_names = set(required_positional + optional_positional + keyword_only_required + keyword_only_optional)
        invalid_kwargs = provided_keyword_names - all_valid_kwarg_names
        
        if invalid_kwargs and not has_varkwargs:
            return ValidationResult(
                status=ValidationStatus.INVALID,
                confidence=0.7,
                message=f"Invalid keyword arguments: {list(invalid_kwargs)}",
                suggestions=[f"Valid parameters: {list(all_valid_kwarg_names)}"]
            )
        
        # Check required keyword-only arguments
        missing_required_kwargs = set(keyword_only_required) - provided_keyword_names
        if missing_required_kwargs:
            return ValidationResult(
                status=ValidationStatus.INVALID,
                confidence=0.8,
                message=f"Missing required keyword arguments: {list(missing_required_kwargs)}"
            )
        
        return ValidationResult(
            status=ValidationStatus.VALID,
            confidence=0.9,
            message="Parameters are valid"
        )
    
    # Neo4j Query Methods
    
    async def _find_modules(self, module_name: str) -> List[str]:
        """Find repository matching the module name, then return its files"""
        async with self.driver.session() as session:
            # First, try to find files with module names that match or start with the search term
            module_query = """
            MATCH (r:Repository)-[:CONTAINS]->(f:File)
            WHERE f.module_name = $module_name 
               OR f.module_name STARTS WITH $module_name + '.'
               OR split(f.module_name, '.')[0] = $module_name
            RETURN DISTINCT r.name as repo_name, count(f) as file_count
            ORDER BY file_count DESC
            LIMIT 5
            """
            
            result = await session.run(module_query, module_name=module_name)
            repos_from_modules = []
            async for record in result:
                repos_from_modules.append(record['repo_name'])
            
            # Also try repository name matching as fallback
            repo_query = """
            MATCH (r:Repository)
            WHERE toLower(r.name) = toLower($module_name)
               OR toLower(replace(r.name, '-', '_')) = toLower($module_name)
               OR toLower(replace(r.name, '_', '-')) = toLower($module_name)
            RETURN r.name as repo_name
            ORDER BY 
                CASE 
                    WHEN toLower(r.name) = toLower($module_name) THEN 1
                    WHEN toLower(replace(r.name, '-', '_')) = toLower($module_name) THEN 2
                    WHEN toLower(replace(r.name, '_', '-')) = toLower($module_name) THEN 3
                END
            LIMIT 5
            """
            
            result = await session.run(repo_query, module_name=module_name)
            repos_from_names = []
            async for record in result:
                repos_from_names.append(record['repo_name'])
            
            # Combine results, prioritizing module-based matches
            all_repos = repos_from_modules + [r for r in repos_from_names if r not in repos_from_modules]
            
            if not all_repos:
                return []
            
            # Get files from the best matching repository
            best_repo = all_repos[0]
            files_query = """
            MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)
            RETURN f.path, f.module_name
            LIMIT 50
            """
            
            result = await session.run(files_query, repo_name=best_repo)
            files = []
            async for record in result:
                files.append(record['f.path'])
            
            return files
    
    async def _get_module_contents(self, module_name: str) -> Tuple[List[str], List[str]]:
        """Get classes and functions available in a repository matching the module name"""
        async with self.driver.session() as session:
            # First, try to find repository by module names in files
            module_query = """
            MATCH (r:Repository)-[:CONTAINS]->(f:File)
            WHERE f.module_name = $module_name 
               OR f.module_name STARTS WITH $module_name + '.'
               OR split(f.module_name, '.')[0] = $module_name
            RETURN DISTINCT r.name as repo_name, count(f) as file_count
            ORDER BY file_count DESC
            LIMIT 1
            """
            
            result = await session.run(module_query, module_name=module_name)
            record = await result.single()
            
            if record:
                repo_name = record['repo_name']
            else:
                # Fallback to repository name matching
                repo_query = """
                MATCH (r:Repository)
                WHERE toLower(r.name) = toLower($module_name)
                   OR toLower(replace(r.name, '-', '_')) = toLower($module_name)
                   OR toLower(replace(r.name, '_', '-')) = toLower($module_name)
                RETURN r.name as repo_name
                ORDER BY 
                    CASE 
                        WHEN toLower(r.name) = toLower($module_name) THEN 1
                        WHEN toLower(replace(r.name, '-', '_')) = toLower($module_name) THEN 2
                        WHEN toLower(replace(r.name, '_', '-')) = toLower($module_name) THEN 3
                    END
                LIMIT 1
                """
                
                result = await session.run(repo_query, module_name=module_name)
                record = await result.single()
                
                if not record:
                    return [], []
                
                repo_name = record['repo_name']
            
            # Get classes from this repository
            class_query = """
            MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)
            RETURN DISTINCT c.name as class_name
            """
            
            result = await session.run(class_query, repo_name=repo_name)
            classes = []
            async for record in result:
                classes.append(record['class_name'])
            
            # Get functions from this repository
            func_query = """
            MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(func:Function)
            RETURN DISTINCT func.name as function_name
            """
            
            result = await session.run(func_query, repo_name=repo_name)
            functions = []
            async for record in result:
                functions.append(record['function_name'])
            
            return classes, functions
    
    async def _find_repository_for_module(self, module_name: str) -> Optional[str]:
        """Find the repository name that matches a module name"""
        if module_name in self.repo_cache:
            return self.repo_cache[module_name]
        
        async with self.driver.session() as session:
            # First, try to find repository by module names in files
            module_query = """
            MATCH (r:Repository)-[:CONTAINS]->(f:File)
            WHERE f.module_name = $module_name 
               OR f.module_name STARTS WITH $module_name + '.'
               OR split(f.module_name, '.')[0] = $module_name
            RETURN DISTINCT r.name as repo_name, count(f) as file_count
            ORDER BY file_count DESC
            LIMIT 1
            """
            
            result = await session.run(module_query, module_name=module_name)
            record = await result.single()
            
            if record:
                repo_name = record['repo_name']
            else:
                # Fallback to repository name matching
                query = """
                MATCH (r:Repository)
                WHERE toLower(r.name) = toLower($module_name)
                   OR toLower(replace(r.name, '-', '_')) = toLower($module_name)
                   OR toLower(replace(r.name, '_', '-')) = toLower($module_name)
                   OR toLower(r.name) CONTAINS toLower($module_name)
                   OR toLower($module_name) CONTAINS toLower(replace(r.name, '-', '_'))
                RETURN r.name as repo_name
                ORDER BY 
                    CASE 
                        WHEN toLower(r.name) = toLower($module_name) THEN 1
                        WHEN toLower(replace(r.name, '-', '_')) = toLower($module_name) THEN 2
                        ELSE 3
                    END
                LIMIT 1
                """
                
                result = await session.run(query, module_name=module_name)
                record = await result.single()
                
                repo_name = record['repo_name'] if record else None
            
            self.repo_cache[module_name] = repo_name
            return repo_name
    
    async def _find_class(self, class_name: str) -> Optional[Dict[str, Any]]:
        """Find class information in knowledge graph"""
        async with self.driver.session() as session:
            # First try exact match
            query = """
            MATCH (c:Class)
            WHERE c.name = $class_name OR c.full_name = $class_name
            RETURN c.name as name, c.full_name as full_name
            LIMIT 1
            """
            
            result = await session.run(query, class_name=class_name)
            record = await result.single()
            
            if record:
                return {
                    'name': record['name'],
                    'full_name': record['full_name']
                }
            
            # If no exact match and class_name has dots, try repository-based search
            if '.' in class_name:
                parts = class_name.split('.')
                module_part = '.'.join(parts[:-1])  # e.g., "pydantic_ai"
                class_part = parts[-1]  # e.g., "Agent"
                
                # Find repository for the module
                repo_name = await self._find_repository_for_module(module_part)
                
                if repo_name:
                    # Search for class within this repository
                    repo_query = """
                    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)
                    WHERE c.name = $class_name
                    RETURN c.name as name, c.full_name as full_name
                    LIMIT 1
                    """
                    
                    result = await session.run(repo_query, repo_name=repo_name, class_name=class_part)
                    record = await result.single()
                    
                    if record:
                        return {
                            'name': record['name'],
                            'full_name': record['full_name']
                        }
            
            return None
    
    async def _find_method(self, class_name: str, method_name: str) -> Optional[Dict[str, Any]]:
        """Find method information for a class"""
        cache_key = f"{class_name}.{method_name}"
        if cache_key in self.method_cache:
            methods = self.method_cache[cache_key]
            return methods[0] if methods else None
        
        async with self.driver.session() as session:
            # First try exact match
            query = """
            MATCH (c:Class)-[:HAS_METHOD]->(m:Method)
            WHERE (c.name = $class_name OR c.full_name = $class_name)
              AND m.name = $method_name
            RETURN m.name as name, m.params_list as params_list, m.params_detailed as params_detailed, 
                   m.return_type as return_type, m.args as args
            LIMIT 1
            """
            
            result = await session.run(query, class_name=class_name, method_name=method_name)
            record = await result.single()
            
            if record:
                # Use detailed params if available, fall back to simple params
                params_to_use = record['params_detailed'] or record['params_list'] or []
                
                method_info = {
                    'name': record['name'],
                    'params_list': params_to_use,
                    'return_type': record['return_type'],
                    'args': record['args'] or []
                }
                self.method_cache[cache_key] = [method_info]
                return method_info
            
            # If no exact match and class_name has dots, try repository-based search
            if '.' in class_name:
                parts = class_name.split('.')
                module_part = '.'.join(parts[:-1])  # e.g., "pydantic_ai"
                class_part = parts[-1]  # e.g., "Agent"
                
                # Find repository for the module
                repo_name = await self._find_repository_for_module(module_part)
                
                if repo_name:
                    # Search for method within this repository's classes
                    repo_query = """
                    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_METHOD]->(m:Method)
                    WHERE c.name = $class_name AND m.name = $method_name
                    RETURN m.name as name, m.params_list as params_list, m.params_detailed as params_detailed,
                           m.return_type as return_type, m.args as args
                    LIMIT 1
                    """
                    
                    result = await session.run(repo_query, repo_name=repo_name, class_name=class_part, method_name=method_name)
                    record = await result.single()
                    
                    if record:
                        # Use detailed params if available, fall back to simple params
                        params_to_use = record['params_detailed'] or record['params_list'] or []
                        
                        method_info = {
                            'name': record['name'],
                            'params_list': params_to_use,
                            'return_type': record['return_type'],
                            'args': record['args'] or []
                        }
                        self.method_cache[cache_key] = [method_info]
                        return method_info
            
            self.method_cache[cache_key] = []
            return None
    
    async def _find_attribute(self, class_name: str, attr_name: str) -> Optional[Dict[str, Any]]:
        """Find attribute information for a class"""
        async with self.driver.session() as session:
            # First try exact match
            query = """
            MATCH (c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)
            WHERE (c.name = $class_name OR c.full_name = $class_name)
              AND a.name = $attr_name
            RETURN a.name as name, a.type as type
            LIMIT 1
            """
            
            result = await session.run(query, class_name=class_name, attr_name=attr_name)
            record = await result.single()
            
            if record:
                return {
                    'name': record['name'],
                    'type': record['type']
                }
            
            # If no exact match and class_name has dots, try repository-based search
            if '.' in class_name:
                parts = class_name.split('.')
                module_part = '.'.join(parts[:-1])  # e.g., "pydantic_ai"
                class_part = parts[-1]  # e.g., "Agent"
                
                # Find repository for the module
                repo_name = await self._find_repository_for_module(module_part)
                
                if repo_name:
                    # Search for attribute within this repository's classes
                    repo_query = """
                    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)
                    WHERE c.name = $class_name AND a.name = $attr_name
                    RETURN a.name as name, a.type as type
                    LIMIT 1
                    """
                    
                    result = await session.run(repo_query, repo_name=repo_name, class_name=class_part, attr_name=attr_name)
                    record = await result.single()
                    
                    if record:
                        return {
                            'name': record['name'],
                            'type': record['type']
                        }
            
            return None
    
    async def _find_function(self, func_name: str) -> Optional[Dict[str, Any]]:
        """Find function information"""
        async with self.driver.session() as session:
            # First try exact match
            query = """
            MATCH (f:Function)
            WHERE f.name = $func_name OR f.full_name = $func_name
            RETURN f.name as name, f.params_list as params_list, f.params_detailed as params_detailed,
                   f.return_type as return_type, f.args as args
            LIMIT 1
            """
            
            result = await session.run(query, func_name=func_name)
            record = await result.single()
            
            if record:
                # Use detailed params if available, fall back to simple params
                params_to_use = record['params_detailed'] or record['params_list'] or []
                
                return {
                    'name': record['name'],
                    'params_list': params_to_use,
                    'return_type': record['return_type'],
                    'args': record['args'] or []
                }
            
            # If no exact match and func_name has dots, try repository-based search
            if '.' in func_name:
                parts = func_name.split('.')
                module_part = '.'.join(parts[:-1])  # e.g., "pydantic_ai"
                func_part = parts[-1]  # e.g., "some_function"
                
                # Find repository for the module
                repo_name = await self._find_repository_for_module(module_part)
                
                if repo_name:
                    # Search for function within this repository
                    repo_query = """
                    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(func:Function)
                    WHERE func.name = $func_name
                    RETURN func.name as name, func.params_list as params_list, func.params_detailed as params_detailed,
                           func.return_type as return_type, func.args as args
                    LIMIT 1
                    """
                    
                    result = await session.run(repo_query, repo_name=repo_name, func_name=func_part)
                    record = await result.single()
                    
                    if record:
                        # Use detailed params if available, fall back to simple params
                        params_to_use = record['params_detailed'] or record['params_list'] or []
                        
                        return {
                            'name': record['name'],
                            'params_list': params_to_use,
                            'return_type': record['return_type'],
                            'args': record['args'] or []
                        }
            
            return None
    
    async def _find_pydantic_ai_result_method(self, method_name: str) -> Optional[Dict[str, Any]]:
        """Find method information for pydantic_ai result objects"""
        # Look for methods on pydantic_ai classes that could be result objects
        async with self.driver.session() as session:
            # Search for common result methods in pydantic_ai repository
            query = """
            MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_METHOD]->(m:Method)
            WHERE m.name = $method_name 
              AND (c.name CONTAINS 'Result' OR c.name CONTAINS 'Stream' OR c.name CONTAINS 'Run')
            RETURN m.name as name, m.params_list as params_list, m.params_detailed as params_detailed,
                   m.return_type as return_type, m.args as args, c.name as class_name
            LIMIT 1
            """
            
            result = await session.run(query, repo_name="pydantic_ai", method_name=method_name)
            record = await result.single()
            
            if record:
                # Use detailed params if available, fall back to simple params
                params_to_use = record['params_detailed'] or record['params_list'] or []
                
                return {
                    'name': record['name'],
                    'params_list': params_to_use,
                    'return_type': record['return_type'],
                    'args': record['args'] or [],
                    'source_class': record['class_name']
                }
            
            return None
    
    async def _find_similar_modules(self, module_name: str) -> List[str]:
        """Find similar repository names for suggestions"""
        async with self.driver.session() as session:
            query = """
            MATCH (r:Repository)
            WHERE toLower(r.name) CONTAINS toLower($partial_name)
               OR toLower(replace(r.name, '-', '_')) CONTAINS toLower($partial_name)
               OR toLower(replace(r.name, '_', '-')) CONTAINS toLower($partial_name)
            RETURN r.name
            LIMIT 5
            """
            
            result = await session.run(query, partial_name=module_name[:3])
            suggestions = []
            async for record in result:
                suggestions.append(record['name'])
            
            return suggestions
    
    async def _find_similar_methods(self, class_name: str, method_name: str) -> List[str]:
        """Find similar method names for suggestions"""
        async with self.driver.session() as session:
            # First try exact class match
            query = """
            MATCH (c:Class)-[:HAS_METHOD]->(m:Method)
            WHERE (c.name = $class_name OR c.full_name = $class_name)
              AND m.name CONTAINS $partial_name
            RETURN m.name as name
            LIMIT 5
            """
            
            result = await session.run(query, class_name=class_name, partial_name=method_name[:3])
            suggestions = []
            async for record in result:
                suggestions.append(record['name'])
            
            # If no suggestions and class_name has dots, try repository-based search
            if not suggestions and '.' in class_name:
                parts = class_name.split('.')
                module_part = '.'.join(parts[:-1])  # e.g., "pydantic_ai"
                class_part = parts[-1]  # e.g., "Agent"
                
                # Find repository for the module
                repo_name = await self._find_repository_for_module(module_part)
                
                if repo_name:
                    repo_query = """
                    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_METHOD]->(m:Method)
                    WHERE c.name = $class_name AND m.name CONTAINS $partial_name
                    RETURN m.name as name
                    LIMIT 5
                    """
                    
                    result = await session.run(repo_query, repo_name=repo_name, class_name=class_part, partial_name=method_name[:3])
                    async for record in result:
                        suggestions.append(record['name'])
            
            return suggestions
    
    def _calculate_overall_confidence(self, result: ScriptValidationResult) -> float:
        """Calculate overall confidence score for the validation (knowledge graph items only)"""
        kg_validations = []
        
        # Only count validations from knowledge graph imports
        for val in result.import_validations:
            if val.validation.details.get('in_knowledge_graph', False):
                kg_validations.append(val.validation.confidence)
        
        # Only count validations from knowledge graph classes
        for val in result.class_validations:
            class_name = val.class_instantiation.full_class_name or val.class_instantiation.class_name
            if self._is_from_knowledge_graph(class_name):
                kg_validations.append(val.validation.confidence)
        
        # Only count validations from knowledge graph methods
        for val in result.method_validations:
            if val.method_call.object_type and self._is_from_knowledge_graph(val.method_call.object_type):
                kg_validations.append(val.validation.confidence)
        
        # Only count validations from knowledge graph attributes
        for val in result.attribute_validations:
            if val.attribute_access.object_type and self._is_from_knowledge_graph(val.attribute_access.object_type):
                kg_validations.append(val.validation.confidence)
        
        # Only count validations from knowledge graph functions
        for val in result.function_validations:
            if val.function_call.full_name and self._is_from_knowledge_graph(val.function_call.full_name):
                kg_validations.append(val.validation.confidence)
        
        if not kg_validations:
            return 1.0  # No knowledge graph items to validate = perfect confidence
        
        return sum(kg_validations) / len(kg_validations)
    
    def _is_from_knowledge_graph(self, class_type: str) -> bool:
        """Check if a class type comes from a module in the knowledge graph"""
        if not class_type:
            return False
        
        # For dotted names like "pydantic_ai.Agent" or "pydantic_ai.StreamedRunResult", check the base module
        if '.' in class_type:
            base_module = class_type.split('.')[0]
            # Exact match only - "pydantic" should not match "pydantic_ai"
            return base_module in self.knowledge_graph_modules
        
        # For simple names, check if any knowledge graph module matches exactly
        # Don't use substring matching to avoid "pydantic" matching "pydantic_ai"
        return class_type in self.knowledge_graph_modules
    
    def _detect_hallucinations(self, result: ScriptValidationResult) -> List[Dict[str, Any]]:
        """Detect and categorize hallucinations"""
        hallucinations = []
        reported_items = set()  # Track reported items to avoid duplicates
        
        # Check method calls (only for knowledge graph classes)
        for val in result.method_validations:
            if (val.validation.status == ValidationStatus.NOT_FOUND and 
                val.method_call.object_type and 
                self._is_from_knowledge_graph(val.method_call.object_type)):
                
                # Create unique key to avoid duplicates
                key = (val.method_call.line_number, val.method_call.method_name, val.method_call.object_type)
                if key not in reported_items:
                    reported_items.add(key)
                    hallucinations.append({
                        'type': 'METHOD_NOT_FOUND',
                        'location': f"line {val.method_call.line_number}",
                        'description': f"Method '{val.method_call.method_name}' not found on class '{val.method_call.object_type}'",
                        'suggestion': val.validation.suggestions[0] if val.validation.suggestions else None
                    })
        
        # Check attributes (only for knowledge graph classes) - but skip if already reported as method
        for val in result.attribute_validations:
            if (val.validation.status == ValidationStatus.NOT_FOUND and 
                val.attribute_access.object_type and 
                self._is_from_knowledge_graph(val.attribute_access.object_type)):
                
                # Create unique key - if this was already reported as a method, skip it
                key = (val.attribute_access.line_number, val.attribute_access.attribute_name, val.attribute_access.object_type)
                if key not in reported_items:
                    reported_items.add(key)
                    hallucinations.append({
                        'type': 'ATTRIBUTE_NOT_FOUND',
                        'location': f"line {val.attribute_access.line_number}",
                        'description': f"Attribute '{val.attribute_access.attribute_name}' not found on class '{val.attribute_access.object_type}'"
                    })
        
        # Check parameter issues (only for knowledge graph methods)
        for val in result.method_validations:
            if (val.parameter_validation and 
                val.parameter_validation.status == ValidationStatus.INVALID and
                val.method_call.object_type and 
                self._is_from_knowledge_graph(val.method_call.object_type)):
                hallucinations.append({
                    'type': 'INVALID_PARAMETERS',
                    'location': f"line {val.method_call.line_number}",
                    'description': f"Invalid parameters for method '{val.method_call.method_name}': {val.parameter_validation.message}"
                })
        
        return hallucinations


================================================
FILE: knowledge_graphs/parse_repo_into_neo4j.py
================================================
"""
Direct Neo4j GitHub Code Repository Extractor

Creates nodes and relationships directly in Neo4j without Graphiti:
- File nodes
- Class nodes  
- Method nodes
- Function nodes
- Import relationships

Bypasses all LLM processing for maximum speed.
"""

import asyncio
import logging
import os
import subprocess
import shutil
from pathlib import Path
from typing import List, Dict, Any, Set
import ast

from dotenv import load_dotenv
from neo4j import AsyncGraphDatabase

# Global semaphore to prevent concurrent Neo4j initialization
_neo4j_init_semaphore = asyncio.Semaphore(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)
logger = logging.getLogger(__name__)


class Neo4jCodeAnalyzer:
    """Analyzes code for direct Neo4j insertion"""
    
    def __init__(self):
        # External modules to ignore
        self.external_modules = {
            # Python standard library
            'os', 'sys', 'json', 'logging', 'datetime', 'pathlib', 'typing', 'collections',
            'asyncio', 'subprocess', 'ast', 're', 'string', 'urllib', 'http', 'email',
            'time', 'uuid', 'hashlib', 'base64', 'itertools', 'functools', 'operator',
            'contextlib', 'copy', 'pickle', 'tempfile', 'shutil', 'glob', 'fnmatch',
            'io', 'codecs', 'locale', 'platform', 'socket', 'ssl', 'threading', 'queue',
            'multiprocessing', 'concurrent', 'warnings', 'traceback', 'inspect',
            'importlib', 'pkgutil', 'types', 'weakref', 'gc', 'dataclasses', 'enum',
            'abc', 'numbers', 'decimal', 'fractions', 'math', 'cmath', 'random', 'statistics',
            
            # Common third-party libraries
            'requests', 'urllib3', 'httpx', 'aiohttp', 'flask', 'django', 'fastapi',
            'pydantic', 'sqlalchemy', 'alembic', 'psycopg2', 'pymongo', 'redis',
            'celery', 'pytest', 'unittest', 'mock', 'faker', 'factory', 'hypothesis',
            'numpy', 'pandas', 'matplotlib', 'seaborn', 'scipy', 'sklearn', 'torch',
            'tensorflow', 'keras', 'opencv', 'pillow', 'boto3', 'botocore', 'azure',
            'google', 'openai', 'anthropic', 'langchain', 'transformers', 'huggingface_hub',
            'click', 'typer', 'rich', 'colorama', 'tqdm', 'python-dotenv', 'pyyaml',
            'toml', 'configargparse', 'marshmallow', 'attrs', 'dataclasses-json',
            'jsonschema', 'cerberus', 'voluptuous', 'schema', 'jinja2', 'mako',
            'cryptography', 'bcrypt', 'passlib', 'jwt', 'authlib', 'oauthlib'
        }
    
    def analyze_python_file(self, file_path: Path, repo_root: Path, project_modules: Set[str]) -> Dict[str, Any]:
        """Extract structure for direct Neo4j insertion"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            relative_path = str(file_path.relative_to(repo_root))
            module_name = self._get_importable_module_name(file_path, repo_root, relative_path)
            
            # Extract structure
            classes = []
            functions = []
            imports = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    # Extract class with its methods and attributes
                    methods = []
                    attributes = []
                    
                    for item in node.body:
                        if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                            if not item.name.startswith('_'):  # Public methods only
                                # Extract comprehensive parameter info
                                params = self._extract_function_parameters(item)
                                
                                # Get return type annotation
                                return_type = self._get_name(item.returns) if item.returns else 'Any'
                                
                                # Create detailed parameter list for Neo4j storage
                                params_detailed = []
                                for p in params:
                                    param_str = f"{p['name']}:{p['type']}"
                                    if p['optional'] and p['default'] is not None:
                                        param_str += f"={p['default']}"
                                    elif p['optional']:
                                        param_str += "=None"
                                    if p['kind'] != 'positional':
                                        param_str = f"[{p['kind']}] {param_str}"
                                    params_detailed.append(param_str)
                                
                                methods.append({
                                    'name': item.name,
                                    'params': params,  # Full parameter objects
                                    'params_detailed': params_detailed,  # Detailed string format
                                    'return_type': return_type,
                                    'args': [arg.arg for arg in item.args.args if arg.arg != 'self']  # Keep for backwards compatibility
                                })
                        elif isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name):
                            # Type annotated attributes
                            if not item.target.id.startswith('_'):
                                attributes.append({
                                    'name': item.target.id,
                                    'type': self._get_name(item.annotation) if item.annotation else 'Any'
                                })
                    
                    classes.append({
                        'name': node.name,
                        'full_name': f"{module_name}.{node.name}",
                        'methods': methods,
                        'attributes': attributes
                    })
                
                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    # Only top-level functions
                    if not any(node in cls_node.body for cls_node in ast.walk(tree) if isinstance(cls_node, ast.ClassDef)):
                        if not node.name.startswith('_'):
                            # Extract comprehensive parameter info
                            params = self._extract_function_parameters(node)
                            
                            # Get return type annotation
                            return_type = self._get_name(node.returns) if node.returns else 'Any'
                            
                            # Create detailed parameter list for Neo4j storage
                            params_detailed = []
                            for p in params:
                                param_str = f"{p['name']}:{p['type']}"
                                if p['optional'] and p['default'] is not None:
                                    param_str += f"={p['default']}"
                                elif p['optional']:
                                    param_str += "=None"
                                if p['kind'] != 'positional':
                                    param_str = f"[{p['kind']}] {param_str}"
                                params_detailed.append(param_str)
                            
                            # Simple format for backwards compatibility
                            params_list = [f"{p['name']}:{p['type']}" for p in params]
                            
                            functions.append({
                                'name': node.name,
                                'full_name': f"{module_name}.{node.name}",
                                'params': params,  # Full parameter objects
                                'params_detailed': params_detailed,  # Detailed string format
                                'params_list': params_list,  # Simple string format for backwards compatibility
                                'return_type': return_type,
                                'args': [arg.arg for arg in node.args.args]  # Keep for backwards compatibility
                            })
                
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    # Track internal imports only
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            if self._is_likely_internal(alias.name, project_modules):
                                imports.append(alias.name)
                    elif isinstance(node, ast.ImportFrom) and node.module:
                        if (node.module.startswith('.') or self._is_likely_internal(node.module, project_modules)):
                            imports.append(node.module)
            
            return {
                'module_name': module_name,
                'file_path': relative_path,
                'classes': classes,
                'functions': functions,
                'imports': list(set(imports)),  # Remove duplicates
                'line_count': len(content.splitlines())
            }
            
        except Exception as e:
            logger.warning(f"Could not analyze {file_path}: {e}")
            return None
    
    def _is_likely_internal(self, import_name: str, project_modules: Set[str]) -> bool:
        """Check if an import is likely internal to the project"""
        if not import_name:
            return False
        
        # Relative imports are definitely internal
        if import_name.startswith('.'):
            return True
        
        # Check if it's a known external module
        base_module = import_name.split('.')[0]
        if base_module in self.external_modules:
            return False
        
        # Check if it matches any project module
        for project_module in project_modules:
            if import_name.startswith(project_module):
                return True
        
        # If it's not obviously external, consider it internal
        if (not any(ext in base_module.lower() for ext in ['test', 'mock', 'fake']) and
            not base_module.startswith('_') and
            len(base_module) > 2):
            return True
        
        return False
    
    def _get_importable_module_name(self, file_path: Path, repo_root: Path, relative_path: str) -> str:
        """Determine the actual importable module name for a Python file"""
        # Start with the default: convert file path to module path
        default_module = relative_path.replace('/', '.').replace('\\', '.').replace('.py', '')
        
        # Common patterns to detect the actual package root
        path_parts = Path(relative_path).parts
        
        # Look for common package indicators
        package_roots = []
        
        # Check each directory level for __init__.py to find package boundaries
        current_path = repo_root
        for i, part in enumerate(path_parts[:-1]):  # Exclude the .py file itself
            current_path = current_path / part
            if (current_path / '__init__.py').exists():
                # This is a package directory, mark it as a potential root
                package_roots.append(i)
        
        if package_roots:
            # Use the first (outermost) package as the root
            package_start = package_roots[0]
            module_parts = path_parts[package_start:]
            module_name = '.'.join(module_parts).replace('.py', '')
            return module_name
        
        # Fallback: look for common Python project structures
        # Skip common non-package directories
        skip_dirs = {'src', 'lib', 'source', 'python', 'pkg', 'packages'}
        
        # Find the first directory that's not in skip_dirs
        filtered_parts = []
        for part in path_parts:
            if part.lower() not in skip_dirs or filtered_parts:  # Once we start including, include everything
                filtered_parts.append(part)
        
        if filtered_parts:
            module_name = '.'.join(filtered_parts).replace('.py', '')
            return module_name
        
        # Final fallback: use the default
        return default_module
    
    def _extract_function_parameters(self, func_node):
        """Comprehensive parameter extraction from function definition"""
        params = []
        
        # Regular positional arguments
        for i, arg in enumerate(func_node.args.args):
            if arg.arg == 'self':
                continue
                
            param_info = {
                'name': arg.arg,
                'type': self._get_name(arg.annotation) if arg.annotation else 'Any',
                'kind': 'positional',
                'optional': False,
                'default': None
            }
            
            # Check if this argument has a default value
            defaults_start = len(func_node.args.args) - len(func_node.args.defaults)
            if i >= defaults_start:
                default_idx = i - defaults_start
                if default_idx < len(func_node.args.defaults):
                    param_info['optional'] = True
                    param_info['default'] = self._get_default_value(func_node.args.defaults[default_idx])
            
            params.append(param_info)
        
        # *args parameter
        if func_node.args.vararg:
            params.append({
                'name': f"*{func_node.args.vararg.arg}",
                'type': self._get_name(func_node.args.vararg.annotation) if func_node.args.vararg.annotation else 'Any',
                'kind': 'var_positional',
                'optional': True,
                'default': None
            })
        
        # Keyword-only arguments (after *)
        for i, arg in enumerate(func_node.args.kwonlyargs):
            param_info = {
                'name': arg.arg,
                'type': self._get_name(arg.annotation) if arg.annotation else 'Any',
                'kind': 'keyword_only',
                'optional': True,  # All kwonly args are optional unless explicitly required
                'default': None
            }
            
            # Check for default value
            if i < len(func_node.args.kw_defaults) and func_node.args.kw_defaults[i] is not None:
                param_info['default'] = self._get_default_value(func_node.args.kw_defaults[i])
            else:
                param_info['optional'] = False  # No default = required kwonly arg
            
            params.append(param_info)
        
        # **kwargs parameter
        if func_node.args.kwarg:
            params.append({
                'name': f"**{func_node.args.kwarg.arg}",
                'type': self._get_name(func_node.args.kwarg.annotation) if func_node.args.kwarg.annotation else 'Dict[str, Any]',
                'kind': 'var_keyword',
                'optional': True,
                'default': None
            })
        
        return params
    
    def _get_default_value(self, default_node):
        """Extract default value from AST node"""
        try:
            if isinstance(default_node, ast.Constant):
                return repr(default_node.value)
            elif isinstance(default_node, ast.Name):
                return default_node.id
            elif isinstance(default_node, ast.Attribute):
                return self._get_name(default_node)
            elif isinstance(default_node, ast.List):
                return "[]"
            elif isinstance(default_node, ast.Dict):
                return "{}"
            else:
                return "..."
        except Exception:
            return "..."
    
    def _get_name(self, node):
        """Extract name from AST node, handling complex types safely"""
        if node is None:
            return "Any"
        
        try:
            if isinstance(node, ast.Name):
                return node.id
            elif isinstance(node, ast.Attribute):
                if hasattr(node, 'value'):
                    return f"{self._get_name(node.value)}.{node.attr}"
                else:
                    return node.attr
            elif isinstance(node, ast.Subscript):
                # Handle List[Type], Dict[K,V], etc.
                base = self._get_name(node.value)
                if hasattr(node, 'slice'):
                    if isinstance(node.slice, ast.Name):
                        return f"{base}[{node.slice.id}]"
                    elif isinstance(node.slice, ast.Tuple):
                        elts = [self._get_name(elt) for elt in node.slice.elts]
                        return f"{base}[{', '.join(elts)}]"
                    elif isinstance(node.slice, ast.Constant):
                        return f"{base}[{repr(node.slice.value)}]"
                    elif isinstance(node.slice, ast.Attribute):
                        return f"{base}[{self._get_name(node.slice)}]"
                    elif isinstance(node.slice, ast.Subscript):
                        return f"{base}[{self._get_name(node.slice)}]"
                    else:
                        # Try to get the name of the slice, fallback to Any if it fails
                        try:
                            slice_name = self._get_name(node.slice)
                            return f"{base}[{slice_name}]"
                        except:
                            return f"{base}[Any]"
                return base
            elif isinstance(node, ast.Constant):
                return str(node.value)
            elif isinstance(node, ast.Str):  # Python < 3.8
                return f'"{node.s}"'
            elif isinstance(node, ast.Tuple):
                elts = [self._get_name(elt) for elt in node.elts]
                return f"({', '.join(elts)})"
            elif isinstance(node, ast.List):
                elts = [self._get_name(elt) for elt in node.elts]
                return f"[{', '.join(elts)}]"
            else:
                # Fallback for complex types - return a simple string representation
                return "Any"
        except Exception:
            # If anything goes wrong, return a safe default
            return "Any"


class DirectNeo4jExtractor:
    """Creates nodes and relationships directly in Neo4j"""
    
    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):
        self.neo4j_uri = neo4j_uri
        self.neo4j_user = neo4j_user
        self.neo4j_password = neo4j_password
        self.driver = None
        self.analyzer = Neo4jCodeAnalyzer()
    
    async def initialize(self):
        """Initialize Neo4j connection with deadlock prevention"""
        # Use semaphore to prevent concurrent initialization causing deadlocks
        async with _neo4j_init_semaphore:
            logger.info("Initializing Neo4j connection...")
            
            try:
                self.driver = AsyncGraphDatabase.driver(
                    self.neo4j_uri, 
                    auth=(self.neo4j_user, self.neo4j_password)
                )
                
                # Test connection first
                async with self.driver.session() as session:
                    await session.run("RETURN 1")
                
                # Create constraints and indexes (protected by semaphore)
                logger.info("Creating constraints and indexes...")
                async with self.driver.session() as session:
                    # Create constraints - using MERGE-friendly approach
                    await session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (f:File) REQUIRE f.path IS UNIQUE")
                    await session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Class) REQUIRE c.full_name IS UNIQUE")
                    # Remove unique constraints for methods/attributes since they can be duplicated across classes
                    # await session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (m:Method) REQUIRE m.full_name IS UNIQUE")
                    # await session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (f:Function) REQUIRE f.full_name IS UNIQUE")
                    # await session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (a:Attribute) REQUIRE a.full_name IS UNIQUE")
                    
                    # Create indexes for performance
                    await session.run("CREATE INDEX IF NOT EXISTS FOR (f:File) ON (f.name)")
                    await session.run("CREATE INDEX IF NOT EXISTS FOR (c:Class) ON (c.name)")
                    await session.run("CREATE INDEX IF NOT EXISTS FOR (m:Method) ON (m.name)")
                
                logger.info("Neo4j initialized successfully")
                
            except Exception as e:
                logger.error(f"Failed to initialize Neo4j: {e}")
                if self.driver:
                    await self.driver.close()
                    self.driver = None
                raise
    
    async def clear_repository_data(self, repo_name: str):
        """Clear all data for a specific repository"""
        logger.info(f"Clearing existing data for repository: {repo_name}")
        async with self.driver.session() as session:
            # Delete in specific order to avoid constraint issues
            
            # 1. Delete methods and attributes (they depend on classes)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_METHOD]->(m:Method)
                DETACH DELETE m
            """, repo_name=repo_name)
            
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)
                DETACH DELETE a
            """, repo_name=repo_name)
            
            # 2. Delete functions (they depend on files)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(func:Function)
                DETACH DELETE func
            """, repo_name=repo_name)
            
            # 3. Delete classes (they depend on files)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)
                DETACH DELETE c
            """, repo_name=repo_name)
            
            # 4. Delete files (they depend on repository)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)
                DETACH DELETE f
            """, repo_name=repo_name)
            
            # 5. Finally delete the repository
            await session.run("""
                MATCH (r:Repository {name: $repo_name})
                DETACH DELETE r
            """, repo_name=repo_name)
            
        logger.info(f"Cleared data for repository: {repo_name}")
    
    async def close(self):
        """Close Neo4j connection"""
        if self.driver:
            await self.driver.close()
    
    def clone_repo(self, repo_url: str, target_dir: str) -> str:
        """Clone repository with shallow clone"""
        logger.info(f"Cloning repository to: {target_dir}")
        if os.path.exists(target_dir):
            logger.info(f"Removing existing directory: {target_dir}")
            try:
                def handle_remove_readonly(func, path, exc):
                    try:
                        if os.path.exists(path):
                            os.chmod(path, 0o777)
                            func(path)
                    except PermissionError:
                        logger.warning(f"Could not remove {path} - file in use, skipping")
                        pass
                shutil.rmtree(target_dir, onerror=handle_remove_readonly)
            except Exception as e:
                logger.warning(f"Could not fully remove {target_dir}: {e}. Proceeding anyway...")
        
        logger.info(f"Running git clone from {repo_url}")
        subprocess.run(['git', 'clone', '--depth', '1', repo_url, target_dir], check=True)
        logger.info("Repository cloned successfully")
        return target_dir
    
    def get_python_files(self, repo_path: str) -> List[Path]:
        """Get Python files, focusing on main source directories"""
        python_files = []
        exclude_dirs = {
            'tests', 'test', '__pycache__', '.git', 'venv', 'env',
            'node_modules', 'build', 'dist', '.pytest_cache', 'docs',
            'examples', 'example', 'demo', 'benchmark'
        }
        
        for root, dirs, files in os.walk(repo_path):
            dirs[:] = [d for d in dirs if d not in exclude_dirs and not d.startswith('.')]
            
            for file in files:
                if file.endswith('.py') and not file.startswith('test_'):
                    file_path = Path(root) / file
                    if (file_path.stat().st_size < 500_000 and 
                        file not in ['setup.py', 'conftest.py']):
                        python_files.append(file_path)
        
        return python_files
    
    async def analyze_repository(self, repo_url: str, temp_dir: str = None):
        """Analyze repository and create nodes/relationships in Neo4j"""
        repo_name = repo_url.split('/')[-1].replace('.git', '')
        logger.info(f"Analyzing repository: {repo_name}")
        
        # Clear existing data for this repository before re-processing
        await self.clear_repository_data(repo_name)
        
        # Set default temp_dir to repos folder at script level
        if temp_dir is None:
            script_dir = Path(__file__).parent
            temp_dir = str(script_dir / "repos" / repo_name)
        
        # Clone and analyze
        repo_path = Path(self.clone_repo(repo_url, temp_dir))
        
        try:
            logger.info("Getting Python files...")
            python_files = self.get_python_files(str(repo_path))
            logger.info(f"Found {len(python_files)} Python files to analyze")
            
            # First pass: identify project modules
            logger.info("Identifying project modules...")
            project_modules = set()
            for file_path in python_files:
                relative_path = str(file_path.relative_to(repo_path))
                module_parts = relative_path.replace('/', '.').replace('.py', '').split('.')
                if len(module_parts) > 0 and not module_parts[0].startswith('.'):
                    project_modules.add(module_parts[0])
            
            logger.info(f"Identified project modules: {sorted(project_modules)}")
            
            # Second pass: analyze files and collect data
            logger.info("Analyzing Python files...")
            modules_data = []
            for i, file_path in enumerate(python_files):
                if i % 20 == 0:
                    logger.info(f"Analyzing file {i+1}/{len(python_files)}: {file_path.name}")
                
                analysis = self.analyzer.analyze_python_file(file_path, repo_path, project_modules)
                if analysis:
                    modules_data.append(analysis)
            
            logger.info(f"Found {len(modules_data)} files with content")
            
            # Create nodes and relationships in Neo4j
            logger.info("Creating nodes and relationships in Neo4j...")
            await self._create_graph(repo_name, modules_data)
            
            # Print summary
            total_classes = sum(len(mod['classes']) for mod in modules_data)
            total_methods = sum(len(cls['methods']) for mod in modules_data for cls in mod['classes'])
            total_functions = sum(len(mod['functions']) for mod in modules_data)
            total_imports = sum(len(mod['imports']) for mod in modules_data)
            
            print(f"\\n=== Direct Neo4j Repository Analysis for {repo_name} ===")
            print(f"Files processed: {len(modules_data)}")
            print(f"Classes created: {total_classes}")
            print(f"Methods created: {total_methods}")
            print(f"Functions created: {total_functions}")
            print(f"Import relationships: {total_imports}")
            
            logger.info(f"Successfully created Neo4j graph for {repo_name}")
            
        finally:
            if os.path.exists(temp_dir):
                logger.info(f"Cleaning up temporary directory: {temp_dir}")
                try:
                    def handle_remove_readonly(func, path, exc):
                        try:
                            if os.path.exists(path):
                                os.chmod(path, 0o777)
                                func(path)
                        except PermissionError:
                            logger.warning(f"Could not remove {path} - file in use, skipping")
                            pass
                    
                    shutil.rmtree(temp_dir, onerror=handle_remove_readonly)
                    logger.info("Cleanup completed")
                except Exception as e:
                    logger.warning(f"Cleanup failed: {e}. Directory may remain at {temp_dir}")
                    # Don't fail the whole process due to cleanup issues
    
    async def _create_graph(self, repo_name: str, modules_data: List[Dict]):
        """Create all nodes and relationships in Neo4j"""
        
        async with self.driver.session() as session:
            # Create Repository node
            await session.run(
                "CREATE (r:Repository {name: $repo_name, created_at: datetime()})",
                repo_name=repo_name
            )
            
            nodes_created = 0
            relationships_created = 0
            
            for i, mod in enumerate(modules_data):
                # 1. Create File node
                await session.run("""
                    CREATE (f:File {
                        name: $name,
                        path: $path,
                        module_name: $module_name,
                        line_count: $line_count,
                        created_at: datetime()
                    })
                """, 
                    name=mod['file_path'].split('/')[-1],
                    path=mod['file_path'],
                    module_name=mod['module_name'],
                    line_count=mod['line_count']
                )
                nodes_created += 1
                
                # 2. Connect File to Repository
                await session.run("""
                    MATCH (r:Repository {name: $repo_name})
                    MATCH (f:File {path: $file_path})
                    CREATE (r)-[:CONTAINS]->(f)
                """, repo_name=repo_name, file_path=mod['file_path'])
                relationships_created += 1
                
                # 3. Create Class nodes and relationships
                for cls in mod['classes']:
                    # Create Class node using MERGE to avoid duplicates
                    await session.run("""
                        MERGE (c:Class {full_name: $full_name})
                        ON CREATE SET c.name = $name, c.created_at = datetime()
                    """, name=cls['name'], full_name=cls['full_name'])
                    nodes_created += 1
                    
                    # Connect File to Class
                    await session.run("""
                        MATCH (f:File {path: $file_path})
                        MATCH (c:Class {full_name: $class_full_name})
                        MERGE (f)-[:DEFINES]->(c)
                    """, file_path=mod['file_path'], class_full_name=cls['full_name'])
                    relationships_created += 1
                    
                    # 4. Create Method nodes - use MERGE to avoid duplicates
                    for method in cls['methods']:
                        method_full_name = f"{cls['full_name']}.{method['name']}"
                        # Create method with unique ID to avoid conflicts
                        method_id = f"{cls['full_name']}::{method['name']}"
                        
                        await session.run("""
                            MERGE (m:Method {method_id: $method_id})
                            ON CREATE SET m.name = $name, 
                                         m.full_name = $full_name,
                                         m.args = $args,
                                         m.params_list = $params_list,
                                         m.params_detailed = $params_detailed,
                                         m.return_type = $return_type,
                                         m.created_at = datetime()
                        """, 
                            name=method['name'], 
                            full_name=method_full_name,
                            method_id=method_id,
                            args=method['args'],
                            params_list=[f"{p['name']}:{p['type']}" for p in method['params']],  # Simple format
                            params_detailed=method.get('params_detailed', []),  # Detailed format
                            return_type=method['return_type']
                        )
                        nodes_created += 1
                        
                        # Connect Class to Method
                        await session.run("""
                            MATCH (c:Class {full_name: $class_full_name})
                            MATCH (m:Method {method_id: $method_id})
                            MERGE (c)-[:HAS_METHOD]->(m)
                        """, 
                            class_full_name=cls['full_name'], 
                            method_id=method_id
                        )
                        relationships_created += 1
                    
                    # 5. Create Attribute nodes - use MERGE to avoid duplicates
                    for attr in cls['attributes']:
                        attr_full_name = f"{cls['full_name']}.{attr['name']}"
                        # Create attribute with unique ID to avoid conflicts
                        attr_id = f"{cls['full_name']}::{attr['name']}"
                        await session.run("""
                            MERGE (a:Attribute {attr_id: $attr_id})
                            ON CREATE SET a.name = $name,
                                         a.full_name = $full_name,
                                         a.type = $type,
                                         a.created_at = datetime()
                        """, 
                            name=attr['name'], 
                            full_name=attr_full_name,
                            attr_id=attr_id,
                            type=attr['type']
                        )
                        nodes_created += 1
                        
                        # Connect Class to Attribute
                        await session.run("""
                            MATCH (c:Class {full_name: $class_full_name})
                            MATCH (a:Attribute {attr_id: $attr_id})
                            MERGE (c)-[:HAS_ATTRIBUTE]->(a)
                        """, 
                            class_full_name=cls['full_name'], 
                            attr_id=attr_id
                        )
                        relationships_created += 1
                
                # 6. Create Function nodes (top-level) - use MERGE to avoid duplicates
                for func in mod['functions']:
                    func_id = f"{mod['file_path']}::{func['name']}"
                    await session.run("""
                        MERGE (f:Function {func_id: $func_id})
                        ON CREATE SET f.name = $name,
                                     f.full_name = $full_name,
                                     f.args = $args,
                                     f.params_list = $params_list,
                                     f.params_detailed = $params_detailed,
                                     f.return_type = $return_type,
                                     f.created_at = datetime()
                    """, 
                        name=func['name'], 
                        full_name=func['full_name'],
                        func_id=func_id,
                        args=func['args'],
                        params_list=func.get('params_list', []),  # Simple format for backwards compatibility
                        params_detailed=func.get('params_detailed', []),  # Detailed format
                        return_type=func['return_type']
                    )
                    nodes_created += 1
                    
                    # Connect File to Function
                    await session.run("""
                        MATCH (file:File {path: $file_path})
                        MATCH (func:Function {func_id: $func_id})
                        MERGE (file)-[:DEFINES]->(func)
                    """, file_path=mod['file_path'], func_id=func_id)
                    relationships_created += 1
                
                # 7. Create Import relationships
                for import_name in mod['imports']:
                    # Try to find the target file
                    await session.run("""
                        MATCH (source:File {path: $source_path})
                        OPTIONAL MATCH (target:File) 
                        WHERE target.module_name = $import_name OR target.module_name STARTS WITH $import_name
                        WITH source, target
                        WHERE target IS NOT NULL
                        MERGE (source)-[:IMPORTS]->(target)
                    """, source_path=mod['file_path'], import_name=import_name)
                    relationships_created += 1
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(modules_data)} files...")
            
            logger.info(f"Created {nodes_created} nodes and {relationships_created} relationships")
    
    async def search_graph(self, query_type: str, **kwargs):
        """Search the Neo4j graph directly"""
        async with self.driver.session() as session:
            if query_type == "files_importing":
                target = kwargs.get('target')
                result = await session.run("""
                    MATCH (source:File)-[:IMPORTS]->(target:File)
                    WHERE target.module_name CONTAINS $target
                    RETURN source.path as file, target.module_name as imports
                """, target=target)
                return [{"file": record["file"], "imports": record["imports"]} async for record in result]
            
            elif query_type == "classes_in_file":
                file_path = kwargs.get('file_path')
                result = await session.run("""
                    MATCH (f:File {path: $file_path})-[:DEFINES]->(c:Class)
                    RETURN c.name as class_name, c.full_name as full_name
                """, file_path=file_path)
                return [{"class_name": record["class_name"], "full_name": record["full_name"]} async for record in result]
            
            elif query_type == "methods_of_class":
                class_name = kwargs.get('class_name')
                result = await session.run("""
                    MATCH (c:Class)-[:HAS_METHOD]->(m:Method)
                    WHERE c.name CONTAINS $class_name OR c.full_name CONTAINS $class_name
                    RETURN m.name as method_name, m.args as args
                """, class_name=class_name)
                return [{"method_name": record["method_name"], "args": record["args"]} async for record in result]


async def main():
    """Example usage"""
    load_dotenv()
    
    neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')
    neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')
    neo4j_password = os.environ.get('NEO4J_PASSWORD', 'password')
    
    extractor = DirectNeo4jExtractor(neo4j_uri, neo4j_user, neo4j_password)
    
    try:
        await extractor.initialize()
        
        # Analyze repository - direct Neo4j, no LLM processing!
        # repo_url = "https://github.com/pydantic/pydantic-ai.git"
        repo_url = "https://github.com/getzep/graphiti.git"
        await extractor.analyze_repository(repo_url)
        
        # Direct graph queries
        print("\\n=== Direct Neo4j Queries ===")
        
        # Which files import from models?
        results = await extractor.search_graph("files_importing", target="models")
        print(f"\\nFiles importing from 'models': {len(results)}")
        for result in results[:3]:
            print(f"- {result['file']} imports {result['imports']}")
        
        # What classes are in a specific file?
        results = await extractor.search_graph("classes_in_file", file_path="pydantic_ai/models/openai.py")
        print(f"\\nClasses in openai.py: {len(results)}")
        for result in results:
            print(f"- {result['class_name']}")
        
        # What methods does OpenAIModel have?
        results = await extractor.search_graph("methods_of_class", class_name="OpenAIModel")
        print(f"\\nMethods of OpenAIModel: {len(results)}")
        for result in results[:5]:
            print(f"- {result['method_name']}({', '.join(result['args'])})")
    
    finally:
        await extractor.close()


if __name__ == "__main__":
    asyncio.run(main())


================================================
FILE: knowledge_graphs/query_knowledge_graph.py
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x8d in position 1989: character maps to <undefined>


================================================
FILE: PRPs/README.md
================================================
# Product Requirement Prompt (PRP) Concept

"Over-specifying what to build while under-specifying the context, and how to build it, is why so many AI-driven coding attempts stall at 80%. A Product Requirement Prompt (PRP) fixes that by fusing the disciplined scope of a classic Product Requirements Document (PRD) with the “context-is-king” mindset of modern prompt engineering."

## What is a PRP?

Product Requirement Prompt (PRP)
A PRP is a structured prompt that supplies an AI coding agent with everything it needs to deliver a vertical slice of working software—no more, no less.

### How it differs from a PRD

A traditional PRD clarifies what the product must do and why customers need it, but deliberately avoids how it will be built.

A PRP keeps the goal and justification sections of a PRD yet adds three AI-critical layers:

### Context

- Precise file paths and content, library versions and library context, code snippets examples. LLMs generate higher-quality code when given direct, in-prompt references instead of broad descriptions. Usage of a ai_docs/ directory to pipe in library and other docs.

### Implementation Details and Strategy

- In contrast of a traditional PRD, a PRP explicitly states how the product will be built. This includes the use of API endpoints, test runners, or agent patterns (ReAct, Plan-and-Execute) to use. Usage of typehints, dependencies, architectural patterns and other tools to ensure the code is built correctly.

### Validation Gates

- Deterministic checks such as pytest, ruff, or static type passes “Shift-left” quality controls catch defects early and are cheaper than late re-work.
  Example: Each new funtion should be individaully tested, Validation gate = all tests pass.

### PRP Layer Why It Exists

- The PRP folder is used to prepare and pipe PRPs to the agentic coder.

## Why context is non-negotiable

Large-language-model outputs are bounded by their context window; irrelevant or missing context literally squeezes out useful tokens

The industry mantra “Garbage In → Garbage Out” applies doubly to prompt engineering and especially in agentic engineering: sloppy input yields brittle code

## In short

A PRP is PRD + curated codebase intelligence + agent/runbook—the minimum viable packet an AI needs to plausibly ship production-ready code on the first pass.

The PRP can be small and focusing on a single task or large and covering multiple tasks.
The true power of PRP is in the ability to chain tasks together in a PRP to build, self-validate and ship complex features.



================================================
FILE: PRPs/ai_docs/github_cloning_best_practices.md
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x9d in position 5778: character maps to <undefined>


================================================
FILE: PRPs/ai_docs/python_ast_docstring_extraction.md
================================================
# Python AST Docstring Extraction Best Practices

This document provides comprehensive guidance for extracting docstrings from Python files using the AST module, specifically for the smart_crawl_github multi-file type enhancement.

## Official Documentation

- **Python 3.13 AST Documentation**: https://docs.python.org/3/library/ast.html
- **Source Code**: https://github.com/python/cpython/tree/3.13/Lib/ast.py
- **Key Function**: `ast.get_docstring(node, clean=True)` - Official built-in function for docstring extraction

## Core Implementation Pattern

```python
import ast
from typing import List, Dict, Any, Optional

class PythonDocstringExtractor:
    """Extract docstrings from Python files using AST parsing."""
    
    def extract_from_file(self, file_path: str) -> Dict[str, Any]:
        """Extract all docstrings from a Python file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source = f.read()
            
            tree = ast.parse(source, filename=file_path)
            docstrings = []
            
            # Extract module docstring
            module_doc = ast.get_docstring(tree, clean=True)
            if module_doc:
                docstrings.append({
                    'type': 'module',
                    'name': file_path,
                    'docstring': module_doc,
                    'lineno': 1,
                    'signature': None,
                    'parent': None
                })
            
            # Extract class and function docstrings
            for node in ast.walk(tree):
                self._extract_node_docstring(node, docstrings)
            
            return {
                'docstrings': docstrings,
                'success': True,
                'metadata': {
                    'total_functions': len([d for d in docstrings if d['type'] == 'function']),
                    'total_classes': len([d for d in docstrings if d['type'] == 'class']),
                    'total_methods': len([d for d in docstrings if d['type'] == 'method'])
                }
            }
            
        except SyntaxError as e:
            return {
                'success': False,
                'error': f'Syntax error at line {e.lineno}: {e.msg}',
                'docstrings': []
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'docstrings': []
            }
    
    def _extract_node_docstring(self, node: ast.AST, docstrings: List[Dict]) -> None:
        """Extract docstring from AST node."""
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            docstring = ast.get_docstring(node, clean=True)
            if docstring:
                docstrings.append({
                    'type': 'function',  # or 'method' if inside class
                    'name': node.name,
                    'docstring': docstring,
                    'lineno': node.lineno,
                    'signature': self._extract_signature(node),
                    'is_async': isinstance(node, ast.AsyncFunctionDef)
                })
        
        elif isinstance(node, ast.ClassDef):
            docstring = ast.get_docstring(node, clean=True)
            if docstring:
                docstrings.append({
                    'type': 'class',
                    'name': node.name,
                    'docstring': docstring,
                    'lineno': node.lineno,
                    'signature': None,
                    'bases': [self._ast_to_string(base) for base in node.bases]
                })
    
    def _extract_signature(self, node: ast.FunctionDef) -> str:
        """Extract function signature with type annotations."""
        try:
            args = []
            for arg in node.args.args:
                arg_str = arg.arg
                if arg.annotation:
                    arg_str += f": {ast.unparse(arg.annotation)}"
                args.append(arg_str)
            
            signature = f"({', '.join(args)})"
            if node.returns:
                signature += f" -> {ast.unparse(node.returns)}"
            
            return signature
        except Exception:
            return "(signature_extraction_failed)"
    
    def _ast_to_string(self, node: ast.AST) -> str:
        """Convert AST node to string."""
        try:
            return ast.unparse(node)
        except Exception:
            return str(type(node).__name__)
```

## Error Handling Requirements

1. **MANDATORY**: Use `ast.get_docstring()` - never regex for Python docstrings
2. **Handle SyntaxError**: Files with syntax errors should be skipped gracefully
3. **Handle encoding issues**: Use UTF-8 with error handling
4. **Preserve file processing**: Skip problematic files but continue processing others

## Metadata Structure for RAG

```python
{
    'content': docstring_text,           # Clean docstring content
    'file_path': 'src/module.py',        # Source file path
    'type': 'function',                  # module/class/function/method
    'name': 'function_name',             # Symbol name
    'signature': 'func(arg: int) -> str', # Function signature with types
    'lineno': 45,                        # Line number in source
    'parent_class': 'ClassName',         # For methods only
    'is_async': False,                   # For async functions
    'language': 'python'                 # File type marker
}
```

## Integration with Chunking Pipeline

- **Individual chunks**: Each docstring becomes a separate chunk
- **Rich metadata**: Include file path, symbol name, signature for context
- **Content focus**: Index the docstring content, not the code
- **Context preservation**: Metadata provides full context for RAG queries

## Performance Considerations

- **AST parsing**: Fast and memory-efficient for most Python files
- **Error isolation**: Skip problematic files without affecting others
- **Lazy evaluation**: Only parse files that pass basic validation checks
- **Size limits**: Skip extremely large Python files (>1MB recommended)

## Security Notes

- **AST parsing is safe**: No code execution, unlike `eval()` or `exec()`
- **File size limits**: Prevent DoS attacks with large files
- **Encoding safety**: Handle encoding errors gracefully
- **Path validation**: Ensure file paths are within expected directories


================================================
FILE: PRPs/ai_docs/typescript_jsdoc_parsing.md
================================================
# TypeScript JSDoc/TSDoc Parsing Approaches

This document provides comprehensive guidance for extracting JSDoc/TSDoc comments from TypeScript files for the smart_crawl_github multi-file type enhancement.

## Official Documentation

- **TSDoc Specification**: https://tsdoc.org/
- **JSDoc Documentation**: https://jsdoc.app/
- **TypeScript Handbook**: https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html

## Comment Format Standards

### JSDoc Format
```typescript
/**
 * Calculates the area of a rectangle.
 * @param width - The width of the rectangle
 * @param height - The height of the rectangle
 * @returns The area of the rectangle
 * @example
 * ```typescript
 * const area = calculateArea(5, 10);
 * console.log(area); // 50
 * ```
 */
function calculateArea(width: number, height: number): number {
    return width * height;
}
```

### TSDoc Format
```typescript
/**
 * A class representing a database connection.
 * 
 * @public
 */
export class DatabaseConnection {
    /**
     * Connects to the database using the provided configuration.
     * 
     * @param config - The database configuration object
     * @returns A promise that resolves when the connection is established
     * 
     * @throws {@link ConnectionError}
     * When the connection cannot be established
     * 
     * @example
     * ```typescript
     * const db = new DatabaseConnection();
     * await db.connect({ host: 'localhost', port: 5432 });
     * ```
     */
    async connect(config: DatabaseConfig): Promise<void> {
        // Implementation
    }
}
```

## Implementation Approaches

### 1. Regex-Based Approach (Recommended for Simplicity)

```python
import re
from typing import List, Dict, Any, Optional
from pathlib import Path

class TypeScriptDocExtractor:
    """Extract JSDoc/TSDoc comments from TypeScript files using regex."""
    
    # Comprehensive regex pattern for JSDoc comments
    JSDOC_PATTERN = re.compile(
        r'/\*\*\s*\n((?:\s*\*[^\n]*\n)*)\s*\*/',
        re.MULTILINE | re.DOTALL
    )
    
    # Pattern to match function/class/interface declarations
    DECLARATION_PATTERNS = {
        'function': re.compile(
            r'(?:export\s+)?(?:async\s+)?function\s+(\w+)\s*\([^)]*\)(?:\s*:\s*[^{]+)?',
            re.MULTILINE
        ),
        'class': re.compile(
            r'(?:export\s+)?(?:abstract\s+)?class\s+(\w+)(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?',
            re.MULTILINE
        ),
        'interface': re.compile(
            r'(?:export\s+)?interface\s+(\w+)(?:\s+extends\s+[\w,\s]+)?',
            re.MULTILINE
        ),
        'method': re.compile(
            r'(?:async\s+)?(\w+)\s*\([^)]*\)(?:\s*:\s*[^{]+)?',
            re.MULTILINE
        )
    }
    
    def extract_from_file(self, file_path: str) -> Dict[str, Any]:
        """Extract JSDoc comments from TypeScript file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Find all JSDoc comments
            comments = self._extract_jsdoc_comments(content)
            
            # Associate comments with declarations
            documented_items = self._associate_comments_with_declarations(content, comments)
            
            return {
                'documented_items': documented_items,
                'success': True,
                'metadata': {
                    'total_comments': len(comments),
                    'total_functions': len([item for item in documented_items if item['type'] == 'function']),
                    'total_classes': len([item for item in documented_items if item['type'] == 'class']),
                    'total_interfaces': len([item for item in documented_items if item['type'] == 'interface'])
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'documented_items': []
            }
    
    def _extract_jsdoc_comments(self, content: str) -> List[Dict[str, Any]]:
        """Extract all JSDoc comments from content."""
        comments = []
        
        for match in self.JSDOC_PATTERN.finditer(content):
            comment_text = match.group(1)
            start_pos = match.start()
            
            # Clean up comment text
            lines = comment_text.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line.startswith('*'):
                    line = line[1:].strip()
                if line:
                    cleaned_lines.append(line)
            
            cleaned_comment = '\n'.join(cleaned_lines)
            
            # Calculate line number
            line_number = content[:start_pos].count('\n') + 1
            
            comments.append({
                'text': cleaned_comment,
                'start_pos': start_pos,
                'end_pos': match.end(),
                'line_number': line_number
            })
        
        return comments
    
    def _associate_comments_with_declarations(self, content: str, comments: List[Dict]) -> List[Dict[str, Any]]:
        """Associate JSDoc comments with their corresponding declarations."""
        documented_items = []
        
        for comment in comments:
            # Look for declaration after the comment
            after_comment = content[comment['end_pos']:]
            declaration = self._find_next_declaration(after_comment)
            
            if declaration:
                documented_items.append({
                    'type': declaration['type'],
                    'name': declaration['name'],
                    'signature': declaration.get('signature', ''),
                    'docstring': comment['text'],
                    'line_number': comment['line_number'],
                    'language': 'typescript'
                })
        
        return documented_items
    
    def _find_next_declaration(self, content: str) -> Optional[Dict[str, Any]]:
        """Find the next function/class/interface declaration."""
        # Remove leading whitespace and newlines
        content = content.lstrip()
        
        # Try each declaration pattern
        for decl_type, pattern in self.DECLARATION_PATTERNS.items():
            match = pattern.search(content)
            if match and match.start() < 200:  # Must be close to comment
                return {
                    'type': decl_type,
                    'name': match.group(1),
                    'signature': match.group(0)
                }
        
        return None

# Enhanced regex patterns for better matching
class EnhancedTypeScriptDocExtractor(TypeScriptDocExtractor):
    """Enhanced extractor with more sophisticated patterns."""
    
    # More comprehensive function pattern
    FUNCTION_PATTERN = re.compile(
        r'(?:export\s+)?(?:async\s+)?(?:function\s+(\w+)|(?:const|let)\s+(\w+)\s*=\s*(?:async\s+)?\([^)]*\)\s*=>)',
        re.MULTILINE
    )
    
    # Arrow function pattern
    ARROW_FUNCTION_PATTERN = re.compile(
        r'(?:export\s+)?(?:const|let)\s+(\w+)\s*=\s*(?:async\s+)?\([^)]*\)\s*:\s*[^=]*=>\s*{',
        re.MULTILINE
    )
    
    # Method pattern (inside classes)
    METHOD_PATTERN = re.compile(
        r'(?:public|private|protected)?\s*(?:async\s+)?(\w+)\s*\([^)]*\)(?:\s*:\s*[^{]+)?',
        re.MULTILINE
    )
```

### 2. Tree-sitter Approach (Recommended for Production)

```python
try:
    from tree_sitter import Language, Parser
    TREE_SITTER_AVAILABLE = True
except ImportError:
    TREE_SITTER_AVAILABLE = False

class TreeSitterTypeScriptExtractor:
    """Extract JSDoc using Tree-sitter for accurate parsing."""
    
    def __init__(self):
        if not TREE_SITTER_AVAILABLE:
            raise ImportError("tree-sitter not available. Install with: pip install tree-sitter")
        
        # Note: You need to build the TypeScript grammar
        # See: https://github.com/tree-sitter/tree-sitter-typescript
        try:
            TS_LANGUAGE = Language.build_library(
                'build/languages.so',
                ['tree-sitter-typescript/typescript']
            )
            self.parser = Parser()
            self.parser.set_language(TS_LANGUAGE)
        except Exception as e:
            raise RuntimeError(f"Failed to load TypeScript grammar: {e}")
    
    def extract_from_file(self, file_path: str) -> Dict[str, Any]:
        """Extract documentation using Tree-sitter parsing."""
        try:
            with open(file_path, 'rb') as f:
                source_code = f.read()
            
            tree = self.parser.parse(source_code)
            root_node = tree.root_node
            
            documented_items = []
            self._traverse_tree(root_node, source_code, documented_items)
            
            return {
                'documented_items': documented_items,
                'success': True,
                'metadata': {
                    'parser': 'tree-sitter',
                    'total_items': len(documented_items)
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'documented_items': []
            }
    
    def _traverse_tree(self, node, source_code: bytes, documented_items: List[Dict]):
        """Traverse AST and extract documented items."""
        # Look for comment nodes followed by declarations
        if node.type == 'comment' and '/**' in node.text.decode('utf-8'):
            # Process JSDoc comment
            comment_text = self._clean_jsdoc_comment(node.text.decode('utf-8'))
            
            # Find next sibling that's a declaration
            next_sibling = node.next_sibling
            if next_sibling and next_sibling.type in ['function_declaration', 'class_declaration', 'interface_declaration']:
                documented_items.append({
                    'type': next_sibling.type.replace('_declaration', ''),
                    'name': self._extract_name_from_node(next_sibling, source_code),
                    'docstring': comment_text,
                    'line_number': node.start_point[0] + 1,
                    'language': 'typescript'
                })
        
        # Recurse through child nodes
        for child in node.children:
            self._traverse_tree(child, source_code, documented_items)
```

### 3. Hybrid Approach (Recommended Implementation)

```python
class HybridTypeScriptExtractor:
    """Hybrid approach using regex with Tree-sitter fallback."""
    
    def __init__(self):
        self.regex_extractor = TypeScriptDocExtractor()
        self.tree_sitter_available = TREE_SITTER_AVAILABLE
        if self.tree_sitter_available:
            try:
                self.tree_sitter_extractor = TreeSitterTypeScriptExtractor()
            except Exception:
                self.tree_sitter_available = False
    
    def extract_from_file(self, file_path: str) -> Dict[str, Any]:
        """Extract using best available method."""
        # Try Tree-sitter first for accuracy
        if self.tree_sitter_available:
            try:
                result = self.tree_sitter_extractor.extract_from_file(file_path)
                if result['success']:
                    return result
            except Exception:
                pass  # Fall back to regex
        
        # Fallback to regex approach
        return self.regex_extractor.extract_from_file(file_path)
```

## Error Handling Best Practices

```python
def robust_typescript_extraction(file_path: str) -> Dict[str, Any]:
    """Robust TypeScript documentation extraction."""
    try:
        # Check file size
        file_size = Path(file_path).stat().st_size
        if file_size > 5 * 1024 * 1024:  # 5MB limit
            return {
                'success': False,
                'error': f'File too large: {file_size / 1024 / 1024:.1f}MB',
                'documented_items': []
            }
        
        # Check if file is minified
        with open(file_path, 'r', encoding='utf-8') as f:
            first_line = f.readline()
            if len(first_line) > 1000 and '\n' not in first_line[:1000]:
                return {
                    'success': False,
                    'error': 'File appears to be minified',
                    'documented_items': []
                }
        
        # Extract documentation
        extractor = HybridTypeScriptExtractor()
        return extractor.extract_from_file(file_path)
        
    except UnicodeDecodeError:
        return {
            'success': False,
            'error': 'File encoding not supported',
            'documented_items': []
        }
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'documented_items': []
        }
```

## Metadata Structure for RAG

```python
{
    'content': 'Calculates the area of a rectangle...',  # Clean JSDoc content
    'file_path': 'src/utils.ts',                         # Source file path
    'type': 'function',                                  # function/class/interface/method
    'name': 'calculateArea',                             # Symbol name
    'signature': 'calculateArea(width: number, height: number): number',  # Full signature
    'line_number': 15,                                   # Line number in source
    'language': 'typescript',                            # File type marker
    'tags': ['param', 'returns', 'example']            # JSDoc tags present
}
```

## Performance and Security Notes

- **Regex approach**: Fast but may miss edge cases
- **Tree-sitter approach**: Accurate but requires grammar installation
- **File size limits**: Skip large files (>5MB) to prevent performance issues
- **Minification detection**: Skip minified files as they lack meaningful documentation
- **Encoding handling**: Handle non-UTF-8 files gracefully
- **Memory management**: Use streaming for very large files if needed

## Installation Requirements

For enhanced Tree-sitter support:
```bash
pip install tree-sitter
git clone https://github.com/tree-sitter/tree-sitter-typescript
# Build grammar according to tree-sitter documentation
```

For basic regex approach: No additional dependencies required.


================================================
FILE: PRPs/scripts/prp_runner.py
================================================
#!/usr/bin/env -S uv run --script
"""Run an AI coding agent against a PRP.

KISS version - no repo-specific assumptions.

Typical usage:
    uv run RUNNERS/claude_runner.py --prp test --interactive
    uv run RUNNERS/claude_runner.py --prp test --output-format json
    uv run RUNNERS/claude_runner.py --prp test --output-format stream-json

Arguments:
    --prp-path       Path to a PRP markdown file (overrides --prp)
    --prp            Feature key; resolves to PRPs/{feature}.md
    --model          CLI executable for the LLM (default: "claude") Only Claude Code is supported for now
    --interactive    Pass through to run the model in chat mode; otherwise headless.
    --output-format  Output format for headless mode: text, json, stream-json (default: text)
"""

from __future__ import annotations

import argparse
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, Iterator

ROOT = Path(__file__).resolve().parent.parent  # project root

META_HEADER = """Ingest and understand the Product Requirement Prompt (PRP) below in detail.

    # WORKFLOW GUIDANCE:

    ## Planning Phase
    - Think hard before you code. Create a comprehensive plan addressing all requirements.
    - Break down complex tasks into smaller, manageable steps.
    - Use the TodoWrite tool to create and track your implementation plan.
    - Identify implementation patterns from existing code to follow.

    ## Implementation Phase
    - Follow code conventions and patterns found in existing files.
    - Implement one component at a time and verify it works correctly.
    - Write clear, maintainable code with appropriate comments.
    - Consider error handling, edge cases, and potential security issues.
    - Use type hints to ensure type safety.

    ## Testing Phase
    - Test each component thoroughly as you build it.
    - Use the provided validation gates to verify your implementation.
    - Verify that all requirements have been satisfied.
    - Run the project tests when finished and output "DONE" when they pass.

    ## Example Implementation Approach:
    1. Analyze the PRP requirements in detail
    2. Search for and understand existing patterns in the codebase
    3. Search the Web and gather additional context and examples
    4. Create a step-by-step implementation plan with TodoWrite
    5. Implement core functionality first, then additional features
    6. Test and validate each component
    7. Ensure all validation gates pass

    ***When you are finished, move the completed PRP to the PRPs/completed folder***
    """


def build_prompt(prp_path: Path) -> str:
    return META_HEADER + prp_path.read_text()


def stream_json_output(process: subprocess.Popen) -> Iterator[Dict[str, Any]]:
    """Parse streaming JSON output line by line."""
    for line in process.stdout:
        line = line.strip()
        if line:
            try:
                yield json.loads(line)
            except json.JSONDecodeError as e:
                print(f"Warning: Failed to parse JSON line: {e}", file=sys.stderr)
                print(f"Line content: {line}", file=sys.stderr)


def handle_json_output(output: str) -> Dict[str, Any]:
    """Parse the JSON output from Claude Code."""
    try:
        return json.loads(output)
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON output: {e}", file=sys.stderr)
        return {"error": "Failed to parse JSON output", "raw": output}


def run_model(
    prompt: str,
    model: str = "claude",
    interactive: bool = False,
    output_format: str = "text",
) -> None:
    if interactive:
        # Chat mode: feed prompt via STDIN, no -p flag so the user can continue the session.
        cmd = [
            model,
            "--allowedTools",
            "Edit,Bash,Write,MultiEdit,NotebookEdit,WebFetch,Agent,LS,Grep,Read,NotebookRead,TodoRead,TodoWrite,WebSearch",
        ]
        subprocess.run(cmd, input=prompt.encode(), check=True)
    else:
        # Headless: pass prompt via -p for non-interactive mode
        cmd = [
            model,
            "-p",  # This is the --print flag for non-interactive mode
            prompt,
            "--allowedTools",
            "Edit,Bash,Write,MultiEdit,NotebookEdit,WebFetch,Agent,LS,Grep,Read,NotebookRead,TodoRead,TodoWrite,WebSearch",
            # "--max-turns",
            # "30",  # Safety limit for headless mode uncomment if needed
            "--output-format",
            output_format,
        ]

        if output_format == "stream-json":
            # Handle streaming JSON output
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,  # Line buffered
            )

            try:
                for message in stream_json_output(process):
                    # Process each message as it arrives
                    if (
                        message.get("type") == "system"
                        and message.get("subtype") == "init"
                    ):
                        print(
                            f"Session started: {message.get('session_id')}",
                            file=sys.stderr,
                        )
                    elif message.get("type") == "assistant":
                        print(
                            f"Assistant: {message.get('message', {}).get('content', '')[:100]}...",
                            file=sys.stderr,
                        )
                    elif message.get("type") == "result":
                        print("\nFinal result:", file=sys.stderr)
                        print(
                            f"  Success: {message.get('subtype') == 'success'}",
                            file=sys.stderr,
                        )
                        print(
                            f"  Cost: ${message.get('cost_usd', 0):.4f}",
                            file=sys.stderr,
                        )
                        print(
                            f"  Duration: {message.get('duration_ms', 0)}ms",
                            file=sys.stderr,
                        )
                        print(
                            f"  Turns: {message.get('num_turns', 0)}", file=sys.stderr
                        )
                        if message.get("result"):
                            print(
                                f"\nResult text:\n{message.get('result')}",
                                file=sys.stderr,
                            )

                    # Print the full message for downstream processing
                    print(json.dumps(message))

                # Wait for process to complete
                process.wait()
                if process.returncode != 0:
                    stderr = process.stderr.read()
                    print(
                        f"Claude Code failed with exit code {process.returncode}",
                        file=sys.stderr,
                    )
                    print(f"Error: {stderr}", file=sys.stderr)
                    sys.exit(process.returncode)

            except KeyboardInterrupt:
                process.terminate()
                print("\nInterrupted by user", file=sys.stderr)
                sys.exit(1)

        elif output_format == "json":
            # Handle complete JSON output
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                print(
                    f"Claude Code failed with exit code {result.returncode}",
                    file=sys.stderr,
                )
                print(f"Error: {result.stderr}", file=sys.stderr)
                sys.exit(result.returncode)

            # Parse and pretty print the JSON
            json_data = handle_json_output(result.stdout)
            print(json.dumps(json_data, indent=2))

            # Print summary to stderr for user visibility
            if isinstance(json_data, dict):
                if json_data.get("type") == "result":
                    print("\nSummary:", file=sys.stderr)
                    print(
                        f"  Success: {not json_data.get('is_error', False)}",
                        file=sys.stderr,
                    )
                    print(
                        f"  Cost: ${json_data.get('cost_usd', 0):.4f}", file=sys.stderr
                    )
                    print(
                        f"  Duration: {json_data.get('duration_ms', 0)}ms",
                        file=sys.stderr,
                    )
                    print(
                        f"  Session: {json_data.get('session_id', 'unknown')}",
                        file=sys.stderr,
                    )

        else:
            # Default text output
            subprocess.run(cmd, check=True)


def main() -> None:
    parser = argparse.ArgumentParser(description="Run a PRP with an LLM agent.")
    parser.add_argument(
        "--prp-path", help="Relative path to PRP file eg: PRPs/feature.md"
    )
    parser.add_argument(
        "--prp", help="The file name of the PRP without the .md extension eg: feature"
    )
    parser.add_argument(
        "--interactive", action="store_true", help="Launch interactive chat session"
    )
    parser.add_argument("--model", default="claude", help="Model CLI executable name")
    parser.add_argument(
        "--output-format",
        choices=["text", "json", "stream-json"],
        default="text",
        help="Output format for headless mode (default: text)",
    )
    args = parser.parse_args()

    if not args.prp_path and not args.prp:
        sys.exit("Must supply --prp or --prp-path")

    prp_path = Path(args.prp_path) if args.prp_path else ROOT / f"PRPs/{args.prp}.md"
    if not prp_path.exists():
        sys.exit(f"PRP not found: {prp_path}")

    os.chdir(ROOT)  # ensure relative paths match PRP expectations
    prompt = build_prompt(prp_path)
    run_model(
        prompt,
        model=args.model,
        interactive=args.interactive,
        output_format=args.output_format,
    )


if __name__ == "__main__":
    main()



================================================
FILE: PRPs/templates/prp_base.md
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x9d in position 6350: character maps to <undefined>


================================================
FILE: PRPs/templates/prp_base_typescript.md
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x9d in position 7362: character maps to <undefined>


================================================
FILE: PRPs/templates/prp_planning.md
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x9d in position 6879: character maps to <undefined>


================================================
FILE: PRPs/templates/prp_spec.md
================================================
# Specification Template (prompt inspired by IndyDevDan)

> Ingest the information from this file, implement the Low-Level Tasks, and generate the code that will satisfy the High and Mid-Level Objectives.

## High-Level Objective

- [High level goal goes here - what do you want to build?]

## Mid-Level Objective

- [List of mid-level objectives - what are the steps to achieve the high-level objective?]
- [Each objective should be concrete and measurable]
- [But not too detailed - save details for implementation notes]

## Implementation Notes

- [Important technical details - what are the important technical details?]
- [Dependencies and requirements - what are the dependencies and requirements?]
- [Coding standards to follow - what are the coding standards to follow?]
- [Other technical guidance - what are other technical guidance?]

## Context

### Beginning context

- [List of files that exist at start - what files exist at start?]

### Ending context

- [List of files that will exist at end - what files will exist at end?]

## Low-Level Tasks

> Ordered from start to finish

1. [First task - what is the first task?]

```
What prompt would you run to complete this task?
What file do you want to CREATE or UPDATE?
What function do you want to CREATE or UPDATE?
What are details you want to add to drive the code changes?
```

2. [Second task - what is the second task?]

```
What prompt would you run to complete this task?
What file do you want to CREATE or UPDATE?
What function do you want to CREATE or UPDATE?
What are details you want to add to drive the code changes?
```

3. [Third task - what is the third task?]

```
What prompt would you run to complete this task?
What file do you want to CREATE or UPDATE?
What function do you want to CREATE or UPDATE?
What are details you want to add to drive the code changes?
```



================================================
FILE: PRPs/templates/prp_task.md
================================================
---
Intended for Jira/GitHub tasks or other task management systems to break down and plan the implementation.
---

# Task Template v2 - Information Dense with Validation Loops

> Concise, executable tasks with embedded context and validation commands

## Format

```
[ACTION] path/to/file:
  - [OPERATION]: [DETAILS]
  - VALIDATE: [COMMAND]
  - IF_FAIL: [DEBUG_HINT]
```

## Actions keywords to use when creating tasks for concise and meaningful descriptions

- **READ**: Understand existing patterns
- **CREATE**: New file with specific content
- **UPDATE**: Modify existing file
- **DELETE**: Remove file/code
- **FIND**: Search for patterns
- **TEST**: Verify behavior
- **FIX**: Debug and repair

## Critical Context Section

```yaml
# Include these BEFORE tasks when context is crucial
context:
  docs:
    - url: [API documentation]
      focus: [specific method/section]

  patterns:
    - file: existing/example.py
      copy: [pattern name]

  gotchas:
    - issue: "Library X requires Y"
      fix: "Always do Z first"
```

## Task Examples with Validation

### Setup Tasks

```
READ src/config/settings.py:
  - UNDERSTAND: Current configuration structure
  - FIND: Model configuration pattern
  - NOTE: Config uses pydantic BaseSettings

READ tests/test_models.py:
  - UNDERSTAND: Test pattern for models
  - FIND: Fixture setup approach
  - NOTE: Uses pytest-asyncio for async tests
```

### Implementation Tasks

````
UPDATE path/to/file:
  - FIND: MODEL_REGISTRY = {
  - ADD: "new-model": NewModelClass,
  - VALIDATE: python -c "from path/to/file import MODEL_REGISTRY; assert 'new-model' in MODEL_REGISTRY"
  - IF_FAIL: Check import statement for NewModelClass

CREATE path/to/file:
  - COPY_PATTERN: path/to/other/file
  - IMPLEMENT:
   - [Detailed description of what needs to be implemented based on codebase intelligence]
  - VALIDATE: uv run pytest path/to/file -v

UPDATE path/to/file:
  - FIND: app.include_router(
  - ADD_AFTER:
    ```python
    from .endpoints import new_model_router
    app.include_router(new_model_router, prefix="/api/v1")
    ```
  - VALIDATE: uv run pytest path/to/file -v
````

## Validation Checkpoints

```
CHECKPOINT syntax:
  - RUN: ruff check && mypy .
  - FIX: Any reported issues
  - CONTINUE: Only when clean

CHECKPOINT tests:
  - RUN: uv run pytest path/to/file -v
  - REQUIRE: All passing
  - DEBUG: uv run pytest -vvs path/to/file/failing_test.py
  - CONTINUE: Only when all green

CHECKPOINT integration:
  - START: docker-compose up -d
  - RUN: ./scripts/integration_test.sh
  - EXPECT: "All tests passed"
  - CLEANUP: docker-compose down
```

## Debug Patterns

```
DEBUG import_error:
  - CHECK: File exists at path
  - CHECK: __init__.py in all parent dirs
  - TRY: python -c "import path/to/file"
  - FIX: Add to PYTHONPATH or fix import

DEBUG test_failure:
  - RUN: uv run pytest -vvs path/to/test.py::test_name
  - ADD: print(f"Debug: {variable}")
  - IDENTIFY: Assertion vs implementation issue
  - FIX: Update test or fix code

DEBUG api_error:
  - CHECK: Server running (ps aux | grep uvicorn)
  - TEST: curl http://localhost:8000/health
  - READ: Server logs for stack trace
  - FIX: Based on specific error
```

## Common Task examples

### Add New Feature

```
1. READ existing similar feature
2. CREATE new feature file (COPY pattern)
3. UPDATE registry/router to include
4. CREATE tests for feature
5. TEST all tests pass
6. FIX any linting/type issues
7. TEST integration works
```

### Fix Bug

```
1. CREATE failing test that reproduces bug
2. TEST confirm test fails
3. READ relevant code to understand
4. UPDATE code with fix
5. TEST confirm test now passes
6. TEST no other tests broken
7. UPDATE changelog
```

### Refactor Code

```
1. TEST current tests pass (baseline)
2. CREATE new structure (don't delete old yet)
3. UPDATE one usage to new structure
4. TEST still passes
5. UPDATE remaining usages incrementally
6. DELETE old structure
7. TEST full suite passes
```

## Tips for Effective Tasks

- Use VALIDATE after every change
- Include IF_FAIL hints for common issues
- Reference specific line numbers or patterns
- Keep validation commands simple and fast
- Chain related tasks with clear dependencies
- Always include rollback/undo steps for risky changes



================================================
FILE: scripts/README.md
================================================
# Qdrant Database Scripts

This directory contains utility scripts for managing the Qdrant vector database.

## clean_qdrant.py

A comprehensive script for cleaning and managing Qdrant collections.

### Prerequisites

Make sure the Qdrant server is running:
```bash
docker-compose up -d qdrant
```

### Usage Examples

#### List all collections
```bash
python scripts/clean_qdrant.py --list
```

#### Clean all collections (with backup)
```bash
python scripts/clean_qdrant.py --backup --all
```

#### Clean a specific collection
```bash
python scripts/clean_qdrant.py --collection crawled_pages
```

#### Recreate all collections (complete reset)
```bash
python scripts/clean_qdrant.py --recreate
```

#### Force operation without confirmation
```bash
python scripts/clean_qdrant.py --force --all
```

### Features

- **Safe Operations**: Includes confirmation prompts and backup options
- **Backup Support**: Can create JSON backups of collection metadata
- **Batch Processing**: Efficiently handles large collections
- **Detailed Logging**: Comprehensive logging of all operations
- **Collection Recreation**: Can delete and recreate collections with fresh configuration
- **Error Handling**: Robust error handling and recovery

### Safety Notes

- Always use `--backup` flag when cleaning important data
- Use `--list` first to see what collections exist
- The script preserves collection structure when cleaning (only removes data)
- Use `--recreate` only when you want to completely reset collections


================================================
FILE: scripts/benchmark_embedding_cache.py
================================================
#!/usr/bin/env python3
"""
Benchmark script for Redis embedding cache performance.

This script demonstrates the performance benefits of the Redis embedding cache
by measuring cache operations and simulating API call reductions.
"""
import sys
import time
import statistics
from pathlib import Path

# Add src to path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from embedding_cache import EmbeddingCache


def benchmark_cache_operations():
    """Benchmark basic cache operations."""
    print("Redis Embedding Cache Performance Benchmark")
    print("=" * 50)
    
    # Initialize cache
    cache = EmbeddingCache()
    health = cache.health_check()
    
    if health['status'] != 'healthy':
        print(f"Redis not available: {health}")
        return
    
    print(f"Redis connected: {health['latency_ms']:.2f}ms initial latency")
    print()
    
    # Test data
    test_texts = [f"Test embedding text {i}" for i in range(100)]
    model = "benchmark-model"
    embeddings = {text: [float(i)] * 1536 for i, text in enumerate(test_texts)}
    
    # Benchmark cache writes
    print("Benchmarking Cache Writes...")
    write_times = []
    batch_sizes = [1, 5, 10, 25, 50, 100]
    
    for batch_size in batch_sizes:
        batch_texts = test_texts[:batch_size]
        batch_embeddings = {text: embeddings[text] for text in batch_texts}
        
        start_time = time.time()
        cache.set_batch(batch_embeddings, model, ttl=60)
        end_time = time.time()
        
        duration_ms = (end_time - start_time) * 1000
        write_times.append(duration_ms)
        
        print(f"  Batch size {batch_size:3d}: {duration_ms:6.2f}ms ({duration_ms/batch_size:.2f}ms per item)")
    
    print()
    
    # Benchmark cache reads
    print("Benchmarking Cache Reads...")
    read_times = []
    
    for batch_size in batch_sizes:
        batch_texts = test_texts[:batch_size]
        
        # Warm up cache
        cache.get_batch(batch_texts, model)
        
        # Measure read performance
        times = []
        for _ in range(10):  # Multiple runs for accuracy
            start_time = time.time()
            result = cache.get_batch(batch_texts, model)
            end_time = time.time()
            
            duration_ms = (end_time - start_time) * 1000
            times.append(duration_ms)
            
            # Verify all items were found
            assert len(result) == batch_size, f"Expected {batch_size} items, got {len(result)}"
        
        avg_time = statistics.mean(times)
        read_times.append(avg_time)
        
        print(f"  Batch size {batch_size:3d}: {avg_time:6.2f}ms ({avg_time/batch_size:.2f}ms per item)")
    
    print()
    
    # Performance summary
    print("Performance Summary")
    print("-" * 30)
    print(f"â€¢ Average write latency: {statistics.mean(write_times):.2f}ms")
    print(f"â€¢ Average read latency:  {statistics.mean(read_times):.2f}ms")
    print(f"â€¢ Best read performance: {min(read_times):.2f}ms")
    print(f"â€¢ Single item read:      {read_times[0]:.2f}ms")
    print()
    
    # Simulate API cost savings
    print("Simulated Cost Savings")
    print("-" * 30)
    
    # Assume different cache hit rates
    hit_rates = [0.5, 0.7, 0.8, 0.9, 0.95]
    monthly_embeddings = 1000000  # 1M embeddings per month
    cost_per_1k = 0.02  # $0.02 per 1K embeddings (text-embedding-3-small)
    
    for hit_rate in hit_rates:
        api_calls_saved = monthly_embeddings * hit_rate
        cost_saved = (api_calls_saved / 1000) * cost_per_1k
        
        print(f"  {hit_rate*100:2.0f}% hit rate: {api_calls_saved:8,.0f} API calls saved = ${cost_saved:5.2f}/month")
    
    print()
    
    # Memory usage estimate
    embedding_size = 1536 * 4  # 1536 floats * 4 bytes each = 6KB per embedding
    cache_entries = [10000, 50000, 100000, 250000]
    
    print("Memory Usage Estimates")
    print("-" * 30)
    
    for entries in cache_entries:
        memory_mb = (entries * embedding_size) / (1024 * 1024)
        ttl_hours = 24
        
        print(f"  {entries:6,} embeddings: {memory_mb:5.1f}MB (TTL: {ttl_hours}h)")
    
    print()
    print("Recommendations")
    print("-" * 30)
    print("* Target 70-85% cache hit rate for optimal cost savings")
    print("* Monitor Redis memory usage and adjust TTL as needed")
    print("* Use batch operations when possible for better performance")
    print("* Circuit breaker provides automatic failover protection")


if __name__ == "__main__":
    try:
        benchmark_cache_operations()
    except KeyboardInterrupt:
        print("\nBenchmark interrupted by user")
    except Exception as e:
        print(f"Benchmark failed: {e}")
        sys.exit(1)


================================================
FILE: scripts/clean_qdrant.py
================================================
#!/usr/bin/env python3
"""
Qdrant Database Cleanup Script

This script safely cleans all data from Qdrant collections while preserving
the collection structure. It provides options for:
- Cleaning all collections
- Cleaning specific collections
- Backing up data before cleaning
- Recreating collections with fresh configuration

Usage:
    python clean_qdrant.py --all                    # Clean all collections
    python clean_qdrant.py --collection crawled_pages  # Clean specific collection
    python clean_qdrant.py --recreate               # Delete and recreate all collections
    python clean_qdrant.py --backup --all           # Backup before cleaning all
"""

import os
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path

# Add src directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QdrantCleaner:
    """Qdrant database cleanup utility"""
    
    def __init__(self):
        """Initialize the cleaner with Qdrant connection"""
        try:
            self.client = QdrantClient(
                host=os.getenv("QDRANT_HOST", "localhost"),
                port=int(os.getenv("QDRANT_PORT", "6333"))
            )
            logger.info("Connected to Qdrant successfully")
        except Exception as e:
            logger.error(f"Failed to connect to Qdrant: {e}")
            sys.exit(1)
    
    def list_collections(self):
        """List all existing collections"""
        try:
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]
            logger.info(f"Found collections: {collection_names}")
            return collection_names
        except Exception as e:
            logger.error(f"Failed to list collections: {e}")
            return []
    
    def get_collection_info(self, collection_name):
        """Get detailed information about a collection"""
        try:
            info = self.client.get_collection(collection_name)
            count = self.client.count(collection_name)
            return {
                "name": collection_name,
                "vectors_count": count.count,
                "config": info.config,
                "status": info.status
            }
        except Exception as e:
            logger.error(f"Failed to get info for collection {collection_name}: {e}")
            return None
    
    def backup_collection(self, collection_name, backup_dir="backups"):
        """Create a backup of collection metadata and optionally data"""
        try:
            # Create backup directory
            backup_path = Path(backup_dir)
            backup_path.mkdir(exist_ok=True)
            
            # Create timestamped backup file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = backup_path / f"{collection_name}_backup_{timestamp}.json"
            
            # Get collection info
            info = self.get_collection_info(collection_name)
            if not info:
                return False
            
            # Save backup
            with open(backup_file, 'w') as f:
                json.dump(info, f, indent=2, default=str)
            
            logger.info(f"Backup created: {backup_file}")
            logger.info(f"Collection {collection_name}: {info['vectors_count']} vectors backed up")
            return True
            
        except Exception as e:
            logger.error(f"Failed to backup collection {collection_name}: {e}")
            return False
    
    def clean_collection(self, collection_name, backup=False):
        """Clean all data from a specific collection"""
        try:
            # Check if collection exists
            collections = self.list_collections()
            if collection_name not in collections:
                logger.warning(f"Collection {collection_name} does not exist")
                return False
            
            # Get collection info before cleaning
            info = self.get_collection_info(collection_name)
            if not info:
                return False
            
            original_count = info['vectors_count']
            logger.info(f"Collection {collection_name} has {original_count} vectors")
            
            # Create backup if requested
            if backup:
                if not self.backup_collection(collection_name):
                    logger.error("Backup failed, aborting clean operation")
                    return False
            
            # Clean the collection by deleting all points
            if original_count > 0:
                # Delete all points using scroll and delete
                scroll_result = self.client.scroll(
                    collection_name=collection_name,
                    limit=10000,  # Process in batches
                    with_payload=False,
                    with_vectors=False
                )
                
                while scroll_result[0]:  # While there are points
                    point_ids = [point.id for point in scroll_result[0]]
                    if point_ids:
                        self.client.delete(
                            collection_name=collection_name,
                            points_selector=point_ids
                        )
                        logger.info(f"Deleted {len(point_ids)} points from {collection_name}")
                    
                    # Get next batch
                    if scroll_result[1]:  # If there's a next_page_offset
                        scroll_result = self.client.scroll(
                            collection_name=collection_name,
                            offset=scroll_result[1],
                            limit=10000,
                            with_payload=False,
                            with_vectors=False
                        )
                    else:
                        break
            
            # Verify cleaning
            final_count = self.client.count(collection_name).count
            logger.info(f"Collection {collection_name} cleaned: {original_count} â†’ {final_count} vectors")
            
            return final_count == 0
            
        except Exception as e:
            logger.error(f"Failed to clean collection {collection_name}: {e}")
            return False
    
    def recreate_collection(self, collection_name):
        """Delete and recreate a collection with fresh configuration"""
        try:
            # Get current configuration before deletion
            info = self.get_collection_info(collection_name)
            if not info:
                logger.warning(f"Collection {collection_name} does not exist, cannot recreate")
                return False
            
            original_count = info['vectors_count']
            vector_config = info['config'].params.vectors
            
            logger.info(f"Recreating collection {collection_name} (had {original_count} vectors)")
            
            # Delete the collection
            self.client.delete_collection(collection_name)
            logger.info(f"Deleted collection {collection_name}")
            
            # Recreate with same configuration
            # Handle both single vector and named vectors configurations
            if hasattr(vector_config, 'size'):
                # Single vector configuration
                vectors_config = VectorParams(
                    size=vector_config.size,
                    distance=vector_config.distance
                )
            else:
                # Named vectors configuration - use default configuration
                vectors_config = VectorParams(
                    size=1024,  # Default embedding dimension
                    distance=Distance.COSINE
                )
            
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config=vectors_config
            )
            logger.info(f"Recreated collection {collection_name}")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to recreate collection {collection_name}: {e}")
            return False
    
    def clean_all_collections(self, backup=False):
        """Clean all collections"""
        collections = self.list_collections()
        if not collections:
            logger.info("No collections found to clean")
            return True
        
        success_count = 0
        for collection_name in collections:
            if self.clean_collection(collection_name, backup):
                success_count += 1
        
        logger.info(f"Successfully cleaned {success_count}/{len(collections)} collections")
        return success_count == len(collections)
    
    def recreate_all_collections(self):
        """Recreate all collections with fresh configuration"""
        collections = self.list_collections()
        if not collections:
            logger.info("No collections found to recreate")
            return True
        
        success_count = 0
        for collection_name in collections:
            if self.recreate_collection(collection_name):
                success_count += 1
        
        logger.info(f"Successfully recreated {success_count}/{len(collections)} collections")
        return success_count == len(collections)


def main():
    """Main function with CLI interface"""
    parser = argparse.ArgumentParser(
        description="Clean Qdrant database collections",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    %(prog)s --all                    # Clean all collections
    %(prog)s --collection crawled_pages  # Clean specific collection
    %(prog)s --recreate               # Delete and recreate all collections
    %(prog)s --backup --all           # Backup before cleaning all
    %(prog)s --list                   # List all collections
        """
    )
    
    # Action arguments
    parser.add_argument("--all", action="store_true", help="Clean all collections")
    parser.add_argument("--collection", type=str, help="Clean specific collection")
    parser.add_argument("--recreate", action="store_true", help="Delete and recreate all collections")
    parser.add_argument("--list", action="store_true", help="List all collections and their info")
    
    # Options
    parser.add_argument("--backup", action="store_true", help="Create backup before cleaning")
    parser.add_argument("--force", action="store_true", help="Skip confirmation prompts")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Configure logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Validate arguments
    if not any([args.all, args.collection, args.recreate, args.list]):
        parser.error("Must specify one of: --all, --collection, --recreate, or --list")
    
    # Initialize cleaner
    cleaner = QdrantCleaner()
    
    try:
        # List collections
        if args.list:
            collections = cleaner.list_collections()
            if collections:
                print("\nQdrant Collections:")
                print("=" * 50)
                for collection_name in collections:
                    info = cleaner.get_collection_info(collection_name)
                    if info:
                        print(f"â€¢ {collection_name}: {info['vectors_count']} vectors")
                print()
            else:
                print("No collections found.")
            return
        
        # Confirmation for destructive operations
        if not args.force:
            if args.recreate:
                response = input("WARNING: This will DELETE and recreate ALL collections. Continue? (yes/no): ")
            elif args.all:
                response = input("WARNING: This will CLEAN all collections (delete all data). Continue? (yes/no): ")
            elif args.collection:
                response = input(f"WARNING: This will CLEAN collection '{args.collection}'. Continue? (yes/no): ")
            else:
                response = "yes"
            
            if response.lower() not in ['yes', 'y']:
                print("Operation cancelled.")
                return
        
        # Execute operations
        success = False
        
        if args.recreate:
            logger.info("Starting recreation of all collections...")
            success = cleaner.recreate_all_collections()
            
        elif args.all:
            logger.info("Starting cleanup of all collections...")
            success = cleaner.clean_all_collections(backup=args.backup)
            
        elif args.collection:
            logger.info(f"Starting cleanup of collection: {args.collection}")
            success = cleaner.clean_collection(args.collection, backup=args.backup)
        
        # Report results
        if success:
            print("SUCCESS: Operation completed successfully!")
        else:
            print("ERROR: Operation failed. Check logs for details.")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nOperation cancelled by user.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()


================================================
FILE: scripts/fix_qdrant_dimensions.py
================================================
#!/usr/bin/env python3
"""
Fix Qdrant dimensions mismatch issue.

This script recreates Qdrant collections with the correct dimensions
based on the current embedding model configuration.
"""

import os
import sys
import logging
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Add src directory to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from embedding_config import get_embedding_dimensions, validate_embeddings_config

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    """Fix Qdrant dimension mismatch."""
    
    print("Fixing Qdrant dimension mismatch...")
    print("=" * 50)
    
    try:
        # Step 1: Validate embedding configuration
        print("[1/5] Validating embedding configuration...")
        validate_embeddings_config()
        current_dims = get_embedding_dimensions()
        print(f"OK Current embedding dimensions: {current_dims}")
        
        # Step 2: Connect to Qdrant
        print("\n[2/5] Connecting to Qdrant...")
        qdrant_host = os.getenv("QDRANT_HOST", "localhost")
        qdrant_port = int(os.getenv("QDRANT_PORT", "6333"))
        
        client = QdrantClient(host=qdrant_host, port=qdrant_port)
        print(f"OK Connected to Qdrant at {qdrant_host}:{qdrant_port}")
        
        # Step 3: Check existing collections
        print("\n[3/5] Checking existing collections...")
        collections = client.get_collections()
        
        target_collections = ["crawled_pages", "code_examples"]
        existing_collections = [col.name for col in collections.collections]
        
        for collection_name in target_collections:
            if collection_name in existing_collections:
                collection_info = client.get_collection(collection_name)
                # Handle different Qdrant client versions
                vectors_config = collection_info.config.params.vectors
                if hasattr(vectors_config, 'size'):
                    # Single vector configuration
                    current_collection_dims = vectors_config.size
                elif hasattr(vectors_config, '__getitem__'):
                    # Named vectors configuration (dict-like)
                    current_collection_dims = list(vectors_config.values())[0].size
                else:
                    # Fallback: assume it's a dict
                    current_collection_dims = vectors_config.size
                    
                print(f"INFO Collection '{collection_name}': {current_collection_dims} dimensions")
                
                if current_collection_dims != current_dims:
                    print(f"WARNING Dimension mismatch detected for '{collection_name}'!")
                    print(f"   Expected: {current_dims}, Found: {current_collection_dims}")
                else:
                    print(f"OK Collection '{collection_name}' has correct dimensions")
            else:
                print(f"INFO Collection '{collection_name}' not found (will be created)")
        
        # Step 4: Recreate collections with correct dimensions
        print(f"\n[4/5] Recreating collections with {current_dims} dimensions...")
        
        vector_config = VectorParams(size=current_dims, distance=Distance.COSINE)
        
        for collection_name in target_collections:
            print(f"\nProcessing collection '{collection_name}'...")
            
            # Delete existing collection if it exists
            if collection_name in existing_collections:
                print("   Deleting existing collection...")
                client.delete_collection(collection_name)
                print(f"   DELETED '{collection_name}'")
            
            # Create new collection with correct dimensions
            print(f"   Creating new collection with {current_dims} dimensions...")
            client.create_collection(
                collection_name=collection_name,
                vectors_config=vector_config
            )
            print(f"   CREATED '{collection_name}' with {current_dims} dimensions")
        
        # Step 5: Verify collections
        print("\n[5/5] Verifying collections...")
        for collection_name in target_collections:
            collection_info = client.get_collection(collection_name)
            # Handle different Qdrant client versions for verification
            vectors_config = collection_info.config.params.vectors
            if hasattr(vectors_config, 'size'):
                actual_dims = vectors_config.size
            elif hasattr(vectors_config, '__getitem__'):
                actual_dims = list(vectors_config.values())[0].size
            else:
                actual_dims = vectors_config.size
            
            if actual_dims == current_dims:
                print(f"OK '{collection_name}': {actual_dims} dimensions (correct)")
            else:
                print(f"ERROR '{collection_name}': {actual_dims} dimensions (expected {current_dims})")
                raise Exception(f"Collection '{collection_name}' still has wrong dimensions!")
        
        print("\n" + "=" * 50)
        print("SUCCESS: Qdrant dimension fix completed!")
        print(f"All collections now use {current_dims} dimensions")
        print("Server should now work without dimension errors")
        print("\nNext steps:")
        print("1. Restart the MCP server")
        print("2. Test with a crawling operation")
        print("3. Verify embeddings are working correctly")
        
    except Exception as e:
        print(f"\nERROR fixing Qdrant dimensions: {e}")
        logger.exception("Failed to fix Qdrant dimensions")
        sys.exit(1)

if __name__ == "__main__":
    main()


================================================
FILE: src/__init__.py
================================================
# Empty init file to make src a Python package


================================================
FILE: src/__main__.py
================================================
"""
Entry point for running the Crawl4AI MCP server as a module.
This allows us to use relative imports properly.
"""

import asyncio
from .crawl4ai_mcp import main

if __name__ == "__main__":
    # Apply Windows ConnectionResetError fix before starting event loop
    from .event_loop_fix import setup_event_loop

    setup_event_loop()
    asyncio.run(main())



================================================
FILE: src/crawl4ai_mcp.py
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x8f in position 69547: character maps to <undefined>


================================================
FILE: src/device_manager.py
================================================
"""
Device Management for GPU/CPU CrossEncoder Acceleration.

Provides robust device detection, GPU memory management, and graceful fallback
for CrossEncoder models. Follows production-ready patterns with actual GPU
testing beyond availability flags.
"""

import logging
import os
import warnings
from dataclasses import dataclass
from typing import Optional, Dict, Any, Tuple

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

# Suppress future warnings from PyTorch for cleaner logs
warnings.filterwarnings("ignore", category=FutureWarning, module="torch")


@dataclass
class DeviceConfig:
    """Configuration for device selection and GPU settings."""
    device_type: str              # "cuda", "cpu", "mps"
    device_index: Optional[int]   # GPU index for multi-GPU systems
    precision: str                # "float32", "float16", "bfloat16"
    memory_fraction: float        # GPU memory fraction to use


@dataclass
class DeviceInfo:
    """Information about detected device capabilities."""
    device: str                   # PyTorch device string
    name: str                     # Human-readable device name
    memory_total: Optional[float] # Total memory in GB
    is_available: bool            # Whether device is truly available


def get_optimal_device(preference: str = "auto", gpu_index: int = 0) -> torch.device:
    """
    Get optimal device for CrossEncoder model with robust detection.
    
    CRITICAL: Tests actual GPU operations beyond availability flags.
    Always provides CPU fallback for production reliability.
    
    Args:
        preference: Device preference - "auto", "cuda", "cpu", "mps"
        gpu_index: GPU index for multi-GPU systems (default: 0)
        
    Returns:
        torch.device: Optimal device for model initialization
        
    Raises:
        None - Always returns valid device with fallback to CPU
    """
    if not TORCH_AVAILABLE:
        logging.warning("PyTorch not available. Using CPU.")
        # Return a mock device object that represents CPU when PyTorch unavailable
        class MockDevice:
            def __str__(self):
                return "cpu"
        return MockDevice()
    
    # Force CPU if requested
    if preference == "cpu":
        logging.info("CPU device forced by preference")
        return torch.device("cpu")
    
    # Try GPU (CUDA or MPS) if requested or auto
    if preference in ["auto", "cuda"] and torch.cuda.is_available():
        try:
            # CRITICAL: Test actual GPU operations, not just availability
            device = torch.device(f"cuda:{gpu_index}")
            
            # Verify GPU works with actual tensor operations
            test_tensor = torch.randn(10, 10, device=device)
            _ = test_tensor @ test_tensor.T  # Matrix multiplication test
            
            logging.info(f"GPU device verified: {device} ({torch.cuda.get_device_name(device)})")
            return device
            
        except Exception as e:
            logging.warning(f"GPU test failed: {e}. Falling back to CPU.")
    
    # Try MPS (Apple Silicon) if requested or auto
    if preference in ["auto", "mps"] and hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        try:
            device = torch.device("mps")
            
            # Test MPS with simple operation
            test_tensor = torch.randn(10, 10, device=device)
            _ = test_tensor.sum()
            
            logging.info(f"MPS device verified: {device}")
            return device
            
        except Exception as e:
            logging.warning(f"MPS test failed: {e}. Falling back to CPU.")
    
    # FALLBACK: Always return CPU as last resort
    logging.info("Using CPU device (fallback)")
    return torch.device("cpu")


def device_detection_with_fallback(config: Optional[DeviceConfig] = None) -> Tuple[torch.device, DeviceInfo]:
    """
    Comprehensive device detection with fallback strategy.
    
    Args:
        config: Device configuration preferences
        
    Returns:
        Tuple of (torch.device, DeviceInfo) with detected device and metadata
    """
    if not TORCH_AVAILABLE:
        cpu_device = torch.device("cpu")
        cpu_info = DeviceInfo(
            device="cpu",
            name="CPU",
            memory_total=None,
            is_available=True
        )
        return cpu_device, cpu_info
    
    if config is None:
        # Default configuration from environment variables
        config = DeviceConfig(
            device_type=os.getenv("USE_GPU_ACCELERATION", "auto"),
            device_index=int(os.getenv("GPU_DEVICE_INDEX", "0")),
            precision=os.getenv("GPU_PRECISION", "float32"),
            memory_fraction=float(os.getenv("GPU_MEMORY_FRACTION", "0.8"))
        )
    
    # Get optimal device
    device = get_optimal_device(config.device_type, config.device_index)
    
    # Gather device info
    if device.type == "cuda":
        device_info = DeviceInfo(
            device=str(device),
            name=torch.cuda.get_device_name(device),
            memory_total=torch.cuda.get_device_properties(device).total_memory / (1024**3),  # GB
            is_available=True
        )
    elif device.type == "mps":
        device_info = DeviceInfo(
            device=str(device),
            name="Apple Silicon GPU (MPS)",
            memory_total=None,  # MPS doesn't expose memory info
            is_available=True
        )
    else:
        device_info = DeviceInfo(
            device=str(device),
            name="CPU",
            memory_total=None,
            is_available=True
        )
    
    return device, device_info


def cleanup_gpu_memory() -> None:
    """
    Clean up GPU memory to prevent OOM in long-running processes.
    
    CRITICAL: Required for CrossEncoder in production environments.
    Safe to call even when GPU is not available.
    """
    if not TORCH_AVAILABLE:
        return
    
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logging.debug("GPU memory cache cleared")
    except Exception as e:
        logging.debug(f"GPU memory cleanup failed: {e}")


def get_device_info() -> Dict[str, Any]:
    """
    Get comprehensive device information for diagnostics.
    
    Returns:
        Dict with device capabilities and status information
    """
    info = {
        "torch_available": TORCH_AVAILABLE,
        "cuda_available": False,
        "mps_available": False,
        "device_count": 0,
        "devices": []
    }
    
    if not TORCH_AVAILABLE:
        return info
    
    # CUDA information
    if torch.cuda.is_available():
        info["cuda_available"] = True
        info["device_count"] = torch.cuda.device_count()
        
        for i in range(torch.cuda.device_count()):
            try:
                device = torch.device(f"cuda:{i}")
                props = torch.cuda.get_device_properties(device)
                device_info = {
                    "index": i,
                    "name": props.name,
                    "memory_total_gb": props.total_memory / (1024**3),
                    "memory_allocated_gb": torch.cuda.memory_allocated(device) / (1024**3),
                    "is_current": i == torch.cuda.current_device()
                }
                info["devices"].append(device_info)
            except Exception as e:
                logging.debug(f"Could not get info for CUDA device {i}: {e}")
    
    # MPS information
    if hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        info["mps_available"] = True
        info["devices"].append({
            "name": "Apple Silicon GPU (MPS)",
            "type": "mps",
            "memory_total_gb": None,  # Not available for MPS
            "is_available": True
        })
    
    return info


def get_model_kwargs_for_device(device: torch.device, precision: str = "float32") -> Dict[str, Any]:
    """
    Get model_kwargs for CrossEncoder based on device and precision.
    
    Args:
        device: Target device
        precision: Desired precision ("float32", "float16", "bfloat16")
        
    Returns:
        Dict with model_kwargs for CrossEncoder initialization
    """
    model_kwargs = {}
    
    # Apply precision settings for GPU devices
    if device.type in ["cuda", "mps"] and precision != "float32":
        if precision == "float16":
            model_kwargs["torch_dtype"] = torch.float16
        elif precision == "bfloat16":
            model_kwargs["torch_dtype"] = torch.bfloat16
        else:
            logging.warning(f"Unknown precision '{precision}', using float32")
    
    return model_kwargs


# Environment variable pattern compatibility
def get_gpu_preference() -> str:
    """
    Get GPU preference from environment variables.
    
    Returns:
        GPU preference string compatible with get_optimal_device()
    """
    env_value = os.getenv("USE_GPU_ACCELERATION", "auto").lower()
    
    # Handle boolean-style values for backward compatibility
    if env_value in ["true", "1", "yes"]:
        return "auto"
    elif env_value in ["false", "0", "no"]:
        return "cpu"
    else:
        return env_value  # "auto", "cuda", "mps", etc.


================================================
FILE: src/embedding_cache.py
================================================
"""
Redis-based embedding cache for the Crawl4AI MCP server.

This module provides a high-performance caching layer for vector embeddings to reduce API costs,
decrease latency during data ingestion, and improve system resilience. The cache intercepts 
requests for embeddings, checks Redis for existing results, and only calls external embedding 
APIs for texts that haven't been processed before.

Features:
- Cost reduction: 60-85% reduction in embedding API calls
- Performance: 6.86x faster embedding retrieval 
- Resilience: Graceful degradation when Redis unavailable
- Memory efficiency: Optimized serialization and TTL strategies
"""
import os
import time
import pickle
import hashlib
import logging
import redis
from typing import List, Dict, Any, Optional
from enum import Enum


class CircuitState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, bypass operations
    HALF_OPEN = "half_open"  # Testing recovery


class CircuitBreaker:
    """
    Circuit breaker for Redis operations to provide graceful degradation.
    
    Prevents cascading failures by temporarily disabling Redis operations
    when multiple consecutive failures occur.
    """
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
        """
        Initialize circuit breaker.
        
        Args:
            failure_threshold: Number of failures before opening circuit
            recovery_timeout: Seconds to wait before attempting recovery
        """
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
        
    def call(self, func, *args, **kwargs):
        """
        Execute function with circuit breaker protection.
        
        Args:
            func: Function to execute
            *args, **kwargs: Function arguments
            
        Returns:
            Function result or None if circuit is open
        """
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
                logging.info("Circuit breaker transitioning to half-open state")
            else:
                return None  # Fail fast
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            # Catch all exceptions since we want to handle any Redis-related failures
            self._on_failure()
            logging.warning(f"Redis operation failed: {e}")
            return None  # Graceful degradation
    
    def _should_attempt_reset(self) -> bool:
        """Check if enough time has passed to attempt reset."""
        if self.last_failure_time is None:
            return True
        return time.time() - self.last_failure_time >= self.recovery_timeout
    
    def _on_success(self):
        """Handle successful operation."""
        self.failure_count = 0
        if self.state == CircuitState.HALF_OPEN:
            self.state = CircuitState.CLOSED
            logging.info("Circuit breaker reset to closed state")
    
    def _on_failure(self):
        """Handle failed operation."""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
            logging.error(f"Circuit breaker opened after {self.failure_count} failures")


class EmbeddingCache:
    """
    Redis-based cache for vector embeddings with circuit breaker protection.
    
    Provides high-performance caching with graceful degradation when Redis is unavailable.
    Uses pickle serialization for optimal Python compatibility and performance.
    """
    
    def __init__(self, host: Optional[str] = None, port: Optional[int] = None, 
                 db: Optional[int] = None, password: Optional[str] = None):
        """
        Initialize Redis embedding cache.
        
        Args:
            host: Redis host (defaults to REDIS_HOST env var or localhost)
            port: Redis port (defaults to REDIS_PORT env var or 6379)
            db: Redis database number (defaults to REDIS_DB env var or 0)
            password: Redis password (defaults to REDIS_PASSWORD env var)
        """
        # Configuration from environment
        self.host = host or os.getenv("REDIS_HOST", "localhost")
        self.port = port or int(os.getenv("REDIS_PORT", "6379"))
        self.db = db or int(os.getenv("REDIS_DB", "0"))
        self.password = password or os.getenv("REDIS_PASSWORD")
        self.username = os.getenv("REDIS_USERNAME")
        self.ssl = os.getenv("REDIS_SSL", "false").lower() == "true"
        
        # Connection settings
        self.connection_timeout = int(os.getenv("REDIS_CONNECTION_TIMEOUT", "5"))
        self.socket_timeout = int(os.getenv("REDIS_SOCKET_TIMEOUT", "5"))
        self.max_connections = int(os.getenv("REDIS_MAX_CONNECTIONS", "20"))
        self.health_check_interval = int(os.getenv("REDIS_HEALTH_CHECK_INTERVAL", "30"))
        
        # Cache behavior settings
        self.default_ttl = int(os.getenv("REDIS_EMBEDDING_TTL", "86400"))  # 24 hours
        
        # Circuit breaker settings
        failure_threshold = int(os.getenv("REDIS_CIRCUIT_BREAKER_FAILURES", "5"))
        recovery_timeout = int(os.getenv("REDIS_CIRCUIT_BREAKER_TIMEOUT", "60"))
        self.circuit_breaker = CircuitBreaker(failure_threshold, recovery_timeout)
        
        # Initialize Redis client
        self.redis = None
        self._initialization_error = None
        self._initialize_client()
        
        logging.info(f"EmbeddingCache initialized: {self.host}:{self.port}")
    
    def _initialize_client(self):
        """Initialize Redis client with connection pooling."""
        try:
            # Build connection pool parameters
            pool_params = {
                'host': self.host,
                'port': self.port,
                'db': self.db,
                'max_connections': self.max_connections,
                'socket_connect_timeout': self.connection_timeout,
                'socket_timeout': self.socket_timeout,
                'health_check_interval': self.health_check_interval,
                'decode_responses': False,  # We handle binary data (pickle)
            }
            
            # Add authentication if configured
            if self.password:
                pool_params['password'] = self.password
            if self.username:
                pool_params['username'] = self.username
            
            # Add SSL if configured
            if self.ssl:
                pool_params['ssl'] = True
                pool_params['ssl_cert_reqs'] = None
            
            pool = redis.ConnectionPool(**pool_params)
            
            self.redis = redis.Redis(connection_pool=pool)
            
            # Test connection
            self.redis.ping()
            logging.info("Redis connection established successfully")
            
        except Exception as e:
            logging.error(f"Failed to initialize Redis client: {e}")
            self.redis = None
            # Store the exception for health check
            self._initialization_error = str(e)
            # Don't raise exception - graceful degradation
    
    def _generate_cache_key(self, text: str, model: str) -> str:
        """
        Generate cache key for text and model combination.
        
        Args:
            text: Input text
            model: Embedding model name
            
        Returns:
            Cache key string
        """
        # Create hash of text for collision resistance and space efficiency
        text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
        return f"embedding:{model}:{text_hash}"
    
    def get_batch(self, texts: List[str], model: str) -> Dict[str, List[float]]:
        """
        Retrieve embeddings for multiple texts from cache.
        
        Args:
            texts: List of texts to retrieve embeddings for
            model: Embedding model name
            
        Returns:
            Dictionary mapping text to embedding for cached items
        """
        if not self.redis or not texts:
            return {}
        
        def _get_batch_operation():
            # Generate cache keys
            cache_keys = [self._generate_cache_key(text, model) for text in texts]
            
            # Batch retrieve using pipeline for performance
            pipeline = self.redis.pipeline()
            for key in cache_keys:
                pipeline.get(key)
            
            cached_values = pipeline.execute()
            
            # Build result dictionary
            result = {}
            for i, (text, cached_value) in enumerate(zip(texts, cached_values)):
                if cached_value is not None:
                    try:
                        embedding = pickle.loads(cached_value)
                        result[text] = embedding
                    except Exception as e:
                        logging.warning(f"Failed to deserialize cached embedding for text {i}: {e}")
            
            return result
        
        # Execute with circuit breaker protection
        result = self.circuit_breaker.call(_get_batch_operation)
        return result if result is not None else {}
    
    def set_batch(self, embeddings: Dict[str, List[float]], model: str, ttl: Optional[int] = None):
        """
        Store embeddings for multiple texts in cache.
        
        Args:
            embeddings: Dictionary mapping text to embedding
            model: Embedding model name
            ttl: Time to live in seconds (defaults to configured TTL)
        """
        if not self.redis or not embeddings:
            return
        
        ttl = ttl or self.default_ttl
        
        def _set_batch_operation():
            # Batch store using pipeline for performance
            pipeline = self.redis.pipeline()
            
            for text, embedding in embeddings.items():
                try:
                    cache_key = self._generate_cache_key(text, model)
                    serialized_embedding = pickle.dumps(embedding)
                    pipeline.setex(cache_key, ttl, serialized_embedding)
                except Exception as e:
                    logging.warning(f"Failed to serialize embedding for caching: {e}")
            
            pipeline.execute()
            logging.debug(f"Cached {len(embeddings)} embeddings with TTL {ttl}s")
        
        # Execute with circuit breaker protection
        self.circuit_breaker.call(_set_batch_operation)
    
    def health_check(self) -> Dict[str, Any]:
        """
        Comprehensive cache health check.
        
        Returns:
            Dictionary with health status and metrics
        """
        if not self.redis:
            error_msg = getattr(self, '_initialization_error', None) or 'Redis client not initialized'
            return {
                'status': 'unhealthy',
                'error': error_msg,
                'circuit_breaker_state': self.circuit_breaker.state.value
            }
        
        try:
            start_time = time.time()
            
            # Test basic connectivity
            ping_result = self.redis.ping()
            
            # Test read/write operations
            test_key = f"health_check:{int(time.time())}"
            self.redis.setex(test_key, 10, "test")
            read_result = self.redis.get(test_key)
            self.redis.delete(test_key)
            
            latency = (time.time() - start_time) * 1000  # milliseconds
            
            return {
                'status': 'healthy',
                'ping': ping_result,
                'read_write': read_result == b'test',
                'latency_ms': round(latency, 2),
                'circuit_breaker_state': self.circuit_breaker.state.value,
                'connection_info': {
                    'host': self.host,
                    'port': self.port,
                    'db': self.db
                }
            }
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'circuit_breaker_state': self.circuit_breaker.state.value
            }


def validate_redis_config():
    """
    Validate Redis configuration environment variables.
    
    Raises:
        ValueError: If configuration is invalid
    """
    host = os.getenv("REDIS_HOST", "localhost")
    port = os.getenv("REDIS_PORT", "6379")
    
    try:
        port_int = int(port)
        if not (1 <= port_int <= 65535):
            raise ValueError(f"Invalid Redis port: {port}")
    except ValueError as e:
        raise ValueError(f"Invalid Redis port configuration: {e}")
    
    # Validate TTL setting
    ttl = os.getenv("REDIS_EMBEDDING_TTL", "86400")
    try:
        ttl_int = int(ttl)
        if ttl_int <= 0:
            raise ValueError(f"Invalid TTL value: {ttl}")
    except ValueError as e:
        raise ValueError(f"Invalid Redis TTL configuration: {e}")
    
    logging.info(f"Redis configuration validated: {host}:{port_int}")


# Global instance following existing codebase patterns
_embedding_cache = None

def get_embedding_cache() -> Optional[EmbeddingCache]:
    """
    Get global embedding cache instance (singleton pattern).
    
    Returns:
        EmbeddingCache instance or None if caching disabled/unavailable
    """
    global _embedding_cache
    
    # Only initialize if caching is enabled
    if os.getenv("USE_REDIS_CACHE", "false").lower() != "true":
        return None
    
    if _embedding_cache is None:
        try:
            _embedding_cache = EmbeddingCache()
        except Exception as e:
            logging.error(f"Failed to initialize embedding cache: {e}")
            _embedding_cache = None
    
    return _embedding_cache


================================================
FILE: src/embedding_config.py
================================================
"""
Embedding configuration utilities.

This module provides configuration utilities for embedding dimensions
and model-specific settings.
"""
import os


def get_embedding_dimensions() -> int:
    """
    Get embedding dimensions based on configuration.
    
    Returns:
        int: The number of dimensions for embeddings
        
    Raises:
        ValueError: If configured dimensions are invalid
    """
    # Check for explicit dimension configuration first
    explicit_dims = os.getenv("EMBEDDINGS_DIMENSIONS")
    if explicit_dims:
        try:
            dims = int(explicit_dims)
            if dims <= 0:
                raise ValueError("EMBEDDINGS_DIMENSIONS must be positive")
            return dims
        except ValueError as e:
            if "positive" in str(e):
                raise
            raise ValueError("EMBEDDINGS_DIMENSIONS must be a valid integer")
    
    # Auto-detect based on model
    embeddings_model = os.getenv("EMBEDDINGS_MODEL", "text-embedding-3-small")
    
    # Model dimension mappings
    model_dimensions = {
        # OpenAI models
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536,
        
        # DeepInfra models
        "Qwen/Qwen3-Embedding-0.6B": 1024,
        "BAAI/bge-large-en-v1.5": 1024,
        "BAAI/bge-small-en-v1.5": 384,
        "BAAI/bge-base-en-v1.5": 768,
        "sentence-transformers/all-MiniLM-L6-v2": 384,
        "sentence-transformers/all-mpnet-base-v2": 768,
        
        # Hugging Face models commonly used with DeepInfra
        "intfloat/e5-large-v2": 1024,
        "intfloat/e5-base-v2": 768,
        "intfloat/e5-small-v2": 384,
    }
    
    # Check if we have a known model
    if embeddings_model in model_dimensions:
        detected_dims = model_dimensions[embeddings_model]
        print(f"Auto-detected embedding dimensions for {embeddings_model}: {detected_dims}")
        return detected_dims
    
    # Default fallback
    default_dims = 1536
    print(f"Unknown embedding model '{embeddings_model}'. Using default dimensions: {default_dims}")
    return default_dims


def validate_embeddings_config() -> bool:
    """
    Validate embeddings configuration.
    
    Returns:
        bool: True if configuration is valid
        
    Raises:
        ValueError: If configuration is invalid
    """
    # Check API key
    api_key = os.getenv("EMBEDDINGS_API_KEY")
    if not api_key:
        raise ValueError("No API key configured for embeddings. Please set EMBEDDINGS_API_KEY")
    
    # Validate dimensions configuration
    try:
        dims = get_embedding_dimensions()
        print(f"Embeddings configuration validated - dimensions: {dims}")
        return True
    except Exception as e:
        raise ValueError(f"Invalid embeddings configuration: {e}")


================================================
FILE: src/event_loop_fix.py
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x8f in position 12519: character maps to <undefined>


================================================
FILE: src/qdrant_wrapper.py
================================================
"""
Qdrant client wrapper for the Crawl4AI MCP server.

This module provides a wrapper around the Qdrant client that maintains compatibility
with the existing Supabase-based interface while using Qdrant as the vector database.

Note: This module is named 'qdrant_client' but provides a wrapper class 
'QdrantClientWrapper' to avoid conflicts with the installed qdrant-client package.
"""
import os
import uuid
import logging
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse
from datetime import datetime, timezone

from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct, Filter, FieldCondition, 
    MatchValue
)
from tenacity import retry, stop_after_attempt, wait_exponential

# Import dynamic dimension utilities
try:
    from .embedding_config import get_embedding_dimensions
except ImportError:
    from embedding_config import get_embedding_dimensions

def get_collections_config():
    """
    Generate collection configurations with dynamic embedding dimensions.
    
    Returns:
        dict: Collection configurations with current embedding dimensions
    """
    embedding_dims = get_embedding_dimensions()
    
    return {
        "crawled_pages": {
            "vectors_config": VectorParams(size=embedding_dims, distance=Distance.COSINE),
            "payload_schema": {
                "url": str,
                "content": str,
                "chunk_number": int,
                "source_id": str,
                "metadata": dict,
                "created_at": str
            }
        },
        "code_examples": {
            "vectors_config": VectorParams(size=embedding_dims, distance=Distance.COSINE),
            "payload_schema": {
                "url": str,
                "content": str,
                "summary": str,
                "chunk_number": int,
                "source_id": str,
                "metadata": dict,
                "created_at": str
            }
        },
        "sources": {
            "vectors_config": VectorParams(size=embedding_dims, distance=Distance.COSINE),
            "payload_schema": {
                "source_id": str,
                "summary": str,
                "total_words": int,
                "created_at": str,
                "updated_at": str
            }
        }
    }

# Legacy global collections - replaced by get_collections_config()
# Maintained for backward compatibility during transition
COLLECTIONS = {
    "crawled_pages": {
        "vectors_config": VectorParams(size=1536, distance=Distance.COSINE),
        "payload_schema": {
            "url": str,
            "content": str,
            "chunk_number": int,
            "source_id": str,
            "metadata": dict,
            "created_at": str
        }
    },
    "code_examples": {
        "vectors_config": VectorParams(size=1536, distance=Distance.COSINE),
        "payload_schema": {
            "url": str,
            "content": str,
            "summary": str,
            "chunk_number": int,
            "source_id": str,
            "metadata": dict,
            "created_at": str
        }
    }
}

# Sources are now stored persistently in Qdrant 'sources' collection

class QdrantClientWrapper:
    """
    Wrapper class for Qdrant client that provides Supabase-compatible interface.
    """
    
    # Class-level cache for collection existence to avoid redundant checks
    _collections_verified = False
    
    def __init__(self, host: str = None, port: int = None):
        """
        Initialize Qdrant client wrapper.
        
        Args:
            host: Qdrant host (defaults to env QDRANT_HOST or localhost)
            port: Qdrant port (defaults to env QDRANT_PORT or 6333)
        """
        self.host = host or os.getenv("QDRANT_HOST", "localhost")
        self.port = port or int(os.getenv("QDRANT_PORT", "6333"))
        
        # Initialize Qdrant client with retry logic
        self.client = self._create_client()
        
        # Ensure collections exist (only if not already verified)
        if not QdrantClientWrapper._collections_verified:
            self._ensure_collections_exist()
            QdrantClientWrapper._collections_verified = True
            logging.info("Qdrant collections verified and created if necessary")
        else:
            logging.debug("Qdrant collections already verified, skipping check")
        
        logging.info(f"Qdrant client initialized: {self.host}:{self.port}")
    
    def _create_client(self) -> QdrantClient:
        """Create Qdrant client with error handling."""
        try:
            client = QdrantClient(
                host=self.host,
                port=self.port,
                prefer_grpc=True,  # Better performance
                timeout=30,        # Longer timeout for large operations
            )
            # Test connection
            client.get_collections()
            return client
        except Exception as e:
            logging.error(f"Failed to connect to Qdrant at {self.host}:{self.port}: {e}")
            raise ConnectionError(f"Cannot connect to Qdrant: {e}")
    
    def _collection_exists(self, collection_name: str) -> bool:
        """Check if collection exists."""
        try:
            self.client.get_collection(collection_name)
            return True
        except Exception:
            return False
    
    def _validate_collection_dimensions(self, collection_name: str, expected_config: VectorParams) -> Dict[str, Any]:
        """
        Validate collection dimensions against expected configuration.
        
        Args:
            collection_name: Name of the collection to validate
            expected_config: Expected vector configuration
            
        Returns:
            dict: Validation results with dimensions and recreation status
        """
        try:
            collection_info = self.client.get_collection(collection_name)
            current_size = collection_info.config.params.vectors.size
            expected_size = expected_config.size
            
            return {
                "size_match": current_size == expected_size,
                "needs_recreation": current_size != expected_size,
                "current_size": current_size,
                "expected_size": expected_size
            }
        except Exception as e:
            logging.debug(f"Failed to validate collection {collection_name}: {e}")
            return {"needs_recreation": True, "error": str(e)}
    
    def _recreate_collection_safely(self, collection_name: str, vectors_config: VectorParams):
        """
        Safely recreate a collection with new dimensions.
        
        Args:
            collection_name: Name of collection to recreate
            vectors_config: New vector configuration
        """
        try:
            # Delete existing collection if it exists
            if self._collection_exists(collection_name):
                logging.warning(f"Deleting collection {collection_name} due to dimension mismatch")
                self.client.delete_collection(collection_name)
            
            # Create new collection with updated configuration
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config=vectors_config
            )
            logging.info(f"Recreated collection {collection_name} with dimensions: {vectors_config.size}")
            
        except Exception as e:
            logging.error(f"Failed to recreate collection {collection_name}: {e}")
            raise
    
    def _ensure_collections_exist(self):
        """Initialize collections with dimension validation and controlled recreation."""
        collections_config = get_collections_config()
        
        for name, config in collections_config.items():
            vectors_config = config["vectors_config"]
            
            if self._collection_exists(name):
                # Validate existing collection dimensions
                validation = self._validate_collection_dimensions(name, vectors_config)
                
                logging.info(f"Validation result for {name}: {validation}")
                
                if validation.get("needs_recreation", False):
                    current_size = validation.get("current_size", "unknown")
                    expected_size = validation.get("expected_size", vectors_config.size)
                    
                    # CONSERVATIVE APPROACH: Log warning but DO NOT auto-recreate
                    logging.warning(
                        f"Collection {name} has dimension mismatch "
                        f"(current: {current_size}, expected: {expected_size}). "
                        f"AUTO-RECREATION DISABLED. Use reset_verification_cache() to force recreation if needed."
                    )
                    # Continue using existing collection instead of recreating
                else:
                    logging.info(f"Collection {name} dimensions validated successfully - no recreation needed")
            else:
                # Create new collection only if it doesn't exist
                try:
                    logging.info(f"Collection {name} does not exist, creating with dimensions: {vectors_config.size}")
                    self.client.create_collection(
                        collection_name=name,
                        vectors_config=vectors_config
                    )
                    logging.info(f"Created collection {name} with dimensions: {vectors_config.size}")
                except Exception as e:
                    logging.error(f"Failed to create collection {name}: {e}")
                    raise
    
    def generate_point_id(self, url: str, chunk_number: int) -> str:
        """Generate consistent UUID from URL and chunk number."""
        # Create deterministic UUID from URL and chunk number
        namespace = uuid.uuid5(uuid.NAMESPACE_URL, url)
        return str(uuid.uuid5(namespace, str(chunk_number)))
    
    def normalize_search_results(self, qdrant_results: List[Any]) -> List[Dict[str, Any]]:
        """Convert Qdrant results to Supabase-compatible format."""
        normalized = []
        for hit in qdrant_results:
            result = {
                "id": hit.id,
                "similarity": hit.score,
                **hit.payload  # Include all payload fields at top level
            }
            normalized.append(result)
        return normalized
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def _robust_operation(self, operation_func, *args, **kwargs):
        """Execute Qdrant operation with retry logic."""
        try:
            return operation_func(*args, **kwargs)
        except Exception as e:
            logging.error(f"Qdrant operation failed: {e}")
            raise
    
    def add_documents_to_qdrant(
        self, 
        urls: List[str], 
        chunk_numbers: List[int],
        contents: List[str], 
        metadatas: List[Dict[str, Any]],
        url_to_full_document: Dict[str, str],
        batch_size: int = 100
    ) -> None:
        """
        Add documents to Qdrant crawled_pages collection.
        Maintains same signature as Supabase version.
        """
        if not urls:
            return
        
        # Get unique URLs for deletion (Qdrant equivalent of DELETE WHERE url IN ...)
        unique_urls = list(set(urls))
        
        # Delete existing records for these URLs
        for url in unique_urls:
            try:
                # Scroll through existing points with this URL
                existing_points = self.client.scroll(
                    collection_name="crawled_pages",
                    scroll_filter=Filter(
                        must=[FieldCondition(key="url", match=MatchValue(value=url))]
                    ),
                    limit=10000,  # Large limit to get all points
                    with_payload=False  # Only need IDs
                )[0]
                
                if existing_points:
                    point_ids = [point.id for point in existing_points]
                    self.client.delete(
                        collection_name="crawled_pages",
                        points_selector=point_ids
                    )
                    logging.info(f"Deleted {len(point_ids)} existing points for URL: {url}")
                    
            except Exception as e:
                logging.error(f"Error deleting existing records for URL {url}: {e}")
        
        # Process in batches
        total_points = len(urls)
        for i in range(0, total_points, batch_size):
            batch_end = min(i + batch_size, total_points)
            
            # Prepare batch points
            points = []
            for j in range(i, batch_end):
                # Generate consistent point ID
                point_id = self.generate_point_id(urls[j], chunk_numbers[j])
                
                # Extract source_id from URL
                parsed_url = urlparse(urls[j])
                source_id = parsed_url.netloc or parsed_url.path
                
                # Create point payload (embed all metadata)
                payload = {
                    "url": urls[j],
                    "content": contents[j],
                    "chunk_number": chunk_numbers[j],
                    "source_id": source_id,
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    **metadatas[j]  # Include all metadata fields
                }
                
                # Note: embedding will be handled by the calling function
                # This maintains compatibility with existing create_embeddings_batch logic
                points.append({
                    "id": point_id,
                    "payload": payload,
                    "content": contents[j]  # For embedding creation
                })
            
            yield points  # Yield batch for embedding creation by caller
    
    def upsert_points(self, collection_name: str, points: List[PointStruct]):
        """Upsert points to Qdrant collection with retry logic."""
        return self._robust_operation(
            self.client.upsert,
            collection_name=collection_name,
            points=points,
            wait=True
        )
    
    def search_documents(
        self, 
        query_embedding: List[float], 
        match_count: int = 10, 
        filter_metadata: Optional[Dict[str, Any]] = None,
        source_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Search documents in Qdrant with same interface as Supabase RPC function.
        """
        # Build filter conditions
        filter_conditions = []
        
        if filter_metadata:
            for key, value in filter_metadata.items():
                filter_conditions.append(
                    FieldCondition(key=key, match=MatchValue(value=value))
                )
        
        if source_filter:
            filter_conditions.append(
                FieldCondition(key="source_id", match=MatchValue(value=source_filter))
            )
        
        query_filter = Filter(must=filter_conditions) if filter_conditions else None
        
        try:
            results = self.client.search(
                collection_name="crawled_pages",
                query_vector=query_embedding,
                query_filter=query_filter,
                limit=match_count,
                with_payload=True,
                score_threshold=0.0  # Include all results like Supabase
            )
            
            return self.normalize_search_results(results)
            
        except Exception as e:
            logging.error(f"Error searching documents: {e}")
            return []
    
    def search_code_examples(
        self,
        query_embedding: List[float],
        match_count: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None,
        source_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Search code examples in Qdrant with same interface as Supabase RPC function.
        """
        # Build filter conditions
        filter_conditions = []
        
        if filter_metadata:
            for key, value in filter_metadata.items():
                filter_conditions.append(
                    FieldCondition(key=key, match=MatchValue(value=value))
                )
        
        if source_filter:
            filter_conditions.append(
                FieldCondition(key="source_id", match=MatchValue(value=source_filter))
            )
        
        query_filter = Filter(must=filter_conditions) if filter_conditions else None
        
        try:
            results = self.client.search(
                collection_name="code_examples",
                query_vector=query_embedding,
                query_filter=query_filter,
                limit=match_count,
                with_payload=True,
                score_threshold=0.0
            )
            
            return self.normalize_search_results(results)
            
        except Exception as e:
            logging.error(f"Error searching code examples: {e}")
            return []
    
    def add_code_examples_to_qdrant(
        self,
        urls: List[str],
        chunk_numbers: List[int],
        code_examples: List[str],
        summaries: List[str],
        metadatas: List[Dict[str, Any]],
        batch_size: int = 100
    ):
        """
        Add code examples to Qdrant with same interface as Supabase version.
        """
        if not urls:
            return
        
        # Delete existing records for these URLs
        unique_urls = list(set(urls))
        for url in unique_urls:
            try:
                existing_points = self.client.scroll(
                    collection_name="code_examples",
                    scroll_filter=Filter(
                        must=[FieldCondition(key="url", match=MatchValue(value=url))]
                    ),
                    limit=10000,
                    with_payload=False
                )[0]
                
                if existing_points:
                    point_ids = [point.id for point in existing_points]
                    self.client.delete(
                        collection_name="code_examples",
                        points_selector=point_ids
                    )
                    
            except Exception as e:
                logging.error(f"Error deleting existing code examples for {url}: {e}")
        
        # Process in batches and yield for embedding creation
        total_items = len(urls)
        for i in range(0, total_items, batch_size):
            batch_end = min(i + batch_size, total_items)
            
            points = []
            for j in range(i, batch_end):
                point_id = self.generate_point_id(urls[j], chunk_numbers[j])
                
                # Extract source_id from URL
                parsed_url = urlparse(urls[j])
                source_id = parsed_url.netloc or parsed_url.path
                
                payload = {
                    "url": urls[j],
                    "content": code_examples[j],
                    "summary": summaries[j],
                    "chunk_number": chunk_numbers[j],
                    "source_id": source_id,
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    **metadatas[j]
                }
                
                points.append({
                    "id": point_id,
                    "payload": payload,
                    "combined_text": f"{code_examples[j]}\n\nSummary: {summaries[j]}"
                })
            
            yield points
    
    def scroll_documents(
        self,
        collection_name: str,
        scroll_filter: Filter,
        limit: int = 1000
    ) -> List[Any]:
        """Scroll through documents with filter (for hybrid search)."""
        try:
            results = self.client.scroll(
                collection_name=collection_name,
                scroll_filter=scroll_filter,
                limit=limit,
                with_payload=True
            )[0]  # Get points from scroll result
            return results
        except Exception as e:
            logging.error(f"Error scrolling documents: {e}")
            return []
    
    def update_source_info(self, source_id: str, summary: str, word_count: int):
        """Update source information in Qdrant sources collection."""
        try:
            # Use source_id as the point ID for easy retrieval
            point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, source_id))
            now = datetime.now(timezone.utc).isoformat()
            
            # Create a dummy vector (we're only using this collection for metadata)
            dummy_vector = [0.0] * get_embedding_dimensions()
            
            point = PointStruct(
                id=point_id,
                vector=dummy_vector,
                payload={
                    "source_id": source_id,
                    "summary": summary,
                    "total_words": word_count,
                    "created_at": now,
                    "updated_at": now
                }
            )
            
            self.client.upsert(
                collection_name="sources",
                points=[point],
                wait=True
            )
            logging.info(f"Updated source info for: {source_id}")
        except Exception as e:
            logging.error(f"Error updating source info: {e}")
    
    def get_available_sources(self) -> List[Dict[str, Any]]:
        """Get all available sources from Qdrant sources collection."""
        try:
            # Scroll through all points in sources collection
            result = self.client.scroll(
                collection_name="sources",
                limit=1000,  # Assuming we won't have more than 1000 sources
                with_payload=True
            )[0]  # Get points from scroll result
            
            sources = []
            for point in result:
                payload = point.payload
                sources.append({
                    "source_id": payload["source_id"],
                    "summary": payload["summary"],
                    "total_words": payload["total_words"],
                    "created_at": payload["created_at"],
                    "updated_at": payload["updated_at"]
                })
            
            return sources
        except Exception as e:
            logging.error(f"Error getting available sources: {e}")
            return []
    
    def keyword_search_documents(
        self,
        query: str,
        match_count: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None,
        source_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Perform keyword search on document content using Qdrant scroll functionality.
        
        Args:
            query: Search query string
            match_count: Maximum number of results to return
            filter_metadata: Optional metadata filter
            source_filter: Optional source filter
            
        Returns:
            List of matching documents
        """
        try:
            # Build filter conditions
            filter_conditions = []
            
            if filter_metadata:
                for key, value in filter_metadata.items():
                    filter_conditions.append(
                        FieldCondition(key=key, match=MatchValue(value=value))
                    )
            
            if source_filter:
                filter_conditions.append(
                    FieldCondition(key="source_id", match=MatchValue(value=source_filter))
                )
            
            query_filter = Filter(must=filter_conditions) if filter_conditions else None
            
            # Use scroll to get documents and filter by keyword client-side
            # This is necessary because Qdrant doesn't have built-in text search like PostgreSQL
            results = self.client.scroll(
                collection_name="crawled_pages",
                scroll_filter=query_filter,
                limit=match_count * 10,  # Get more to filter client-side
                with_payload=True
            )[0]
            
            # Filter results by keyword match (case-insensitive)
            keyword_matches = []
            query_lower = query.lower()
            
            for point in results:
                content = point.payload.get("content", "").lower()
                if query_lower in content:
                    # Convert to search result format
                    result = {
                        "id": point.id,
                        "similarity": 0.5,  # Default similarity for keyword matches
                        **point.payload
                    }
                    keyword_matches.append(result)
                    
                    if len(keyword_matches) >= match_count:
                        break
            
            return keyword_matches
            
        except Exception as e:
            logging.error(f"Error in keyword search: {e}")
            return []
    
    def keyword_search_code_examples(
        self,
        query: str,
        match_count: int = 10,
        filter_metadata: Optional[Dict[str, Any]] = None,
        source_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Perform keyword search on code examples using Qdrant scroll functionality.
        
        Args:
            query: Search query string
            match_count: Maximum number of results to return
            filter_metadata: Optional metadata filter
            source_filter: Optional source filter
            
        Returns:
            List of matching code examples
        """
        try:
            # Build filter conditions
            filter_conditions = []
            
            if filter_metadata:
                for key, value in filter_metadata.items():
                    filter_conditions.append(
                        FieldCondition(key=key, match=MatchValue(value=value))
                    )
            
            if source_filter:
                filter_conditions.append(
                    FieldCondition(key="source_id", match=MatchValue(value=source_filter))
                )
            
            query_filter = Filter(must=filter_conditions) if filter_conditions else None
            
            # Use scroll to get code examples and filter by keyword client-side
            results = self.client.scroll(
                collection_name="code_examples",
                scroll_filter=query_filter,
                limit=match_count * 10,  # Get more to filter client-side
                with_payload=True
            )[0]
            
            # Filter results by keyword match in both content and summary (case-insensitive)
            keyword_matches = []
            query_lower = query.lower()
            
            for point in results:
                content = point.payload.get("content", "").lower()
                summary = point.payload.get("summary", "").lower()
                
                if query_lower in content or query_lower in summary:
                    # Convert to search result format
                    result = {
                        "id": point.id,
                        "similarity": 0.5,  # Default similarity for keyword matches
                        **point.payload
                    }
                    keyword_matches.append(result)
                    
                    if len(keyword_matches) >= match_count:
                        break
            
            return keyword_matches
            
        except Exception as e:
            logging.error(f"Error in keyword search for code examples: {e}")
            return []

    def health_check(self) -> Dict[str, Any]:
        """Check Qdrant health and return status."""
        try:
            collections = self.client.get_collections()
            collection_info = {}
            
            for collection in collections.collections:
                info = self.client.get_collection(collection.name)
                collection_info[collection.name] = {
                    "status": info.status,
                    "points_count": info.points_count,
                    "config": {
                        "distance": info.config.params.vectors.distance.value,
                        "size": info.config.params.vectors.size
                    }
                }
            
            return {
                "status": "healthy",
                "collections": collection_info,
                "sources_count": len(self.get_available_sources()),
                "collections_verified": QdrantClientWrapper._collections_verified
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    @classmethod
    def reset_verification_cache(cls):
        """Reset the collections verification cache. Useful for testing or maintenance."""
        cls._collections_verified = False
        logging.info("Qdrant collections verification cache reset")

    @classmethod
    def force_recreate_collections(cls):
        """
        Force recreation of all collections with current configuration.
        
        WARNING: This will delete all existing data!
        Use only when you need to update collection dimensions.
        """
        logging.warning("FORCE RECREATION: This will delete all existing collection data!")
        
        # Reset verification cache first
        cls.reset_verification_cache()
        
        # Create temporary client for recreation
        temp_client = cls()
        collections_config = get_collections_config()
        
        for name, config in collections_config.items():
            vectors_config = config["vectors_config"]
            
            if temp_client._collection_exists(name):
                logging.warning(f"Force recreating collection {name} with dimensions: {vectors_config.size}")
                temp_client._recreate_collection_safely(name, vectors_config)
            else:
                logging.info(f"Creating new collection {name} with dimensions: {vectors_config.size}")
                temp_client.client.create_collection(
                    collection_name=name,
                    vectors_config=vectors_config
                )
        
        logging.info("Force recreation completed. All collections now match current configuration.")


# Global client instance for singleton pattern
_qdrant_client_instance = None

def get_qdrant_client() -> QdrantClientWrapper:
    """
    Get a Qdrant client wrapper instance using singleton pattern.
    
    This prevents unnecessary reconnections and collection checks on server restarts.
    
    Returns:
        QdrantClientWrapper instance
    """
    global _qdrant_client_instance
    
    # Return existing instance if available and healthy
    if _qdrant_client_instance is not None:
        try:
            # Quick health check to ensure connection is still valid
            _qdrant_client_instance.client.get_collections()
            logging.debug("Reusing existing Qdrant client instance")
            return _qdrant_client_instance
        except Exception as e:
            logging.warning(f"Existing Qdrant client unhealthy, creating new instance: {e}")
            _qdrant_client_instance = None
    
    # Create new instance if none exists or previous one is unhealthy
    try:
        _qdrant_client_instance = QdrantClientWrapper()
        logging.info("Created new Qdrant client instance")
        return _qdrant_client_instance
    except Exception as e:
        logging.error(f"Failed to create Qdrant client: {e}")
        raise


================================================
FILE: src/utils.py
================================================
"""
Utility functions for the Crawl4AI MCP server with Qdrant integration.
"""
import os
import concurrent.futures
from typing import List, Dict, Any, Optional, Tuple
import openai
import time
import logging

from qdrant_client.models import PointStruct

# Import embedding configuration utilities
try:
    from .embedding_config import get_embedding_dimensions
except ImportError:
    from embedding_config import get_embedding_dimensions

# Import our Qdrant client wrapper
try:
    from .qdrant_wrapper import QdrantClientWrapper, get_qdrant_client
except ImportError:
    from qdrant_wrapper import QdrantClientWrapper, get_qdrant_client

# Import embedding cache
try:
    from .embedding_cache import get_embedding_cache
except ImportError:
    from embedding_cache import get_embedding_cache


def get_chat_client():
    """
    Get a configured OpenAI client for chat/completion operations.
    
    Supports flexible configuration through environment variables:
    - CHAT_API_KEY: API key for chat model
    - CHAT_API_BASE: Base URL for chat API (defaults to OpenAI)
    
    Returns:
        openai.OpenAI: Configured OpenAI client for chat operations
        
    Raises:
        ValueError: If no API key is configured
    """
    # Get configuration
    api_key = os.getenv("CHAT_API_KEY")
    base_url = os.getenv("CHAT_API_BASE")
    
    if not api_key:
        raise ValueError(
            "No API key configured for chat model. Please set CHAT_API_KEY"
        )
    
    # Log configuration for debugging (without exposing API key)
    if base_url:
        print(f"DEBUG: Using custom chat API endpoint: {base_url}")
    else:
        print("DEBUG: Using default OpenAI API endpoint")
    
    # Create client with optional base_url
    if base_url:
        return openai.OpenAI(api_key=api_key, base_url=base_url)
    else:
        return openai.OpenAI(api_key=api_key)

def get_embeddings_client():
    """
    Get a configured OpenAI client for embeddings operations.
    
    Supports flexible configuration through environment variables:
    - EMBEDDINGS_API_KEY: API key for embeddings
    - EMBEDDINGS_API_BASE: Base URL for embeddings API (defaults to OpenAI)
    
    Returns:
        openai.OpenAI: Configured OpenAI client for embeddings operations
        
    Raises:
        ValueError: If no API key is configured
    """
    # Get configuration
    api_key = os.getenv("EMBEDDINGS_API_KEY")
    base_url = os.getenv("EMBEDDINGS_API_BASE")
    
    if not api_key:
        raise ValueError(
            "No API key configured for embeddings. Please set EMBEDDINGS_API_KEY"
        )
    
    # Create client with optional base_url
    if base_url:
        return openai.OpenAI(api_key=api_key, base_url=base_url)
    else:
        return openai.OpenAI(api_key=api_key)


def get_chat_fallback_client():
    """
    Get a configured OpenAI client for fallback chat/completion operations.
    
    Supports flexible configuration through environment variables with inheritance:
    - CHAT_FALLBACK_API_KEY: API key for fallback chat (inherits CHAT_API_KEY if not set)
    - CHAT_FALLBACK_API_BASE: Base URL for fallback chat API (inherits CHAT_API_BASE if not set)
    
    Returns:
        openai.OpenAI: Configured OpenAI client for fallback chat operations
        
    Raises:
        ValueError: If no API key is configured (primary or fallback)
    """
    # Get fallback configuration with inheritance
    api_key = os.getenv("CHAT_FALLBACK_API_KEY") or os.getenv("CHAT_API_KEY")
    base_url = os.getenv("CHAT_FALLBACK_API_BASE") or os.getenv("CHAT_API_BASE")
    
    if not api_key:
        raise ValueError(
            "No API key configured for fallback chat model. Please set CHAT_FALLBACK_API_KEY or CHAT_API_KEY"
        )
    
    # Log configuration for debugging (without exposing API key)
    if base_url:
        print(f"DEBUG: Using fallback chat API endpoint: {base_url}")
    else:
        print("DEBUG: Using default OpenAI API endpoint for fallback chat")
    
    # Create client with optional base_url
    if base_url:
        return openai.OpenAI(api_key=api_key, base_url=base_url)
    else:
        return openai.OpenAI(api_key=api_key)

def get_embeddings_fallback_client():
    """
    Get a configured OpenAI client for fallback embeddings operations.
    
    Supports flexible configuration through environment variables with inheritance:
    - EMBEDDINGS_FALLBACK_API_KEY: API key for fallback embeddings (inherits EMBEDDINGS_API_KEY if not set)
    - EMBEDDINGS_FALLBACK_API_BASE: Base URL for fallback embeddings API (inherits EMBEDDINGS_API_BASE if not set)
    
    Returns:
        openai.OpenAI: Configured OpenAI client for fallback embeddings operations
        
    Raises:
        ValueError: If no API key is configured (primary or fallback)
    """
    # Get fallback configuration with inheritance
    api_key = os.getenv("EMBEDDINGS_FALLBACK_API_KEY") or os.getenv("EMBEDDINGS_API_KEY")
    base_url = os.getenv("EMBEDDINGS_FALLBACK_API_BASE") or os.getenv("EMBEDDINGS_API_BASE")
    
    if not api_key:
        raise ValueError(
            "No API key configured for fallback embeddings. Please set EMBEDDINGS_FALLBACK_API_KEY or EMBEDDINGS_API_KEY"
        )
    
    # Log configuration for debugging (without exposing API key)
    if base_url:
        print(f"DEBUG: Using fallback embeddings API endpoint: {base_url}")
    else:
        print("DEBUG: Using default OpenAI API endpoint for fallback embeddings")
    
    # Create client with optional base_url
    if base_url:
        return openai.OpenAI(api_key=api_key, base_url=base_url)
    else:
        return openai.OpenAI(api_key=api_key)

def get_adaptive_chat_client(model_preference=None):
    """
    Get an adaptive OpenAI client that tries primary first, then fallback.
    
    Args:
        model_preference: Optional model name preference (uses environment fallback if None)
        
    Returns:
        tuple: (client, model_used, is_fallback)
            - client: Configured OpenAI client
            - model_used: The model name that will be used
            - is_fallback: Boolean indicating if fallback configuration is being used
            
    Raises:
        ValueError: If neither primary nor fallback configuration is available
    """
    # Determine model to use
    if model_preference:
        model_used = model_preference
        is_fallback = False
    else:
        # Try primary model first
        primary_model = os.getenv("CHAT_MODEL")
        if primary_model:
            model_used = primary_model
            is_fallback = False
        else:
            # Fall back to fallback model
            fallback_model = os.getenv("CHAT_FALLBACK_MODEL") or "gpt-4o-mini"
            model_used = fallback_model
            is_fallback = True
    
    # Try to get appropriate client
    try:
        if not is_fallback and os.getenv("CHAT_API_KEY"):
            # Try primary client first
            client = get_chat_client()
            return (client, model_used, False)
        else:
            # Use fallback client
            client = get_chat_fallback_client()
            # If we're using fallback client, make sure we're using fallback model
            if not model_preference:
                fallback_model = os.getenv("CHAT_FALLBACK_MODEL") or "gpt-4o-mini"
                model_used = fallback_model
            return (client, model_used, True)
    except ValueError as e:
        # If primary fails, try fallback
        if not is_fallback:
            try:
                client = get_chat_fallback_client()
                fallback_model = os.getenv("CHAT_FALLBACK_MODEL") or "gpt-4o-mini"
                return (client, fallback_model, True)
            except ValueError:
                pass
        
        # Both failed
        raise ValueError(
            f"No valid API configuration available for chat model. "
            f"Please configure CHAT_API_KEY or CHAT_FALLBACK_API_KEY. Original error: {str(e)}"
        )

def validate_chat_config() -> bool:
    """
    Validate chat model configuration and provide helpful guidance.
    
    Returns:
        bool: True if configuration is valid, False otherwise
        
    Raises:
        ValueError: If critical configuration is missing
    """
    # Check for API key
    chat_api_key = os.getenv("CHAT_API_KEY")
    
    if not chat_api_key:
        raise ValueError(
            "No API key configured for chat model. Please set CHAT_API_KEY"
        )
    
    # Check for model configuration
    chat_model = os.getenv("CHAT_MODEL")
    
    if not chat_model:
        logging.warning(
            "No chat model specified. Please set CHAT_MODEL environment variable. "
            "Defaulting to configured fallback model."
        )
    
    # Log configuration being used
    effective_key_source = "CHAT_API_KEY"
    effective_model = chat_model or os.getenv("CHAT_FALLBACK_MODEL") or "default"
    base_url = os.getenv("CHAT_API_BASE", "default OpenAI")
    
    logging.debug(f"Chat configuration - Model: {effective_model}, Key source: {effective_key_source}, Base URL: {base_url}")
    
    return True


def validate_chat_fallback_config() -> bool:
    """
    Validate chat fallback model configuration with inheritance support.
    
    Returns:
        bool: True if fallback configuration is valid (direct or inherited)
        
    Raises:
        ValueError: If no valid fallback configuration is available
    """
    # Check for fallback API key (direct or inherited)
    fallback_api_key = os.getenv("CHAT_FALLBACK_API_KEY") or os.getenv("CHAT_API_KEY")
    
    if not fallback_api_key:
        raise ValueError(
            "No API key configured for chat fallback. Please set CHAT_FALLBACK_API_KEY or CHAT_API_KEY"
        )
    
    # Validate base URL format if provided
    fallback_base_url = os.getenv("CHAT_FALLBACK_API_BASE") or os.getenv("CHAT_API_BASE")
    if fallback_base_url:
        try:
            from urllib.parse import urlparse
            parsed = urlparse(fallback_base_url)
            if not parsed.scheme or not parsed.netloc:
                raise ValueError(f"Invalid base URL format: {fallback_base_url}")
        except Exception as e:
            raise ValueError(f"Invalid fallback base URL configuration: {str(e)}")
    
    # Log effective configuration for debugging
    effective_key_source = "CHAT_FALLBACK_API_KEY" if os.getenv("CHAT_FALLBACK_API_KEY") else "CHAT_API_KEY (inherited)"
    effective_base_source = "CHAT_FALLBACK_API_BASE" if os.getenv("CHAT_FALLBACK_API_BASE") else "CHAT_API_BASE (inherited)" if fallback_base_url else "default OpenAI"
    fallback_model = os.getenv("CHAT_FALLBACK_MODEL", "gpt-4o-mini")
    
    logging.debug(f"Chat fallback configuration - Model: {fallback_model}, Key source: {effective_key_source}, Base URL source: {effective_base_source}")
    
    return True

def validate_embeddings_fallback_config() -> bool:
    """
    Validate embeddings fallback model configuration with inheritance support.
    
    Returns:
        bool: True if fallback configuration is valid (direct or inherited)
        
    Raises:
        ValueError: If no valid fallback configuration is available
    """
    # Check for fallback API key (direct or inherited)
    fallback_api_key = os.getenv("EMBEDDINGS_FALLBACK_API_KEY") or os.getenv("EMBEDDINGS_API_KEY")
    
    if not fallback_api_key:
        raise ValueError(
            "No API key configured for embeddings fallback. Please set EMBEDDINGS_FALLBACK_API_KEY or EMBEDDINGS_API_KEY"
        )
    
    # Validate base URL format if provided
    fallback_base_url = os.getenv("EMBEDDINGS_FALLBACK_API_BASE") or os.getenv("EMBEDDINGS_API_BASE")
    if fallback_base_url:
        try:
            from urllib.parse import urlparse
            parsed = urlparse(fallback_base_url)
            if not parsed.scheme or not parsed.netloc:
                raise ValueError(f"Invalid base URL format: {fallback_base_url}")
        except Exception as e:
            raise ValueError(f"Invalid fallback base URL configuration: {str(e)}")
    
    # Log effective configuration for debugging
    effective_key_source = "EMBEDDINGS_FALLBACK_API_KEY" if os.getenv("EMBEDDINGS_FALLBACK_API_KEY") else "EMBEDDINGS_API_KEY (inherited)"
    effective_base_source = "EMBEDDINGS_FALLBACK_API_BASE" if os.getenv("EMBEDDINGS_FALLBACK_API_BASE") else "EMBEDDINGS_API_BASE (inherited)" if fallback_base_url else "default OpenAI"
    fallback_model = os.getenv("EMBEDDINGS_FALLBACK_MODEL", "text-embedding-3-small")
    
    logging.debug(f"Embeddings fallback configuration - Model: {fallback_model}, Key source: {effective_key_source}, Base URL source: {effective_base_source}")
    
    return True

def get_effective_fallback_config():
    """
    Get the effective fallback configuration with inheritance resolved.
    Useful for debugging and monitoring.
    
    Returns:
        dict: Configuration details showing what will actually be used
    """
    config = {
        "chat_fallback": {
            "model": os.getenv("CHAT_FALLBACK_MODEL", "gpt-4o-mini"),
            "api_key_source": "CHAT_FALLBACK_API_KEY" if os.getenv("CHAT_FALLBACK_API_KEY") else "CHAT_API_KEY (inherited)" if os.getenv("CHAT_API_KEY") else None,
            "base_url": os.getenv("CHAT_FALLBACK_API_BASE") or os.getenv("CHAT_API_BASE"),
            "base_url_source": "CHAT_FALLBACK_API_BASE" if os.getenv("CHAT_FALLBACK_API_BASE") else "CHAT_API_BASE (inherited)" if os.getenv("CHAT_API_BASE") else "default OpenAI"
        },
        "embeddings_fallback": {
            "model": os.getenv("EMBEDDINGS_FALLBACK_MODEL", "text-embedding-3-small"),
            "api_key_source": "EMBEDDINGS_FALLBACK_API_KEY" if os.getenv("EMBEDDINGS_FALLBACK_API_KEY") else "EMBEDDINGS_API_KEY (inherited)" if os.getenv("EMBEDDINGS_API_KEY") else None,
            "base_url": os.getenv("EMBEDDINGS_FALLBACK_API_BASE") or os.getenv("EMBEDDINGS_API_BASE"),
            "base_url_source": "EMBEDDINGS_FALLBACK_API_BASE" if os.getenv("EMBEDDINGS_FALLBACK_API_BASE") else "EMBEDDINGS_API_BASE (inherited)" if os.getenv("EMBEDDINGS_API_BASE") else "default OpenAI"
        }
    }
    
    return config

def get_supabase_client():
    """
    DEPRECATED: Legacy function name maintained for compatibility.
    Returns Qdrant client wrapper instead.
    """
    return get_qdrant_client()

def create_embeddings_batch(texts: List[str]) -> List[List[float]]:
    """
    Create embeddings for multiple texts with Redis caching support.
    
    This function implements a high-performance caching layer that:
    - Checks Redis cache for existing embeddings first
    - Only calls external APIs for cache misses
    - Stores new embeddings in cache for future use
    - Provides graceful degradation when cache is unavailable
    
    Args:
        texts: List of texts to create embeddings for
        
    Returns:
        List of embeddings (each embedding is a list of floats)
    """
    if not texts:
        return []
    
    embeddings_model = os.getenv("EMBEDDINGS_MODEL", "text-embedding-3-small")
    final_embeddings = [None] * len(texts)
    
    # Try cache first if available
    cache = get_embedding_cache()
    if cache:
        cached_embeddings = cache.get_batch(texts, embeddings_model)
        
        texts_to_embed = []
        indices_to_embed = []
        
        for i, text in enumerate(texts):
            if text in cached_embeddings:
                final_embeddings[i] = cached_embeddings[text]  # Cache hit
                logging.debug(f"Cache hit for text {i}")
            else:
                texts_to_embed.append(text)  # Cache miss
                indices_to_embed.append(i)
        
        if cached_embeddings:
            logging.info(f"Cache hits: {len(cached_embeddings)}/{len(texts)} embeddings")
    else:
        # No cache available, embed all texts
        texts_to_embed = texts
        indices_to_embed = list(range(len(texts)))
    
    # Create embeddings for cache misses using existing retry logic
    if texts_to_embed:
        logging.info(f"Creating {len(texts_to_embed)} new embeddings via API")
        new_embeddings_list = _create_embeddings_api_call(texts_to_embed)
        
        # Store new embeddings in cache
        if cache and new_embeddings_list:
            new_to_cache = {text: emb for text, emb in zip(texts_to_embed, new_embeddings_list)}
            ttl = int(os.getenv("REDIS_EMBEDDING_TTL", "86400"))
            cache.set_batch(new_to_cache, embeddings_model, ttl)
            logging.debug(f"Cached {len(new_to_cache)} new embeddings")
        
        # Place new embeddings in correct positions
        for i, new_embedding in enumerate(new_embeddings_list):
            original_index = indices_to_embed[i]
            final_embeddings[original_index] = new_embedding
    
    return final_embeddings


def _create_embeddings_api_call(texts: List[str]) -> List[List[float]]:
    """
    Create embeddings via API call with retry logic (extracted from original function).
    
    Args:
        texts: List of texts to create embeddings for
        
    Returns:
        List of embeddings (each embedding is a list of floats)
    """
    if not texts:
        return []
    
    max_retries = 3
    retry_delay = 1.0  # Start with 1 second delay
    
    for retry in range(max_retries):
        try:
            embeddings_model = os.getenv("EMBEDDINGS_MODEL", "text-embedding-3-small")
            client = get_embeddings_client()
            response = client.embeddings.create(
                model=embeddings_model,
                input=texts,
                encoding_format="float"  # Explicitly set encoding format for DeepInfra compatibility
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            if retry < max_retries - 1:
                print(f"Error creating batch embeddings (attempt {retry + 1}/{max_retries}): {e}")
                print(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2  # Exponential backoff
            else:
                print(f"Failed to create batch embeddings after {max_retries} attempts: {e}")
                # Try creating embeddings one by one as fallback
                print("Attempting to create embeddings individually...")
                embeddings = []
                successful_count = 0
                
                for i, text in enumerate(texts):
                    try:
                        embeddings_model = os.getenv("EMBEDDINGS_MODEL", "text-embedding-3-small")
                        client = get_embeddings_client()
                        individual_response = client.embeddings.create(
                            model=embeddings_model,
                            input=[text],
                            encoding_format="float"  # Explicitly set encoding format for DeepInfra compatibility
                        )
                        embeddings.append(individual_response.data[0].embedding)
                        successful_count += 1
                    except Exception as individual_error:
                        print(f"Failed to create embedding for text {i}: {individual_error}")
                        # Add zero embedding as fallback
                        embeddings.append([0.0] * get_embedding_dimensions())
                
                print(f"Successfully created {successful_count}/{len(texts)} embeddings individually")
                return embeddings

def create_embedding(text: str) -> List[float]:
    """
    Create an embedding for a single text using OpenAI's API.
    
    Args:
        text: Text to create an embedding for
        
    Returns:
        List of floats representing the embedding
    """
    try:
        embeddings = create_embeddings_batch([text])
        return embeddings[0] if embeddings else [0.0] * get_embedding_dimensions()
    except Exception as e:
        print(f"Error creating embedding: {e}")
        # Return empty embedding if there's an error
        return [0.0] * get_embedding_dimensions()

def generate_contextual_embedding(full_document: str, chunk: str) -> Tuple[str, bool]:
    """
    Generate contextual information for a chunk within a document to improve retrieval.
    
    Uses the chat model configured via CHAT_MODEL environment variable with explicit fallback configuration.
    
    Args:
        full_document: The complete document text
        chunk: The specific chunk of text to generate context for
        
    Returns:
        Tuple containing:
        - The contextual text that situates the chunk within the document
        - Boolean indicating if contextual embedding was performed
    """
    # Get chat model with modern fallback configuration
    model_choice = os.getenv("CHAT_MODEL") or os.getenv("CHAT_FALLBACK_MODEL") or "gpt-4o-mini"
    
    if not model_choice:
        print("Warning: No chat model configured. Set CHAT_MODEL environment variable.")
        return chunk, False
    
    try:
        # Optimize prompt for better token efficiency
        # Reduce document size for better context generation
        doc_limit = 15000 if "gemini" in model_choice.lower() else 25000
        
        # Create a more concise prompt for Gemini models
        if "gemini" in model_choice.lower():
            prompt = f"""Document excerpt: {full_document[:doc_limit]}

Chunk to contextualize: {chunk}

Provide 1-2 sentences of context for this chunk within the document. Be concise."""
        else:
            prompt = f"""<document> 
{full_document[:doc_limit]} 
</document>
Here is the chunk we want to situate within the whole document 
<chunk> 
{chunk}
</chunk> 
Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else."""

        # Get adaptive client that can fallback to different provider
        client, actual_model, is_fallback = get_adaptive_chat_client()
        
        # Adjust max_tokens based on actual model being used
        max_tokens = 150 if "gemini" in actual_model.lower() else 200

        # Call the API to generate contextual information
        response = client.chat.completions.create(
            model=actual_model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that provides concise contextual information."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=max_tokens
        )
        
        # Validate response structure
        if not response.choices:
            print(f"Warning: API returned no choices. Model: {model_choice}")
            return chunk, False
        
        choice = response.choices[0]
        
        # Check if response was truncated due to length
        if choice.finish_reason == 'length' and choice.message.content is None:
            print(f"Warning: Model {model_choice} hit token limit before generating content. Trying shorter prompt.")
            
            # Retry with much shorter prompt for Gemini
            short_prompt = f"Context for '{chunk[:100]}...' in document about: {full_document[:500]}... \nProvide brief context (1 sentence):"
            
            response = client.chat.completions.create(
                model=model_choice,
                messages=[{"role": "user", "content": short_prompt}],
                temperature=0.3,
                max_tokens=50
            )
            
            if response.choices and response.choices[0].message.content:
                choice = response.choices[0]
            else:
                print(f"Warning: Even shorter prompt failed for {model_choice}. Using original chunk.")
                return chunk, False
            
        # Extract the generated context with null check
        content = choice.message.content
        if content is None:
            print(f"Warning: API returned None content for contextual embedding. Model: {model_choice}")
            print(f"Finish reason: {choice.finish_reason}")
            return chunk, False
            
        context = content.strip()
        if not context:
            print(f"Warning: API returned empty content for contextual embedding. Model: {model_choice}")
            return chunk, False
        
        # Combine the context with the original chunk
        contextual_text = f"{context}\n---\n{chunk}"
        
        return contextual_text, True
    
    except Exception as e:
        print(f"Error generating contextual embedding with model {model_choice}: {e}. Using original chunk instead.")
        return chunk, False

def process_chunk_with_context(args):
    """
    Process a single chunk with contextual embedding.
    This function is designed to be used with concurrent.futures.
    
    Args:
        args: Tuple containing (url, content, full_document)
        
    Returns:
        Tuple containing:
        - The contextual text that situates the chunk within the document
        - Boolean indicating if contextual embedding was performed
    """
    url, content, full_document = args
    return generate_contextual_embedding(full_document, content)

def add_documents_to_supabase(
    client: QdrantClientWrapper, 
    urls: List[str], 
    chunk_numbers: List[int],
    contents: List[str], 
    metadatas: List[Dict[str, Any]],
    url_to_full_document: Dict[str, str],
    batch_size: int = 100
) -> None:
    """
    Add documents to Qdrant crawled_pages collection.
    LEGACY FUNCTION NAME: Maintained for compatibility.
    
    Args:
        client: Qdrant client wrapper
        urls: List of URLs
        chunk_numbers: List of chunk numbers
        contents: List of document contents
        metadatas: List of document metadata
        url_to_full_document: Dictionary mapping URLs to their full document content
        batch_size: Size of each batch for insertion
    """
    if not urls:
        return
    
    # Check if contextual embeddings are enabled
    use_contextual_embeddings = os.getenv("USE_CONTEXTUAL_EMBEDDINGS", "false") == "true"
    print(f"\n\nUse contextual embeddings: {use_contextual_embeddings}\n\n")
    
    # Get point batches from Qdrant client (this handles URL deletion)
    point_batches = list(client.add_documents_to_qdrant(
        urls, chunk_numbers, contents, metadatas, url_to_full_document, batch_size
    ))
    
    # Process each batch
    for batch_idx, points_batch in enumerate(point_batches):
        batch_contents = [point["content"] for point in points_batch]
        
        # Apply contextual embedding if enabled
        if use_contextual_embeddings:
            # Prepare arguments for parallel processing
            process_args = []
            for i, point in enumerate(points_batch):
                url = point["payload"]["url"]
                content = point["content"]
                full_document = url_to_full_document.get(url, "")
                process_args.append((url, content, full_document))
            
            # Process in parallel using ThreadPoolExecutor
            contextual_contents = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                # Submit all tasks and collect results
                future_to_idx = {executor.submit(process_chunk_with_context, arg): idx 
                                for idx, arg in enumerate(process_args)}
                
                # Process results as they complete
                results = [None] * len(process_args)  # Pre-allocate to maintain order
                for future in concurrent.futures.as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        result, success = future.result()
                        results[idx] = result
                        if success:
                            points_batch[idx]["payload"]["contextual_embedding"] = True
                    except Exception as e:
                        print(f"Error processing chunk {idx}: {e}")
                        # Use original content as fallback
                        results[idx] = batch_contents[idx]
                
                contextual_contents = results
        else:
            # If not using contextual embeddings, use original contents
            contextual_contents = batch_contents
        
        # Create embeddings for the batch
        batch_embeddings = create_embeddings_batch(contextual_contents)
        
        # Create PointStruct objects
        qdrant_points = []
        for i, point in enumerate(points_batch):
            qdrant_points.append(PointStruct(
                id=point["id"],
                vector=batch_embeddings[i],
                payload=point["payload"]
            ))
        
        # Upsert batch to Qdrant
        try:
            client.upsert_points("crawled_pages", qdrant_points)
            print(f"Successfully inserted batch {batch_idx + 1}/{len(point_batches)}")
        except Exception as e:
            print(f"Error inserting batch {batch_idx + 1}: {e}")
            # Try inserting points individually as fallback
            successful_inserts = 0
            for point in qdrant_points:
                try:
                    client.upsert_points("crawled_pages", [point])
                    successful_inserts += 1
                except Exception as individual_error:
                    print(f"Failed to insert individual point {point.id}: {individual_error}")
            
            if successful_inserts > 0:
                print(f"Successfully inserted {successful_inserts}/{len(qdrant_points)} points individually")

def search_documents(
    client: QdrantClientWrapper, 
    query: str, 
    match_count: int = 10, 
    filter_metadata: Optional[Dict[str, Any]] = None
) -> List[Dict[str, Any]]:
    """
    Search for documents using Qdrant vector similarity.
    
    Args:
        client: Qdrant client wrapper
        query: Query text
        match_count: Maximum number of results to return
        filter_metadata: Optional metadata filter
        
    Returns:
        List of matching documents
    """
    # Create embedding for the query
    query_embedding = create_embedding(query)
    
    # Execute the search using Qdrant client
    try:
        results = client.search_documents(
            query_embedding=query_embedding,
            match_count=match_count,
            filter_metadata=filter_metadata
        )
        return results
    except Exception as e:
        print(f"Error searching documents: {e}")
        return []

def extract_code_blocks(markdown_content: str, min_length: int = 1000) -> List[Dict[str, Any]]:
    """
    Extract code blocks from markdown content along with context.
    
    Args:
        markdown_content: The markdown content to extract code blocks from
        min_length: Minimum length of code blocks to extract (default: 1000 characters)
        
    Returns:
        List of dictionaries containing code blocks and their context
    """
    code_blocks = []
    
    # Skip if content starts with triple backticks (edge case for files wrapped in backticks)
    content = markdown_content.strip()
    start_offset = 0
    if content.startswith('```'):
        # Skip the first triple backticks
        start_offset = 3
        print("Skipping initial triple backticks")
    
    # Find all occurrences of triple backticks
    backtick_positions = []
    pos = start_offset
    while True:
        pos = markdown_content.find('```', pos)
        if pos == -1:
            break
        backtick_positions.append(pos)
        pos += 3
    
    # Process pairs of backticks
    i = 0
    while i < len(backtick_positions) - 1:
        start_pos = backtick_positions[i]
        end_pos = backtick_positions[i + 1]
        
        # Extract the content between backticks
        code_section = markdown_content[start_pos+3:end_pos]
        
        # Check if there's a language specifier on the first line
        lines = code_section.split('\n', 1)
        if len(lines) > 1:
            # Check if first line is a language specifier (no spaces, common language names)
            first_line = lines[0].strip()
            if first_line and ' ' not in first_line and len(first_line) < 20:
                language = first_line
                code_content = lines[1].strip() if len(lines) > 1 else ""
            else:
                language = ""
                code_content = code_section.strip()
        else:
            language = ""
            code_content = code_section.strip()
        
        # Skip if code block is too short
        if len(code_content) < min_length:
            i += 2  # Move to next pair
            continue
        
        # Extract context before (1000 chars)
        context_start = max(0, start_pos - 1000)
        context_before = markdown_content[context_start:start_pos].strip()
        
        # Extract context after (1000 chars)
        context_end = min(len(markdown_content), end_pos + 3 + 1000)
        context_after = markdown_content[end_pos + 3:context_end].strip()
        
        code_blocks.append({
            'code': code_content,
            'language': language,
            'context_before': context_before,
            'context_after': context_after,
            'full_context': f"{context_before}\n\n{code_content}\n\n{context_after}"
        })
        
        # Move to next pair (skip the closing backtick we just processed)
        i += 2
    
    return code_blocks

def generate_code_example_summary(code: str, context_before: str, context_after: str) -> str:
    """
    Generate a summary for a code example using its surrounding context.
    
    Uses the chat model configured via CHAT_MODEL environment variable with explicit fallback configuration.
    
    Args:
        code: The code example
        context_before: Context before the code
        context_after: Context after the code
        
    Returns:
        A summary of what the code example demonstrates
    """
    # Create the prompt
    prompt = f"""<context_before>
{context_before[-500:] if len(context_before) > 500 else context_before}
</context_before>

<code_example>
{code[:1500] if len(code) > 1500 else code}
</code_example>

<context_after>
{context_after[:500] if len(context_after) > 500 else context_after}
</context_after>

Based on the code example and its surrounding context, provide a concise summary (2-3 sentences) that describes what this code example demonstrates and its purpose. Focus on the practical application and key concepts illustrated.
"""
    
    try:
        # Get adaptive client that can fallback to different provider
        client, actual_model, is_fallback = get_adaptive_chat_client()
        response = client.chat.completions.create(
            model=actual_model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that provides concise code example summaries."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=100
        )
        
        return response.choices[0].message.content.strip()
    
    except Exception as e:
        print(f"Error generating code example summary: {e}")
        return "Code example for demonstration purposes."

def add_code_examples_to_supabase(
    client: QdrantClientWrapper,
    urls: List[str],
    chunk_numbers: List[int],
    code_examples: List[str],
    summaries: List[str],
    metadatas: List[Dict[str, Any]],
    batch_size: int = 100
):
    """
    Add code examples to Qdrant code_examples collection.
    LEGACY FUNCTION NAME: Maintained for compatibility.
    
    Args:
        client: Qdrant client wrapper
        urls: List of URLs
        chunk_numbers: List of chunk numbers
        code_examples: List of code example contents
        summaries: List of code example summaries
        metadatas: List of metadata dictionaries
        batch_size: Size of each batch for insertion
    """
    if not urls:
        return
    
    # Get point batches from Qdrant client (this handles URL deletion)
    point_batches = list(client.add_code_examples_to_qdrant(
        urls, chunk_numbers, code_examples, summaries, metadatas, batch_size
    ))
    
    # Process each batch
    for batch_idx, points_batch in enumerate(point_batches):
        # Create embeddings for combined text (code + summary)
        combined_texts = [point["combined_text"] for point in points_batch]
        embeddings = create_embeddings_batch(combined_texts)
        
        # Check if embeddings are valid (not all zeros)
        valid_embeddings = []
        for i, embedding in enumerate(embeddings):
            if embedding and not all(v == 0.0 for v in embedding):
                valid_embeddings.append(embedding)
            else:
                print("Warning: Zero or invalid embedding detected, creating new one...")
                # Try to create a single embedding as fallback
                single_embedding = create_embedding(combined_texts[i])
                valid_embeddings.append(single_embedding)
        
        # Create PointStruct objects
        qdrant_points = []
        for i, point in enumerate(points_batch):
            qdrant_points.append(PointStruct(
                id=point["id"],
                vector=valid_embeddings[i],
                payload=point["payload"]
            ))
        
        # Upsert batch to Qdrant
        try:
            client.upsert_points("code_examples", qdrant_points)
            print(f"Inserted batch {batch_idx + 1} of {len(point_batches)} code examples")
        except Exception as e:
            print(f"Error inserting code examples batch {batch_idx + 1}: {e}")
            # Try inserting points individually as fallback
            successful_inserts = 0
            for point in qdrant_points:
                try:
                    client.upsert_points("code_examples", [point])
                    successful_inserts += 1
                except Exception as individual_error:
                    print(f"Failed to insert individual code example {point.id}: {individual_error}")
            
            if successful_inserts > 0:
                print(f"Successfully inserted {successful_inserts}/{len(qdrant_points)} code examples individually")

def update_source_info(client: QdrantClientWrapper, source_id: str, summary: str, word_count: int):
    """
    Update source information using Qdrant client wrapper.
    
    Args:
        client: Qdrant client wrapper
        source_id: The source ID (domain)
        summary: Summary of the source
        word_count: Total word count for the source
    """
    try:
        client.update_source_info(source_id, summary, word_count)
    except Exception as e:
        print(f"Error updating source {source_id}: {e}")

def extract_source_summary(source_id: str, content: str, max_length: int = 500) -> str:
    """
    Extract a summary for a source from its content using an LLM.
    
    Uses the chat model configured via CHAT_MODEL environment variable with explicit fallback configuration.
    
    Args:
        source_id: The source ID (domain)
        content: The content to extract a summary from
        max_length: Maximum length of the summary
        
    Returns:
        A summary string
    """
    # Default summary if we can't extract anything meaningful
    default_summary = f"Content from {source_id}"
    
    if not content or len(content.strip()) == 0:
        return default_summary
    
    # Limit content length to avoid token limits
    truncated_content = content[:25000] if len(content) > 25000 else content
    
    # Create the prompt for generating the summary
    prompt = f"""<source_content>
{truncated_content}
</source_content>

The above content is from the documentation for '{source_id}'. Please provide a concise summary (3-5 sentences) that describes what this library/tool/framework is about. The summary should help understand what the library/tool/framework accomplishes and the purpose.
"""
    
    try:
        # Get adaptive client that can fallback to different provider
        client, actual_model, is_fallback = get_adaptive_chat_client()
        response = client.chat.completions.create(
            model=actual_model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that provides concise library/tool/framework summaries."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=150
        )
        
        # Extract the generated summary
        summary = response.choices[0].message.content.strip()
        
        # Ensure the summary is not too long
        if len(summary) > max_length:
            summary = summary[:max_length] + "..."
            
        return summary
    
    except Exception as e:
        print(f"Error generating summary with LLM for {source_id}: {e}. Using default summary.")
        return default_summary

def search_code_examples(
    client: QdrantClientWrapper, 
    query: str, 
    match_count: int = 10, 
    filter_metadata: Optional[Dict[str, Any]] = None,
    source_id: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Search for code examples using Qdrant vector similarity.
    
    Args:
        client: Qdrant client wrapper
        query: Query text
        match_count: Maximum number of results to return
        filter_metadata: Optional metadata filter
        source_id: Optional source ID to filter results
        
    Returns:
        List of matching code examples
    """
    # Create a more descriptive query for better embedding match
    enhanced_query = f"Code example for {query}\n\nSummary: Example code showing {query}"
    
    # Create embedding for the enhanced query
    query_embedding = create_embedding(enhanced_query)
    
    # Execute the search using Qdrant client
    try:
        results = client.search_code_examples(
            query_embedding=query_embedding,
            match_count=match_count,
            filter_metadata=filter_metadata,
            source_filter=source_id
        )
        return results
    except Exception as e:
        print(f"Error searching code examples: {e}")
        return []


# Device Management and Diagnostics
# Import device management utilities
try:
    from .device_manager import get_device_info as _get_device_info, cleanup_gpu_memory, get_optimal_device
except ImportError:
    from device_manager import get_device_info as _get_device_info, cleanup_gpu_memory, get_optimal_device


def get_device_info() -> Dict[str, Any]:
    """
    Get comprehensive device information for diagnostics.
    
    Provides information about available compute devices (CPU, CUDA, MPS)
    including memory status, device capabilities, and availability.
    
    Returns:
        Dict with device capabilities and status information including:
        - torch_available: Whether PyTorch is available
        - cuda_available: Whether CUDA GPUs are available
        - mps_available: Whether Apple Silicon MPS is available
        - device_count: Number of available CUDA devices
        - devices: List of detailed device information
    """
    return _get_device_info()


def log_device_status() -> None:
    """
    Log comprehensive device information for debugging and monitoring.
    
    Logs device information including available GPUs, memory status,
    and device capabilities. Useful for troubleshooting GPU acceleration issues.
    """
    device_info = get_device_info()
    
    print("=== Device Status Report ===")
    print(f"PyTorch Available: {device_info['torch_available']}")
    print(f"CUDA Available: {device_info['cuda_available']}")
    print(f"MPS Available: {device_info['mps_available']}")
    print(f"CUDA Device Count: {device_info['device_count']}")
    
    if device_info['devices']:
        print("Available Devices:")
        for device in device_info['devices']:
            if 'name' in device and 'type' not in device:  # CUDA device
                print(f"  - CUDA {device['index']}: {device['name']}")
                print(f"    Total Memory: {device['memory_total_gb']:.2f} GB")
                print(f"    Allocated Memory: {device['memory_allocated_gb']:.2f} GB")
                print(f"    Current Device: {device['is_current']}")
            elif 'type' in device:  # MPS device
                print(f"  - {device['name']} ({device['type']})")
    else:
        print("No GPU devices available")
    
    print("============================")


def monitor_gpu_memory() -> Optional[Dict[str, float]]:
    """
    Monitor GPU memory usage for the current device.
    
    Returns:
        Dict with memory information in GB, or None if CUDA not available:
        - allocated: Currently allocated memory
        - reserved: Reserved memory (includes allocated)
        - max_allocated: Peak allocated memory since last reset
        - max_reserved: Peak reserved memory since last reset
        - total: Total device memory
    """
    try:
        import torch
        if not torch.cuda.is_available():
            return None
        
        current_device = torch.cuda.current_device()
        memory_info = {
            'allocated': torch.cuda.memory_allocated(current_device) / (1024**3),
            'reserved': torch.cuda.memory_reserved(current_device) / (1024**3),
            'max_allocated': torch.cuda.max_memory_allocated(current_device) / (1024**3),
            'max_reserved': torch.cuda.max_memory_reserved(current_device) / (1024**3),
            'total': torch.cuda.get_device_properties(current_device).total_memory / (1024**3)
        }
        return memory_info
    except Exception as e:
        print(f"Error monitoring GPU memory: {e}")
        return None


def log_gpu_memory_status() -> None:
    """
    Log current GPU memory status for monitoring and debugging.
    
    Provides detailed memory usage information including allocated,
    reserved, and peak usage statistics.
    """
    memory_info = monitor_gpu_memory()
    
    if memory_info is None:
        print("GPU memory monitoring not available (CUDA not detected)")
        return
    
    print("=== GPU Memory Status ===")
    print(f"Allocated: {memory_info['allocated']:.2f} GB")
    print(f"Reserved: {memory_info['reserved']:.2f} GB")
    print(f"Max Allocated: {memory_info['max_allocated']:.2f} GB")
    print(f"Max Reserved: {memory_info['max_reserved']:.2f} GB")
    print(f"Total Memory: {memory_info['total']:.2f} GB")
    print(f"Utilization: {(memory_info['allocated']/memory_info['total']*100):.1f}%")
    print("=========================")


def get_optimal_compute_device(preference: str = "auto") -> str:
    """
    Get the optimal compute device for machine learning operations.
    
    Provides a simple interface to device selection with fallback to CPU.
    This function wraps the more comprehensive device_manager functionality.
    
    Args:
        preference: Device preference - "auto", "cuda", "cpu", "mps"
        
    Returns:
        String representation of the optimal device (e.g., "cuda:0", "cpu")
    """
    try:
        device = get_optimal_device(preference)
        return str(device)
    except Exception as e:
        print(f"Error getting optimal device: {e}. Falling back to CPU.")
        return "cpu"


def cleanup_compute_memory() -> None:
    """
    Clean up compute memory (GPU cache) to prevent memory leaks.
    
    Safe wrapper around GPU memory cleanup that handles cases where
    GPU is not available. Should be called after intensive compute operations.
    """
    try:
        cleanup_gpu_memory()
    except Exception as e:
        print(f"Error during memory cleanup: {e}")


def health_check_gpu_acceleration() -> Dict[str, Any]:
    """
    Comprehensive health check for GPU acceleration capabilities.
    
    Performs actual device testing to verify GPU acceleration is working
    correctly. Useful for monitoring and troubleshooting deployment issues.
    
    Returns:
        Dict with health check results including:
        - gpu_available: Whether GPU is detected and working
        - device_name: Name of the GPU device
        - memory_available: Available GPU memory
        - test_passed: Whether GPU operations test passed
        - error_message: Error details if test failed
    """
    health_status = {
        'gpu_available': False,
        'device_name': 'CPU',
        'memory_available_gb': None,
        'test_passed': False,
        'error_message': None
    }
    
    try:
        import torch
        
        if torch.cuda.is_available():
            # Test actual GPU operations
            device = torch.device("cuda:0")
            
            # Perform test operation
            test_tensor = torch.randn(100, 100, device=device)
            _ = test_tensor @ test_tensor.T  # Test GPU matrix operation
            
            # If we get here, GPU test passed
            health_status.update({
                'gpu_available': True,
                'device_name': torch.cuda.get_device_name(device),
                'memory_available_gb': torch.cuda.get_device_properties(device).total_memory / (1024**3),
                'test_passed': True
            })
            
        elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            # Test MPS (Apple Silicon)
            device = torch.device("mps")
            test_tensor = torch.randn(100, 100, device=device)
            _ = test_tensor.sum()  # Test MPS functionality
            
            health_status.update({
                'gpu_available': True,
                'device_name': 'Apple Silicon GPU (MPS)',
                'memory_available_gb': None,  # MPS doesn't expose memory info
                'test_passed': True
            })
            
    except Exception as e:
        health_status['error_message'] = str(e)
    
    return health_status


def health_check_reranking_model(model=None) -> Dict[str, Any]:
    """
    Comprehensive health check for reranking model functionality.
    
    Validates the reranking model with dummy inference to ensure it's working
    correctly. Tests model loading, device allocation, and inference capability.
    
    Args:
        model: Optional CrossEncoder model instance. If None, attempts to access
               from the current lifespan context.
    
    Returns:
        Dict with health check results including:
        - model_available: Whether reranking model is loaded
        - model_name: Name of the loaded model  
        - device: Device the model is running on
        - inference_test_passed: Whether dummy inference succeeded
        - inference_latency_ms: Latency of dummy inference in milliseconds
        - error_message: Error details if test failed
    """
    health_status = {
        'model_available': False,
        'model_name': None,
        'device': None,
        'inference_test_passed': False,
        'inference_latency_ms': None,
        'error_message': None
    }
    
    try:
        # Import CrossEncoder here to avoid circular imports
        from sentence_transformers import CrossEncoder
        import time
        
        # If no model provided, try to get from environment or return not available
        if model is None:
            if os.getenv("USE_RERANKING", "false") != "true":
                health_status['error_message'] = "Reranking not enabled (USE_RERANKING=false)"
                return health_status
            
            # Model not provided and can't access it directly - return not available
            health_status['error_message'] = "Reranking model not accessible for health check"
            return health_status
        
        if not isinstance(model, CrossEncoder):
            health_status['error_message'] = "Invalid model type - expected CrossEncoder"
            return health_status
        
        # Model is available
        health_status['model_available'] = True
        
        # Get model information
        if hasattr(model, 'model') and hasattr(model.model, 'name_or_path'):
            health_status['model_name'] = model.model.name_or_path
        elif hasattr(model, '_model_name'):
            health_status['model_name'] = model._model_name
        else:
            health_status['model_name'] = 'Unknown'
        
        # Get device information
        if hasattr(model, 'device'):
            health_status['device'] = str(model.device)
        else:
            health_status['device'] = 'Unknown'
        
        # Perform dummy inference test
        dummy_pairs = [
            ["health check query", "health check document"],
            ["test reranking", "sample content for validation"]
        ]
        
        start_time = time.time()
        scores = model.predict(dummy_pairs)
        end_time = time.time()
        
        # Validate inference results
        if (isinstance(scores, (list, tuple)) and 
            len(scores) == 2 and 
            all(isinstance(score, (int, float)) for score in scores)):
            
            health_status['inference_test_passed'] = True
            health_status['inference_latency_ms'] = round((end_time - start_time) * 1000, 2)
            
            # Clean up GPU memory after test
            cleanup_gpu_memory()
        else:
            health_status['error_message'] = f"Invalid inference output: {type(scores)} with length {len(scores) if hasattr(scores, '__len__') else 'unknown'}"
            
    except ImportError as e:
        health_status['error_message'] = f"Missing required dependencies: {e}"
    except Exception as e:
        health_status['error_message'] = f"Health check failed: {e}"
    
    return health_status



================================================
FILE: src/utils/__init__.py
================================================
"""
Utility modules for the Crawl4AI MCP server.
"""

from .github_processor import GitHubRepoManager, MarkdownDiscovery, GitHubMetadataExtractor
from .validation import validate_github_url

# Import functions from the main utils.py for backward compatibility
import sys
from pathlib import Path

# Add parent directory to path to import from utils.py
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

try:
    from utils import (
        get_embeddings_client,
        get_supabase_client,
        health_check_gpu_acceleration,
        health_check_reranking_model,
        cleanup_gpu_memory,
        get_chat_client,
        create_embeddings_batch,
        create_embedding,
        add_documents_to_supabase,
        search_documents,
        extract_code_blocks,
        add_code_examples_to_supabase,
        search_code_examples,
        update_source_info,
        extract_source_summary,
        generate_contextual_embedding,
        get_device_info,
        log_device_status,
        cleanup_compute_memory,
        get_optimal_compute_device
    )
except ImportError:
    # If we can't import from utils, define minimal stubs
    def get_embeddings_client():
        raise NotImplementedError("get_embeddings_client not available")
    
    def get_supabase_client():
        raise NotImplementedError("get_supabase_client not available") 
        
    def health_check_gpu_acceleration():
        raise NotImplementedError("health_check_gpu_acceleration not available")
        
    def health_check_reranking_model():
        raise NotImplementedError("health_check_reranking_model not available")
        
    def cleanup_gpu_memory():
        raise NotImplementedError("cleanup_gpu_memory not available")

__all__ = [
    'GitHubRepoManager',
    'MarkdownDiscovery', 
    'GitHubMetadataExtractor',
    'validate_github_url',
    # Backward compatibility exports
    'get_embeddings_client',
    'get_supabase_client', 
    'health_check_gpu_acceleration',
    'health_check_reranking_model',
    'cleanup_gpu_memory',
    'get_chat_client',
    'create_embeddings_batch',
    'create_embedding',
    'add_documents_to_supabase',
    'search_documents',
    'extract_code_blocks',
    'add_code_examples_to_supabase',
    'search_code_examples',
    'update_source_info',
    'extract_source_summary',
    'generate_contextual_embedding',
    'get_device_info',
    'log_device_status',
    'cleanup_compute_memory',
    'get_optimal_compute_device'
]


================================================
FILE: src/utils/github_processor.py
================================================
"""
GitHub repository processing utilities for the Crawl4AI MCP server.

This module provides functionality to clone GitHub repositories, discover markdown files,
and extract metadata for storing in the vector database.
"""
import os
import re
import shutil
import stat
import subprocess
import tempfile
import logging
import ast
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import urlparse


class GitHubRepoManager:
    """Manages GitHub repository cloning and cleanup operations."""
    
    def __init__(self):
        self.temp_dirs: List[str] = []
        self.logger = logging.getLogger(__name__)
    
    def clone_repository(self, repo_url: str, max_size_mb: int = 500) -> str:
        """
        Clone a GitHub repository to a temporary directory with size checks.
        
        Args:
            repo_url: GitHub repository URL
            max_size_mb: Maximum repository size in MB (default: 500MB)
            
        Returns:
            Path to the cloned repository directory
            
        Raises:
            ValueError: If repository URL is invalid
            RuntimeError: If cloning fails or repository is too large
        """
        # Validate GitHub URL
        if not self._is_valid_github_url(repo_url):
            raise ValueError(f"Invalid GitHub repository URL: {repo_url}")
        
        # Create temporary directory
        temp_dir = tempfile.mkdtemp(prefix="github_clone_")
        self.temp_dirs.append(temp_dir)
        
        try:
            # Normalize URL for cloning
            clone_url = self._normalize_clone_url(repo_url)
            
            # Clone with depth=1 for efficiency
            self.logger.info(f"Cloning repository: {clone_url}")
            result = subprocess.run(
                ["git", "clone", "--depth", "1", clone_url, temp_dir],
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )
            
            if result.returncode != 0:
                raise RuntimeError(f"Git clone failed: {result.stderr}")
            
            # Check repository size
            repo_size_mb = self._get_directory_size_mb(temp_dir)
            if repo_size_mb > max_size_mb:
                raise RuntimeError(
                    f"Repository too large: {repo_size_mb:.1f}MB exceeds limit of {max_size_mb}MB"
                )
            
            self.logger.info(f"Successfully cloned repository ({repo_size_mb:.1f}MB)")
            return temp_dir
            
        except Exception as e:
            # Clean up on failure
            self._cleanup_directory(temp_dir)
            if temp_dir in self.temp_dirs:
                self.temp_dirs.remove(temp_dir)
            raise e
    
    def cleanup(self):
        """Clean up all temporary directories."""
        for temp_dir in self.temp_dirs:
            self._cleanup_directory(temp_dir)
        self.temp_dirs.clear()
    
    def _is_valid_github_url(self, url: str) -> bool:
        """Check if URL is a valid GitHub repository URL."""
        try:
            parsed = urlparse(url)
            if parsed.netloc not in ["github.com", "www.github.com"]:
                return False
            
            # Check path format: /owner/repo or /owner/repo.git
            path_parts = parsed.path.strip("/").split("/")
            if len(path_parts) < 2:
                return False
            
            # Basic validation - owner and repo should be non-empty
            owner, repo = path_parts[0], path_parts[1]
            if not owner or not repo:
                return False
            
            return True
        except Exception:
            return False
    
    def _normalize_clone_url(self, url: str) -> str:
        """Normalize GitHub URL for git clone."""
        url = url.rstrip("/")
        if not url.endswith(".git"):
            url += ".git"
        return url
    
    def _get_directory_size_mb(self, directory: str) -> float:
        """Calculate directory size in MB."""
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(directory):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(filepath)
                except (OSError, IOError):
                    # Skip files that can't be accessed
                    continue
        return total_size / (1024 * 1024)
    
    def _cleanup_directory(self, directory: str):
        """Safely remove a directory and its contents."""
        try:
            if os.path.exists(directory):
                # Handle Windows read-only files (common with Git repositories)
                def handle_remove_readonly(func, path, exc):
                    """Handle read-only files on Windows."""
                    if os.path.exists(path):
                        # Clear the readonly bit and try again
                        os.chmod(path, stat.S_IWRITE)
                        func(path)
                
                shutil.rmtree(directory, onerror=handle_remove_readonly)
                self.logger.debug(f"Cleaned up directory: {directory}")
        except Exception as e:
            self.logger.warning(f"Failed to cleanup directory {directory}: {e}")


class MarkdownDiscovery:
    """Discovers and filters markdown files in a repository."""
    
    # Common directories to exclude from markdown discovery
    EXCLUDED_DIRS = {
        ".git", ".github", "node_modules", "__pycache__", ".venv", "venv",
        "build", "dist", ".next", ".nuxt", "target", "vendor", ".cache",
        "coverage", ".coverage", "htmlcov", ".pytest_cache", ".mypy_cache",
        ".tox", ".eggs", "*.egg-info", ".DS_Store", "Thumbs.db"
    }
    
    # File patterns to exclude
    EXCLUDED_PATTERNS = {
        "CHANGELOG*", "HISTORY*", "NEWS*", "RELEASES*", 
        "*.lock", "package-lock.json", "yarn.lock", "Gemfile.lock",
        "*.min.*", "*.bundle.*"
    }
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def discover_markdown_files(
        self, 
        repo_path: str, 
        max_files: int = 100,
        min_size_bytes: int = 100,
        max_size_bytes: int = 1_000_000  # 1MB
    ) -> List[Dict[str, Any]]:
        """
        Discover markdown files in the repository with filtering.
        
        Args:
            repo_path: Path to the cloned repository
            max_files: Maximum number of files to process
            min_size_bytes: Minimum file size in bytes
            max_size_bytes: Maximum file size in bytes
            
        Returns:
            List of dictionaries containing file information
        """
        markdown_files = []
        processed_count = 0
        
        try:
            for root, dirs, files in os.walk(repo_path):
                # Filter out excluded directories
                dirs[:] = [d for d in dirs if not self._should_exclude_dir(d)]
                
                for file in files:
                    if processed_count >= max_files:
                        break
                    
                    if self._is_markdown_file(file):
                        file_path = os.path.join(root, file)
                        file_info = self._analyze_markdown_file(
                            file_path, repo_path, min_size_bytes, max_size_bytes
                        )
                        
                        if file_info:
                            markdown_files.append(file_info)
                            processed_count += 1
                
                if processed_count >= max_files:
                    break
            
            # Sort by priority (README files first, then by size)
            markdown_files.sort(key=self._file_priority_key, reverse=True)
            
            self.logger.info(f"Discovered {len(markdown_files)} markdown files")
            return markdown_files
            
        except Exception as e:
            self.logger.error(f"Error discovering markdown files: {e}")
            return []
    
    def _is_markdown_file(self, filename: str) -> bool:
        """Check if file is a markdown file."""
        return filename.lower().endswith(('.md', '.markdown', '.mdown', '.mkd'))
    
    def _should_exclude_dir(self, dirname: str) -> bool:
        """Check if directory should be excluded."""
        return dirname in self.EXCLUDED_DIRS or dirname.startswith('.')
    
    def _should_exclude_file(self, filename: str) -> bool:
        """Check if file should be excluded based on patterns."""
        import fnmatch
        for pattern in self.EXCLUDED_PATTERNS:
            if fnmatch.fnmatch(filename.lower(), pattern.lower()):
                return True
        return False
    
    def _analyze_markdown_file(
        self, 
        file_path: str, 
        repo_path: str, 
        min_size: int, 
        max_size: int
    ) -> Optional[Dict[str, Any]]:
        """
        Analyze a markdown file and return its metadata.
        
        Args:
            file_path: Absolute path to the file
            repo_path: Path to the repository root
            min_size: Minimum file size in bytes
            max_size: Maximum file size in bytes
            
        Returns:
            Dictionary with file metadata or None if file should be skipped
        """
        try:
            file_stat = os.stat(file_path)
            file_size = file_stat.st_size
            
            # Size filtering
            if file_size < min_size or file_size > max_size:
                return None
            
            # Read file content
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Skip if content is too short or looks like binary
            if len(content.strip()) < 50:
                return None
            
            # Calculate relative path
            relative_path = os.path.relpath(file_path, repo_path)
            
            return {
                'path': file_path,
                'relative_path': relative_path,
                'filename': os.path.basename(file_path),
                'size_bytes': file_size,
                'content': content,
                'word_count': len(content.split()),
                'is_readme': self._is_readme_file(os.path.basename(file_path))
            }
            
        except Exception as e:
            self.logger.warning(f"Error analyzing file {file_path}: {e}")
            return None
    
    def _is_readme_file(self, filename: str) -> bool:
        """Check if file is a README file."""
        return filename.lower().startswith('readme')
    
    def _file_priority_key(self, file_info: Dict[str, Any]) -> Tuple[int, int]:
        """Generate priority key for sorting files."""
        # README files get highest priority
        readme_priority = 1 if file_info['is_readme'] else 0
        
        # Size priority (moderate size preferred)
        size_priority = min(file_info['word_count'], 5000)
        
        return (readme_priority, size_priority)


class FileTypeProcessor:
    """Base class for file type processors."""
    
    def process_file(self, file_path: str, relative_path: str) -> List[Dict[str, Any]]:
        """Process file and return list of extractable content chunks."""
        raise NotImplementedError


class MarkdownProcessor(FileTypeProcessor):
    """Process markdown files using existing content."""
    
    def process_file(self, file_path: str, relative_path: str) -> List[Dict[str, Any]]:
        """Process markdown file and return content."""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
            
            if len(content) < 50:
                return []
            
            return [{
                'content': content,
                'type': 'markdown',
                'name': os.path.basename(file_path),
                'signature': None,
                'line_number': 1,
                'language': 'markdown'
            }]
            
        except Exception:
            return []


class PythonProcessor(FileTypeProcessor):
    """Process Python files using AST for docstring extraction."""
    
    def process_file(self, file_path: str, relative_path: str) -> List[Dict[str, Any]]:
        """Extract docstrings from Python file using AST."""
        try:
            # Size check - skip large Python files
            file_size = os.path.getsize(file_path)
            if file_size > 1_000_000:  # 1MB limit
                return []
            
            with open(file_path, 'r', encoding='utf-8') as f:
                source = f.read()
            
            tree = ast.parse(source, filename=file_path)
            extracted_items = []
            
            # Module docstring
            module_doc = ast.get_docstring(tree, clean=True)
            if module_doc:
                extracted_items.append({
                    'content': module_doc,
                    'type': 'module',
                    'name': relative_path,
                    'signature': None,
                    'line_number': 1,
                    'language': 'python'
                })
            
            # Walk AST for functions and classes
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    docstring = ast.get_docstring(node, clean=True)
                    if docstring:
                        extracted_items.append({
                            'content': docstring,
                            'type': 'function',
                            'name': node.name,
                            'signature': self._extract_signature(node),
                            'line_number': node.lineno,
                            'language': 'python'
                        })
                
                elif isinstance(node, ast.ClassDef):
                    docstring = ast.get_docstring(node, clean=True)
                    if docstring:
                        extracted_items.append({
                            'content': docstring,
                            'type': 'class',
                            'name': node.name,
                            'signature': None,
                            'line_number': node.lineno,
                            'language': 'python'
                        })
            
            return extracted_items
            
        except SyntaxError:
            # Skip files with syntax errors
            return []
        except Exception:
            return []
    
    def _extract_signature(self, node: ast.FunctionDef) -> str:
        """Extract function signature with type annotations."""
        try:
            args = []
            for arg in node.args.args:
                arg_str = arg.arg
                if arg.annotation:
                    arg_str += f": {ast.unparse(arg.annotation)}"
                args.append(arg_str)
            
            signature = f"({', '.join(args)})"
            if node.returns:
                signature += f" -> {ast.unparse(node.returns)}"
            
            return signature
        except Exception:
            return "(signature_extraction_failed)"


class TypeScriptProcessor(FileTypeProcessor):
    """Process TypeScript files for JSDoc comments."""
    
    def process_file(self, file_path: str, relative_path: str) -> List[Dict[str, Any]]:
        """Extract JSDoc comments from TypeScript file."""
        try:
            # Size check - skip large TypeScript files
            file_size = os.path.getsize(file_path)
            if file_size > 1_000_000:  # 1MB limit
                return []
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check if file is minified
            first_line = content.split('\n')[0] if '\n' in content else content
            if len(first_line) > 1000:
                return []  # Skip minified files
            
            # JSDoc comment pattern
            jsdoc_pattern = re.compile(
                r'/\*\*\s*\n((?:\s*\*[^\n]*\n)*)\s*\*/',
                re.MULTILINE | re.DOTALL
            )
            
            # Declaration patterns
            declaration_patterns = {
                'function': re.compile(
                    r'(?:export\s+)?(?:async\s+)?function\s+(\w+)\s*\([^)]*\)',
                    re.MULTILINE
                ),
                'class': re.compile(
                    r'(?:export\s+)?class\s+(\w+)',
                    re.MULTILINE
                ),
                'interface': re.compile(
                    r'(?:export\s+)?interface\s+(\w+)',
                    re.MULTILINE
                )
            }
            
            extracted_items = []
            
            for match in jsdoc_pattern.finditer(content):
                comment_text = match.group(1)
                start_pos = match.start()
                
                # Clean comment text
                lines = comment_text.split('\n')
                cleaned_lines = []
                for line in lines:
                    line = line.strip()
                    if line.startswith('*'):
                        line = line[1:].strip()
                    if line:
                        cleaned_lines.append(line)
                
                cleaned_comment = '\n'.join(cleaned_lines)
                line_number = content[:start_pos].count('\n') + 1
                
                # Find associated declaration
                after_comment = content[match.end():]
                declaration = self._find_next_declaration(after_comment, declaration_patterns)
                
                if declaration and cleaned_comment:
                    extracted_items.append({
                        'content': cleaned_comment,
                        'type': declaration['type'],
                        'name': declaration['name'], 
                        'signature': declaration.get('signature', ''),
                        'line_number': line_number,
                        'language': 'typescript'
                    })
            
            return extracted_items
            
        except Exception:
            return []
    
    def _find_next_declaration(self, content: str, declaration_patterns: Dict) -> Optional[Dict[str, Any]]:
        """Find the next function/class/interface declaration."""
        # Remove leading whitespace and newlines
        content = content.lstrip()
        
        # Try each declaration pattern
        for decl_type, pattern in declaration_patterns.items():
            match = pattern.search(content)
            if match and match.start() < 200:  # Must be close to comment
                return {
                    'type': decl_type,
                    'name': match.group(1),
                    'signature': match.group(0)
                }
        
        return None


class ConfigProcessor(FileTypeProcessor):
    """Process configuration files with full content."""
    
    def process_file(self, file_path: str, relative_path: str) -> List[Dict[str, Any]]:
        """Process configuration file and return full content."""
        try:
            # Size check - skip large config files
            file_size = os.path.getsize(file_path)
            if file_size > 100_000:  # 100KB limit
                return []
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if not content:
                return []
            
            # Determine file type
            ext = os.path.splitext(file_path)[1].lower()
            
            return [{
                'content': content,
                'type': 'configuration',
                'name': os.path.basename(file_path),
                'signature': None,
                'line_number': 1,
                'language': self._get_config_language(ext)
            }]
            
        except Exception:
            return []
    
    def _get_config_language(self, ext: str) -> str:
        """Map file extension to language."""
        mapping = {
            '.json': 'json',
            '.yaml': 'yaml', 
            '.yml': 'yaml',
            '.toml': 'toml'
        }
        return mapping.get(ext, 'text')


class MultiFileDiscovery(MarkdownDiscovery):
    """Enhanced file discovery supporting multiple file types."""
    
    SUPPORTED_EXTENSIONS = {
        '.md', '.markdown', '.mdown', '.mkd',        # Markdown
        '.py',                                        # Python
        '.ts', '.tsx',                               # TypeScript
        '.json', '.yaml', '.yml', '.toml'           # Configuration
    }
    
    # File size limits by type
    FILE_SIZE_LIMITS = {
        '.py': 1_000_000,      # 1MB for Python files
        '.ts': 1_000_000,      # 1MB for TypeScript files  
        '.tsx': 1_000_000,     # 1MB for TypeScript files
        '.json': 100_000,      # 100KB for JSON files
        '.yaml': 100_000,      # 100KB for YAML files
        '.yml': 100_000,       # 100KB for YAML files
        '.toml': 100_000,      # 100KB for TOML files
        '.md': 1_000_000,      # 1MB for Markdown files
        '.markdown': 1_000_000, '.mdown': 1_000_000, '.mkd': 1_000_000
    }
    
    def discover_files(
        self, 
        repo_path: str, 
        file_types: List[str] = ['.md'],
        max_files: int = 100
    ) -> List[Dict[str, Any]]:
        """
        Discover files of specified types with metadata.
        
        Args:
            repo_path: Path to the cloned repository
            file_types: List of file extensions to process
            max_files: Maximum number of files to process
            
        Returns:
            List of dictionaries containing file information
        """
        discovered_files = []
        processed_count = 0
        
        # Normalize file types
        file_types = [ft.lower() for ft in file_types]
        
        try:
            for root, dirs, files in os.walk(repo_path):
                # Filter out excluded directories
                dirs[:] = [d for d in dirs if not self._should_exclude_dir(d)]
                
                for file in files:
                    if processed_count >= max_files:
                        break
                    
                    if self._is_supported_file(file, file_types):
                        file_path = os.path.join(root, file)
                        file_info = self._analyze_file(file_path, repo_path, file_types)
                        
                        if file_info:
                            discovered_files.append(file_info)
                            processed_count += 1
                
                if processed_count >= max_files:
                    break
            
            # Sort by priority (README files first, then by size)
            discovered_files.sort(key=self._file_priority_key, reverse=True)
            
            self.logger.info(f"Discovered {len(discovered_files)} files of types {file_types}")
            return discovered_files
            
        except Exception as e:
            self.logger.error(f"Error discovering files: {e}")
            return []
    
    def _is_supported_file(self, filename: str, file_types: List[str]) -> bool:
        """Check if file is of a supported type."""
        file_ext = os.path.splitext(filename)[1].lower()
        
        # Check if extension is in requested file types
        if file_ext not in file_types:
            return False
        
        # Check if extension is in supported extensions
        if file_ext not in self.SUPPORTED_EXTENSIONS:
            return False
        
        # Additional filtering for excluded patterns
        if self._should_exclude_file(filename):
            return False
        
        return True
    
    def _analyze_file(
        self, 
        file_path: str, 
        repo_path: str, 
        file_types: List[str]
    ) -> Optional[Dict[str, Any]]:
        """
        Analyze a file and return its metadata.
        
        Args:
            file_path: Absolute path to the file
            repo_path: Path to the repository root
            file_types: List of requested file types
            
        Returns:
            Dictionary with file metadata or None if file should be skipped
        """
        try:
            file_stat = os.stat(file_path)
            file_size = file_stat.st_size
            file_ext = os.path.splitext(file_path)[1].lower()
            
            # Size filtering based on file type
            max_size = self.FILE_SIZE_LIMITS.get(file_ext, 1_000_000)
            if file_size > max_size:
                return None
            
            # Basic content check for text files
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    # Read first 1000 chars to check if it's text
                    sample = f.read(1000)
                    if '\x00' in sample:  # Binary file indicator
                        return None
            except UnicodeDecodeError:
                return None  # Skip non-UTF-8 files
            
            # Calculate relative path
            relative_path = os.path.relpath(file_path, repo_path)
            
            return {
                'path': file_path,
                'relative_path': relative_path,
                'filename': os.path.basename(file_path),
                'size_bytes': file_size,
                'file_type': file_ext,
                'is_readme': self._is_readme_file(os.path.basename(file_path)),
                'word_count': max(1, file_size // 5)  # Estimate word count for priority sorting
            }
            
        except Exception as e:
            self.logger.warning(f"Error analyzing file {file_path}: {e}")
            return None


class GitHubMetadataExtractor:
    """Extracts metadata from GitHub repositories."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def extract_repo_metadata(self, repo_url: str, repo_path: str) -> Dict[str, Any]:
        """
        Extract metadata from a GitHub repository.
        
        Args:
            repo_url: Original GitHub repository URL
            repo_path: Path to the cloned repository
            
        Returns:
            Dictionary containing repository metadata
        """
        try:
            # Parse repository info from URL
            owner, repo_name = self._parse_repo_info(repo_url)
            
            # Extract basic metadata
            metadata = {
                'repo_url': repo_url,
                'owner': owner,
                'repo_name': repo_name,
                'full_name': f"{owner}/{repo_name}",
                'source_type': 'github_repository',
                'clone_path': repo_path
            }
            
            # Try to extract additional metadata from common files
            metadata.update(self._extract_package_info(repo_path))
            metadata.update(self._extract_readme_info(repo_path))
            metadata.update(self._extract_git_info(repo_path))
            
            return metadata
            
        except Exception as e:
            self.logger.error(f"Error extracting repository metadata: {e}")
            return {
                'repo_url': repo_url,
                'source_type': 'github_repository',
                'error': str(e)
            }
    
    def _parse_repo_info(self, repo_url: str) -> Tuple[str, str]:
        """Parse owner and repository name from GitHub URL."""
        parsed = urlparse(repo_url)
        path_parts = parsed.path.strip("/").split("/")
        
        if len(path_parts) < 2:
            raise ValueError(f"Invalid GitHub URL format: {repo_url}")
        
        owner = path_parts[0]
        repo_name = path_parts[1]
        
        # Remove .git suffix if present
        if repo_name.endswith('.git'):
            repo_name = repo_name[:-4]
        
        return owner, repo_name
    
    def _extract_package_info(self, repo_path: str) -> Dict[str, Any]:
        """Extract information from package files (package.json, pyproject.toml, etc.)."""
        metadata = {}
        
        # Check for package.json (Node.js)
        package_json_path = os.path.join(repo_path, 'package.json')
        if os.path.exists(package_json_path):
            try:
                import json
                with open(package_json_path, 'r', encoding='utf-8') as f:
                    package_data = json.load(f)
                
                metadata.update({
                    'language': 'javascript',
                    'package_name': package_data.get('name'),
                    'description': package_data.get('description'),
                    'version': package_data.get('version'),
                    'license': package_data.get('license')
                })
            except Exception as e:
                self.logger.warning(f"Error parsing package.json: {e}")
        
        # Check for pyproject.toml (Python)
        pyproject_path = os.path.join(repo_path, 'pyproject.toml')
        if os.path.exists(pyproject_path):
            try:
                # Simple TOML parsing for basic info
                with open(pyproject_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                metadata['language'] = 'python'
                
                # Extract name and description using regex (simple approach)
                name_match = re.search(r'name\s*=\s*["\']([^"\']+)["\']', content)
                if name_match:
                    metadata['package_name'] = name_match.group(1)
                
                desc_match = re.search(r'description\s*=\s*["\']([^"\']+)["\']', content)
                if desc_match:
                    metadata['description'] = desc_match.group(1)
                    
            except Exception as e:
                self.logger.warning(f"Error parsing pyproject.toml: {e}")
        
        # Check for Cargo.toml (Rust)
        cargo_path = os.path.join(repo_path, 'Cargo.toml')
        if os.path.exists(cargo_path):
            try:
                with open(cargo_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                metadata['language'] = 'rust'
                
                # Extract basic info using regex
                name_match = re.search(r'name\s*=\s*["\']([^"\']+)["\']', content)
                if name_match:
                    metadata['package_name'] = name_match.group(1)
                    
            except Exception as e:
                self.logger.warning(f"Error parsing Cargo.toml: {e}")
        
        return metadata
    
    def _extract_readme_info(self, repo_path: str) -> Dict[str, Any]:
        """Extract information from README files."""
        readme_patterns = ['README.md', 'README.txt', 'README.rst', 'readme.md']
        
        for pattern in readme_patterns:
            readme_path = os.path.join(repo_path, pattern)
            if os.path.exists(readme_path):
                try:
                    with open(readme_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    # Extract title from first heading
                    title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
                    if title_match:
                        return {'readme_title': title_match.group(1).strip()}
                        
                except Exception as e:
                    self.logger.warning(f"Error parsing README: {e}")
                break
        
        return {}
    
    def _extract_git_info(self, repo_path: str) -> Dict[str, Any]:
        """Extract Git repository information."""
        git_info = {}
        
        try:
            # Get latest commit info
            result = subprocess.run(
                ["git", "log", "-1", "--format=%H|%s|%ai"],
                cwd=repo_path,
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                parts = result.stdout.strip().split('|')
                if len(parts) >= 3:
                    git_info.update({
                        'latest_commit_hash': parts[0],
                        'latest_commit_message': parts[1],
                        'latest_commit_date': parts[2]
                    })
        except Exception as e:
            self.logger.warning(f"Error extracting git info: {e}")
        
        return git_info


================================================
FILE: src/utils/validation.py
================================================
"""
URL validation utilities for the Crawl4AI MCP server.
"""
import re
from urllib.parse import urlparse
from typing import Tuple


def validate_github_url(url: str) -> Tuple[bool, str]:
    """
    Validate a GitHub repository URL and return validation result.
    
    Args:
        url: GitHub repository URL to validate
        
    Returns:
        Tuple of (is_valid, error_message)
        - is_valid: True if URL is valid, False otherwise
        - error_message: Error description if invalid, empty string if valid
    """
    if not url or not isinstance(url, str):
        return False, "URL must be a non-empty string"
    
    url = url.strip()
    if not url:
        return False, "URL cannot be empty"
    
    try:
        # Parse the URL
        parsed = urlparse(url)
        
        # Check scheme
        if parsed.scheme not in ['http', 'https']:
            return False, "URL must use http or https scheme"
        
        # Check if it's GitHub
        if parsed.netloc not in ['github.com', 'www.github.com']:
            return False, "URL must be from github.com"
        
        # Check path format
        path = parsed.path.strip('/')
        if not path:
            return False, "URL must include repository path"
        
        # Split path into components
        path_parts = path.split('/')
        if len(path_parts) < 2:
            return False, "URL must include both owner and repository name"
        
        owner = path_parts[0]
        repo = path_parts[1]
        
        # Remove .git suffix if present for validation
        if repo.endswith('.git'):
            repo = repo[:-4]
        
        # Validate owner and repository name
        if not owner or not repo:
            return False, "Owner and repository name cannot be empty"
        
        # Check for valid GitHub username/organization format
        # GitHub usernames can contain alphanumeric characters and hyphens
        # but cannot start or end with hyphens
        github_name_pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9-]*[a-zA-Z0-9])?$'
        
        if not re.match(github_name_pattern, owner):
            return False, "Invalid GitHub owner/organization name format"
        
        if not re.match(github_name_pattern, repo):
            return False, "Invalid GitHub repository name format"
        
        # Check for common invalid patterns
        if '..' in path or '//' in path:
            return False, "URL contains invalid path patterns"
        
        # Additional path validation - should not have more than 2 main components
        # (owner/repo), but can have additional paths for specific files/branches
        if len(path_parts) > 2:
            # Allow common GitHub URL patterns like /owner/repo/tree/branch
            # or /owner/repo/blob/branch/file.md
            valid_subpaths = ['tree', 'blob', 'releases', 'issues', 'pull', 'wiki']
            if path_parts[2] not in valid_subpaths:
                return False, "URL contains unsupported path format"
        
        return True, ""
        
    except Exception as e:
        return False, f"Invalid URL format: {str(e)}"


def normalize_github_url(url: str) -> str:
    """
    Normalize a GitHub URL to a standard format for cloning.
    
    Args:
        url: GitHub repository URL
        
    Returns:
        Normalized GitHub URL suitable for git clone
        
    Raises:
        ValueError: If URL is invalid
    """
    is_valid, error = validate_github_url(url)
    if not is_valid:
        raise ValueError(f"Invalid GitHub URL: {error}")
    
    # Parse the URL
    parsed = urlparse(url)
    path = parsed.path.strip('/')
    path_parts = path.split('/')
    
    # Extract owner and repo
    owner = path_parts[0]
    repo = path_parts[1]
    
    # Remove .git suffix for consistency
    if repo.endswith('.git'):
        repo = repo[:-4]
    
    # Return normalized HTTPS URL with .git suffix for cloning
    return f"https://github.com/{owner}/{repo}.git"


================================================
FILE: tests/__init__.py
================================================
# Test package for Crawl4AI MCP RAG with Qdrant


================================================
FILE: tests/conftest.py
================================================
"""
Pytest configuration and fixtures for Crawl4AI MCP RAG tests.
"""
import pytest
import os
import sys
from pathlib import Path
from unittest.mock import Mock

# Add src to path for all tests
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))


@pytest.fixture(autouse=True)
def setup_test_environment():
    """Setup test environment variables."""
    test_env = {
        # Modern configuration
        "CHAT_MODEL": "gpt-3.5-turbo",
        "CHAT_API_KEY": "test-chat-api-key",
        "CHAT_API_BASE": "https://api.openai.com/v1",
        "EMBEDDINGS_MODEL": "text-embedding-3-small",
        "EMBEDDINGS_API_KEY": "test-embeddings-api-key", 
        "EMBEDDINGS_API_BASE": "https://api.openai.com/v1",
        "EMBEDDINGS_DIMENSIONS": "1536",  # Default test dimensions
        
        # Other configuration
        "QDRANT_HOST": "localhost",
        "QDRANT_PORT": "6333",
        "NEO4J_URI": "bolt://localhost:7687",
        "NEO4J_USER": "neo4j",
        "NEO4J_PASSWORD": "test-password",
        "USE_CONTEXTUAL_EMBEDDINGS": "false"
    }
    
    # Set test environment variables
    original_env = {}
    for key, value in test_env.items():
        original_env[key] = os.environ.get(key)
        os.environ[key] = value
    
    yield
    
    # Restore original environment
    for key, value in original_env.items():
        if value is None:
            os.environ.pop(key, None)
        else:
            os.environ[key] = value


@pytest.fixture
def mock_qdrant_client():
    """Provide a mock Qdrant client for testing."""
    mock_client = Mock()
    
    # Setup default behaviors
    mock_client.search_documents.return_value = []
    mock_client.search_code_examples.return_value = []
    mock_client.keyword_search_documents.return_value = []
    mock_client.keyword_search_code_examples.return_value = []
    mock_client.get_available_sources.return_value = []
    mock_client.health_check.return_value = {"status": "healthy"}
    mock_client.update_source_info.return_value = None
    mock_client.upsert_points.return_value = None
    mock_client.add_documents_to_qdrant.return_value = []
    mock_client.add_code_examples_to_qdrant.return_value = []
    
    return mock_client


@pytest.fixture
def sample_documents():
    """Provide sample document data for testing."""
    return [
        {
            "id": "doc1",
            "similarity": 0.95,
            "content": "Python is a high-level programming language.",
            "url": "https://python.org/docs/tutorial",
            "chunk_number": 1,
            "source_id": "python.org",
            "metadata": {"category": "tutorial"}
        },
        {
            "id": "doc2", 
            "similarity": 0.87,
            "content": "JavaScript is used for web development.",
            "url": "https://developer.mozilla.org/js",
            "chunk_number": 1,
            "source_id": "developer.mozilla.org",
            "metadata": {"category": "reference"}
        }
    ]


@pytest.fixture
def sample_code_examples():
    """Provide sample code example data for testing."""
    return [
        {
            "id": "code1",
            "similarity": 0.92,
            "content": "def hello_world():\n    print('Hello, World!')",
            "summary": "Basic Python function that prints Hello World",
            "url": "https://python.org/examples/hello",
            "chunk_number": 1,
            "source_id": "python.org",
            "metadata": {"language": "python", "difficulty": "beginner"}
        },
        {
            "id": "code2",
            "similarity": 0.84,
            "content": "function greet(name) {\n    console.log(`Hello, ${name}!`);\n}",
            "summary": "JavaScript function for greeting with a name parameter",
            "url": "https://developer.mozilla.org/js/examples",
            "chunk_number": 1,
            "source_id": "developer.mozilla.org",
            "metadata": {"language": "javascript", "difficulty": "beginner"}
        }
    ]


@pytest.fixture
def sample_sources():
    """Provide sample source data for testing."""
    return [
        {
            "source_id": "python.org",
            "summary": "Official Python documentation and tutorials",
            "total_word_count": 125000,
            "updated_at": "2024-01-15T10:30:00Z"
        },
        {
            "source_id": "developer.mozilla.org",
            "summary": "Mozilla Developer Network web development resources",
            "total_word_count": 89000,
            "updated_at": "2024-01-10T14:20:00Z"
        }
    ]


@pytest.fixture
def mock_openai_response():
    """Provide mock OpenAI API responses."""
    def create_mock_response(embeddings_data=None, chat_content="Test response"):
        if embeddings_data is None:
            embeddings_data = [[0.1] * 1536, [0.2] * 1536]
        
        mock_response = Mock()
        mock_response.data = [Mock(embedding=emb) for emb in embeddings_data]
        
        # For chat completions
        mock_chat_response = Mock()
        mock_chat_response.choices = [Mock(message=Mock(content=chat_content))]
        
        return mock_response, mock_chat_response
    
    return create_mock_response


@pytest.fixture
def mock_crawl4ai_result():
    """Provide mock Crawl4AI results."""
    mock_result = Mock()
    mock_result.success = True
    mock_result.markdown = """
    # Test Page
    
    This is a test page with some content.
    
    ```python
    def example_function():
        return "Hello, World!"
    ```
    
    More content here for testing.
    """
    mock_result.extracted_content = "This is a test page with some content. More content here for testing."
    mock_result.metadata = {"title": "Test Page", "description": "A test page"}
    
    return mock_result


================================================
FILE: tests/integration_test.py
================================================
#!/usr/bin/env python3
"""
Integration test script for GPU acceleration functionality.
Tests actual CrossEncoder initialization and device selection.
"""

import os
import sys
from pathlib import Path

# Add src to path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

def test_gpu_auto_detection():
    """Test GPU auto-detection functionality."""
    print("=== Testing GPU Auto-Detection ===")
    
    # Set environment for auto detection
    os.environ["USE_GPU_ACCELERATION"] = "auto"
    os.environ["USE_RERANKING"] = "true"
    
    try:
        from device_manager import get_optimal_device, get_device_info
        
        # Test device detection
        device = get_optimal_device(preference="auto")
        print(f"[OK] Auto-detected device: {device}")
        
        # Get device info
        device_info = get_device_info()
        print(f"[OK] PyTorch available: {device_info['torch_available']}")
        print(f"[OK] CUDA available: {device_info['cuda_available']}")
        print(f"[OK] MPS available: {device_info['mps_available']}")
        print(f"[OK] Device count: {device_info['device_count']}")
        
        return True
        
    except Exception as e:
        print(f"[FAIL] GPU auto-detection failed: {e}")
        return False


def test_cpu_forced():
    """Test CPU-forced functionality."""
    print("\n=== Testing CPU Forced ===")
    
    # Set environment for CPU only
    os.environ["USE_GPU_ACCELERATION"] = "cpu"
    
    try:
        from device_manager import get_optimal_device
        
        # Test CPU device selection
        device = get_optimal_device(preference="cpu")
        print(f"[OK] CPU device: {device}")
        
        return True
        
    except Exception as e:
        print(f"[FAIL] CPU forced test failed: {e}")
        return False


def test_crossencoder_initialization():
    """Test actual CrossEncoder initialization."""
    print("\n=== Testing CrossEncoder Initialization ===")
    
    try:
        from device_manager import get_optimal_device, get_model_kwargs_for_device
        
        # Test device-aware initialization logic (without actual CrossEncoder)
        device = get_optimal_device(preference="auto")
        model_kwargs = get_model_kwargs_for_device(device, "float32")
        
        print(f"[OK] Device for CrossEncoder: {device}")
        print(f"[OK] Model kwargs: {model_kwargs}")
        
        # Test precision configuration
        if "cuda" in str(device):
            fp16_kwargs = get_model_kwargs_for_device(device, "float16")
            print(f"[OK] Float16 kwargs: {fp16_kwargs}")
        
        return True
        
    except Exception as e:
        print(f"[FAIL] CrossEncoder initialization test failed: {e}")
        return False


def test_memory_cleanup():
    """Test GPU memory cleanup functionality."""
    print("\n=== Testing Memory Cleanup ===")
    
    try:
        from device_manager import cleanup_gpu_memory
        from utils import cleanup_compute_memory, health_check_gpu_acceleration
        
        # Test memory cleanup functions
        cleanup_gpu_memory()
        print("[OK] GPU memory cleanup executed")
        
        cleanup_compute_memory()
        print("[OK] Compute memory cleanup executed")
        
        # Test health check
        health_status = health_check_gpu_acceleration()
        print("[OK] Health check completed")
        print(f"  - GPU available: {health_status['gpu_available']}")
        print(f"  - Device name: {health_status['device_name']}")
        print(f"  - Test passed: {health_status['test_passed']}")
        if health_status['error_message']:
            print(f"  - Error: {health_status['error_message']}")
        
        return True
        
    except Exception as e:
        print(f"[FAIL] Memory cleanup test failed: {e}")
        return False


def test_rerank_integration():
    """Test reranking function integration."""
    print("\n=== Testing Reranking Integration ===")
    
    try:
        from crawl4ai_mcp import rerank_results
        from unittest.mock import Mock
        
        # Create mock model
        mock_model = Mock()
        mock_model.predict.return_value = [0.9, 0.7, 0.8]
        
        # Test data
        results = [
            {"content": "First document", "id": "doc1"},
            {"content": "Second document", "id": "doc2"},
            {"content": "Third document", "id": "doc3"}
        ]
        
        # Test reranking
        reranked = rerank_results(mock_model, "test query", results)
        
        print("[OK] Reranking completed successfully")
        print(f"[OK] Results count: {len(reranked)}")
        print(f"[OK] Top result score: {reranked[0]['rerank_score']}")
        
        # Verify GPU memory cleanup was called
        print("[OK] GPU memory cleanup integrated in reranking")
        
        return True
        
    except Exception as e:
        print(f"[FAIL] Reranking integration test failed: {e}")
        return False


def main():
    """Run all integration tests."""
    print("GPU Acceleration Integration Tests")
    print("=" * 50)
    
    tests = [
        test_gpu_auto_detection,
        test_cpu_forced,
        test_crossencoder_initialization,
        test_memory_cleanup,
        test_rerank_integration
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"[FAIL] Test {test.__name__} crashed: {e}")
    
    print("\n" + "=" * 50)
    print(f"Integration Test Results: {passed}/{total} passed")
    
    if passed == total:
        print("All integration tests PASSED!")
        return True
    else:
        print("Some integration tests FAILED!")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


================================================
FILE: tests/performance_benchmark.py
================================================
#!/usr/bin/env python3
"""
Performance benchmark script for GPU vs CPU CrossEncoder acceleration.
Tests the 5-10x speedup requirement specified in the PRP.

NOTE: This script requires a CUDA-enabled system to measure GPU performance.
On CPU-only systems, it will only test CPU performance baseline.
"""

import os
import sys
import time
from pathlib import Path

# Add src to path
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

def benchmark_crossencoder_performance():
    """Benchmark CrossEncoder GPU vs CPU performance as specified in PRP."""
    print("CrossEncoder Performance Benchmark")
    print("=" * 50)
    
    try:
        # Import required modules
        from device_manager import get_optimal_device, get_device_info
        from sentence_transformers import CrossEncoder
        
        # Get device information
        device_info = get_device_info()
        print(f"PyTorch available: {device_info['torch_available']}")
        print(f"CUDA available: {device_info['cuda_available']}")
        print(f"Device count: {device_info['device_count']}")
        
        # Test query-document pairs (as specified in PRP)
        query_pairs = [('test query', f'test passage {i}') for i in range(100)]
        print(f"Test data: {len(query_pairs)} query-document pairs")
        
        # Initialize models for comparison
        models_to_test = []
        
        # CPU Model (always available)
        try:
            cpu_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cpu')
            models_to_test.append(('CPU', cpu_model))
            print("CPU model initialized successfully")
        except Exception as e:
            print(f"Failed to initialize CPU model: {e}")
            return False
        
        # GPU Model (if available)
        if device_info['cuda_available']:
            try:
                gpu_device = get_optimal_device('cuda')
                gpu_model = CrossEncoder(
                    'cross-encoder/ms-marco-MiniLM-L-6-v2', 
                    device=str(gpu_device)
                )
                models_to_test.append(('GPU', gpu_model))
                print(f"GPU model initialized successfully on {gpu_device}")
            except Exception as e:
                print(f"Failed to initialize GPU model: {e}")
        else:
            print("No GPU available - CPU-only benchmark")
        
        # Benchmark each model
        results = {}
        
        for model_name, model in models_to_test:
            print(f"\nTesting {model_name} performance...")
            
            # Warmup run
            try:
                _ = model.predict([query_pairs[0]])
                print(f"  Warmup completed for {model_name}")
            except Exception as e:
                print(f"  Warmup failed for {model_name}: {e}")
                continue
            
            # Actual benchmark
            try:
                start_time = time.time()
                scores = model.predict(query_pairs)
                end_time = time.time()
                
                elapsed_time = end_time - start_time
                results[model_name] = {
                    'time': elapsed_time,
                    'scores_count': len(scores),
                    'throughput': len(scores) / elapsed_time
                }
                
                print(f"  {model_name} Results:")
                print(f"    Time: {elapsed_time:.2f}s")
                print(f"    Throughput: {results[model_name]['throughput']:.2f} predictions/sec")
                
            except Exception as e:
                print(f"  Benchmark failed for {model_name}: {e}")
                continue
        
        # Performance comparison
        print(f"\n{'='*50}")
        print("Performance Comparison")
        print(f"{'='*50}")
        
        if len(results) >= 2 and 'GPU' in results and 'CPU' in results:
            gpu_time = results['GPU']['time']
            cpu_time = results['CPU']['time']
            speedup = cpu_time / gpu_time
            
            print(f"GPU time: {gpu_time:.2f}s")
            print(f"CPU time: {cpu_time:.2f}s")
            print(f"Speedup: {speedup:.2f}x")
            
            # Check PRP requirement (5-10x speedup)
            if speedup >= 5.0:
                print(f"[OK] GPU speedup meets PRP requirement (>= 5x): {speedup:.2f}x")
                return True
            else:
                print(f"[WARN] GPU speedup below PRP requirement (< 5x): {speedup:.2f}x")
                return False
                
        elif 'CPU' in results:
            print(f"CPU baseline: {results['CPU']['time']:.2f}s")
            print("[INFO] No GPU available for comparison")
            print("[INFO] On GPU systems, expect 5-10x speedup as per PRP")
            return True
        else:
            print("[FAIL] No benchmark results available")
            return False
            
    except ImportError as e:
        print(f"[FAIL] Missing dependency: {e}")
        print("Install sentence-transformers to run benchmarks")
        return False
    except Exception as e:
        print(f"[FAIL] Benchmark failed: {e}")
        return False


def benchmark_memory_usage():
    """Test GPU memory management during benchmarking."""
    print(f"\n{'='*50}")
    print("Memory Usage Analysis")
    print(f"{'='*50}")
    
    try:
        from utils import monitor_gpu_memory, cleanup_compute_memory
        
        # Monitor memory before
        print("Memory status before operations:")
        memory_before = monitor_gpu_memory()
        if memory_before:
            print(f"  GPU memory allocated: {memory_before['allocated']:.2f} GB")
            print(f"  GPU memory reserved: {memory_before['reserved']:.2f} GB")
        else:
            print("  GPU memory monitoring not available")
        
        # Test memory cleanup
        cleanup_compute_memory()
        print("Memory cleanup executed")
        
        # Monitor memory after
        print("Memory status after cleanup:")
        memory_after = monitor_gpu_memory()
        if memory_after:
            print(f"  GPU memory allocated: {memory_after['allocated']:.2f} GB")
            print(f"  GPU memory reserved: {memory_after['reserved']:.2f} GB")
        else:
            print("  GPU memory monitoring not available")
        
        return True
        
    except Exception as e:
        print(f"Memory analysis failed: {e}")
        return False


def main():
    """Run performance benchmarks."""
    print("GPU Acceleration Performance Validation")
    print("As specified in CrossEncoder GPU Acceleration PRP")
    print("="*60)
    
    # Set environment for GPU testing
    os.environ["USE_RERANKING"] = "true"
    os.environ["USE_GPU_ACCELERATION"] = "auto"
    
    success = True
    
    # Run benchmarks
    try:
        benchmark_success = benchmark_crossencoder_performance()
        memory_success = benchmark_memory_usage()
        
        success = benchmark_success and memory_success
        
    except Exception as e:
        print(f"Benchmark suite failed: {e}")
        success = False
    
    # Final results
    print(f"\n{'='*60}")
    if success:
        print("Performance validation completed successfully")
        print("GPU acceleration implementation ready for production")
    else:
        print("Performance validation encountered issues")
        print("Review GPU setup and dependencies")
    
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


================================================
FILE: tests/test_backward_compatibility.py
================================================
"""
Simple backward compatibility test for smart_crawl_github.

Tests that the default behavior (markdown-only) still works correctly.
"""
import pytest
import tempfile
import os
from pathlib import Path
import sys

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from utils.github_processor import MultiFileDiscovery, MarkdownDiscovery


class TestBackwardCompatibility:
    """Test backward compatibility of multi-file changes."""
    
    def test_multifile_discovery_defaults_to_markdown(self):
        """Test that MultiFileDiscovery works with default markdown behavior."""
        # Create temporary test repository
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a simple markdown file
            readme_content = """# Test Repository

This is a test markdown file for backward compatibility testing.

## Features

- Feature 1
- Feature 2

## Installation

```bash
pip install test-package
```
"""
            with open(os.path.join(temp_dir, "README.md"), "w") as f:
                f.write(readme_content)
            
            # Create non-markdown files that should be ignored by default
            with open(os.path.join(temp_dir, "script.py"), "w") as f:
                f.write("def test(): pass")
            
            with open(os.path.join(temp_dir, "config.json"), "w") as f:
                f.write('{"name": "test"}')
            
            # Test with MultiFileDiscovery using default markdown-only
            multi_discovery = MultiFileDiscovery()
            result = multi_discovery.discover_files(temp_dir, file_types=['.md'])
            
            # Should find only markdown files
            assert len(result) == 1
            assert result[0]['filename'] == 'README.md'
            assert result[0]['file_type'] == '.md'
            assert result[0]['is_readme'] is True
    
    def test_multifile_discovery_vs_original_markdown_discovery(self):
        """Test that MultiFileDiscovery produces similar results to MarkdownDiscovery for markdown files."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create multiple markdown files with sufficient content to meet size requirements
            files_content = {
                "README.md": "# Main README\n\nThis is the main readme file with enough content to meet the minimum size requirements for the original MarkdownDiscovery class. It includes multiple paragraphs and sufficient detail to pass the 100-byte minimum threshold.",
                "docs/guide.md": "# User Guide\n\nThis is a comprehensive user guide that provides detailed instructions and information for users. It contains multiple sections and enough content to meet the size requirements.",
                "docs/api.md": "# API Documentation\n\nThis is the comprehensive API reference documentation that includes detailed information about all available endpoints, parameters, and responses. It provides examples and usage instructions."
            }
            
            # Create directory structure
            os.makedirs(os.path.join(temp_dir, "docs"))
            
            for file_path, content in files_content.items():
                full_path = os.path.join(temp_dir, file_path)
                with open(full_path, "w") as f:
                    f.write(content)
            
            # Test original MarkdownDiscovery
            original_discovery = MarkdownDiscovery()
            original_result = original_discovery.discover_markdown_files(temp_dir)
            
            # Test MultiFileDiscovery with markdown only
            multi_discovery = MultiFileDiscovery()
            multi_result = multi_discovery.discover_files(temp_dir, file_types=['.md'])
            
            # Should find the same number of files
            assert len(original_result) == len(multi_result)
            
            # Should find the same files (by filename)
            original_filenames = {r['filename'] for r in original_result}
            multi_filenames = {r['filename'] for r in multi_result}
            assert original_filenames == multi_filenames
            
            # Both should prioritize README.md first
            assert original_result[0]['is_readme'] is True
            assert multi_result[0]['is_readme'] is True
            assert original_result[0]['filename'] == multi_result[0]['filename']
    
    def test_supported_extensions_include_all_markdown_variants(self):
        """Test that all markdown file extensions are supported."""
        discovery = MultiFileDiscovery()
        
        markdown_extensions = ['.md', '.markdown', '.mdown', '.mkd']
        
        for ext in markdown_extensions:
            assert ext in discovery.SUPPORTED_EXTENSIONS
            
            # Test file type detection
            assert discovery._is_supported_file(f"test{ext}", [ext]) is True
            assert discovery._is_supported_file(f"README{ext}", [ext]) is True
    
    def test_file_size_limits_maintained(self):
        """Test that file size limits are properly maintained."""
        discovery = MultiFileDiscovery()
        
        # Check that size limits exist for all supported file types
        expected_limits = {
            '.md': 1_000_000,      # 1MB
            '.py': 1_000_000,      # 1MB
            '.ts': 1_000_000,      # 1MB
            '.tsx': 1_000_000,     # 1MB
            '.json': 100_000,      # 100KB
            '.yaml': 100_000,      # 100KB
            '.yml': 100_000,       # 100KB
            '.toml': 100_000       # 100KB
        }
        
        for ext, expected_limit in expected_limits.items():
            assert ext in discovery.FILE_SIZE_LIMITS
            assert discovery.FILE_SIZE_LIMITS[ext] == expected_limit
    
    def test_excluded_directories_maintained(self):
        """Test that directory exclusion patterns are maintained."""
        discovery = MultiFileDiscovery()
        
        # Should inherit exclusion patterns from parent class
        common_excluded = {'.git', 'node_modules', '__pycache__', '.venv', 'build', 'dist'}
        
        for excluded_dir in common_excluded:
            assert discovery._should_exclude_dir(excluded_dir) is True
        
        # Should not exclude common directories
        common_included = {'src', 'docs', 'lib', 'tests', 'examples'}
        
        for included_dir in common_included:
            assert discovery._should_exclude_dir(included_dir) is False


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
FILE: tests/test_deepinfra_config.py
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x9d in position 3925: character maps to <undefined>


================================================
FILE: tests/test_deepinfra_integration.py
================================================
"""
Integration tests for DeepInfra embedding provider.

Tests DeepInfra API configuration, embedding creation, error handling,
and integration with the existing flexible API system.
"""
import pytest
import os
import sys
from pathlib import Path
from unittest.mock import Mock, patch

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from utils import (
    get_embeddings_client, create_embedding, create_embeddings_batch
)
from embedding_config import get_embedding_dimensions, validate_embeddings_config
from qdrant_wrapper import QdrantClientWrapper, get_collections_config


class TestDeepInfraConfiguration:
    """Test DeepInfra-specific configuration."""
    
    def setup_method(self):
        """Clean environment setup for each test."""
        self.original_env = {
            key: os.environ.get(key) for key in [
                'EMBEDDINGS_MODEL', 'EMBEDDINGS_API_KEY', 'EMBEDDINGS_API_BASE',
                'EMBEDDINGS_DIMENSIONS'
            ]
        }
        
        # Clear environment for clean testing
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]

    def teardown_method(self):
        """Restore environment after each test."""
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]
        
        for key, value in self.original_env.items():
            if value is not None:
                os.environ[key] = value

    def test_qwen3_embeddings_configuration(self):
        """Test Qwen3-Embedding-0.6B configuration."""
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        os.environ['EMBEDDINGS_API_KEY'] = 'deepinfra-key'
        os.environ['EMBEDDINGS_API_BASE'] = 'https://api.deepinfra.com/v1/openai'
        
        client = get_embeddings_client()
        assert client.api_key == 'deepinfra-key'
        assert str(client.base_url).rstrip('/') == 'https://api.deepinfra.com/v1/openai'

    def test_deepinfra_dimension_detection(self):
        """Test automatic dimension detection for DeepInfra models."""
        # Test Qwen3 model
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        os.environ.pop('EMBEDDINGS_DIMENSIONS', None)
        assert get_embedding_dimensions() == 1024
        
        # Test BGE model
        os.environ['EMBEDDINGS_MODEL'] = 'BAAI/bge-large-en-v1.5'
        assert get_embedding_dimensions() == 1024
        
        # Test smaller model
        os.environ['EMBEDDINGS_MODEL'] = 'BAAI/bge-small-en-v1.5'
        assert get_embedding_dimensions() == 384

    def test_explicit_dimensions_override(self):
        """Test explicit EMBEDDINGS_DIMENSIONS configuration."""
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        os.environ['EMBEDDINGS_DIMENSIONS'] = '512'  # Custom dimension
        assert get_embedding_dimensions() == 512

    def test_dimension_validation(self):
        """Test dimension validation with various inputs."""
        # Valid positive integer
        os.environ['EMBEDDINGS_DIMENSIONS'] = '1024'
        assert get_embedding_dimensions() == 1024
        
        # Invalid negative number
        os.environ['EMBEDDINGS_DIMENSIONS'] = '-1'
        with pytest.raises(ValueError, match="must be positive"):
            get_embedding_dimensions()
        
        # Invalid non-integer
        os.environ['EMBEDDINGS_DIMENSIONS'] = 'invalid'
        with pytest.raises(ValueError, match="must be a valid integer"):
            get_embedding_dimensions()

    @patch('src.utils.get_embeddings_client')
    def test_qwen3_embedding_creation(self, mock_get_client):
        """Test embedding creation with Qwen3 model."""
        # Mock DeepInfra client
        mock_client = Mock()
        mock_response = Mock()
        mock_response.data = [Mock(embedding=[0.1] * 1024)]  # Qwen3 dimension
        mock_client.embeddings.create.return_value = mock_response
        mock_get_client.return_value = mock_client
        
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        os.environ['EMBEDDINGS_API_KEY'] = 'test-key'
        
        # Test embedding creation
        embeddings = create_embeddings_batch(['test text'])
        
        # Verify model and dimensions
        assert len(embeddings) == 1
        assert len(embeddings[0]) == 1024
        call_args = mock_client.embeddings.create.call_args
        assert call_args[1]['model'] == 'Qwen/Qwen3-Embedding-0.6B'

    def test_config_validation_success(self):
        """Test successful configuration validation."""
        os.environ['EMBEDDINGS_API_KEY'] = 'valid-key'
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        
        # Should not raise exception
        assert validate_embeddings_config() is True

    def test_config_validation_missing_key(self):
        """Test validation failure with missing API key."""
        # Ensure no API key is set
        os.environ.pop('EMBEDDINGS_API_KEY', None)
        
        with pytest.raises(ValueError, match="No API key configured"):
            validate_embeddings_config()


class TestDeepInfraCollectionConfig:
    """Test collection configuration with DeepInfra dimensions."""
    
    def setup_method(self):
        """Setup for collection tests."""
        self.original_env = {
            key: os.environ.get(key) for key in [
                'EMBEDDINGS_MODEL', 'EMBEDDINGS_DIMENSIONS'
            ]
        }

    def teardown_method(self):
        """Cleanup after tests."""
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]
        
        for key, value in self.original_env.items():
            if value is not None:
                os.environ[key] = value

    def test_dynamic_collection_config_qwen3(self):
        """Test collection configuration with Qwen3 dimensions."""
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        
        config = get_collections_config()
        
        # Verify both collections have correct dimensions
        assert config['crawled_pages']['vectors_config'].size == 1024
        assert config['code_examples']['vectors_config'].size == 1024
        
        # Verify payload schemas are preserved
        assert 'url' in config['crawled_pages']['payload_schema']
        assert 'content' in config['crawled_pages']['payload_schema']
        assert 'summary' in config['code_examples']['payload_schema']

    def test_dynamic_collection_config_custom_dims(self):
        """Test collection configuration with custom dimensions."""
        os.environ['EMBEDDINGS_DIMENSIONS'] = '768'
        
        config = get_collections_config()
        
        # Verify custom dimensions are used
        assert config['crawled_pages']['vectors_config'].size == 768
        assert config['code_examples']['vectors_config'].size == 768


class TestDimensionValidationIntegration:
    """Test dimension validation and collection recreation."""
    
    def setup_method(self):
        """Setup for integration tests."""
        self.original_env = {
            key: os.environ.get(key) for key in [
                'EMBEDDINGS_MODEL', 'EMBEDDINGS_DIMENSIONS'
            ]
        }

    def teardown_method(self):
        """Cleanup after tests."""
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]
        
        for key, value in self.original_env.items():
            if value is not None:
                os.environ[key] = value

    @patch('qdrant_wrapper.QdrantClient')
    def test_dimension_mismatch_detection(self, mock_qdrant_client):
        """Test detection of dimension mismatches."""
        # Setup mock client
        mock_client = Mock()
        mock_qdrant_client.return_value = mock_client
        
        # Mock existing collection with 1536 dimensions
        mock_collection_info = Mock()
        mock_collection_info.config.params.vectors.size = 1536
        mock_client.get_collection.return_value = mock_collection_info
        mock_client.collection_exists.return_value = True
        
        # Configure for 1024 dimensions (Qwen3)
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        
        # Create wrapper and test validation
        wrapper = QdrantClientWrapper()
        from qdrant_client.models import VectorParams, Distance
        
        validation = wrapper._validate_collection_dimensions(
            'test_collection', 
            VectorParams(size=1024, distance=Distance.COSINE)
        )
        
        # Should detect mismatch
        assert validation['needs_recreation'] is True
        assert validation['current_size'] == 1536
        assert validation['expected_size'] == 1024

    @patch('qdrant_wrapper.QdrantClient')
    def test_collection_recreation_flow(self, mock_qdrant_client):
        """Test complete collection recreation flow."""
        # Setup mock client
        mock_client = Mock()
        mock_qdrant_client.return_value = mock_client
        mock_client.collection_exists.return_value = True
        
        # Configure environment
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        
        # Create wrapper
        wrapper = QdrantClientWrapper()
        
        # Test recreation
        from qdrant_client.models import VectorParams, Distance
        vectors_config = VectorParams(size=1024, distance=Distance.COSINE)
        wrapper._recreate_collection_safely('test_collection', vectors_config)
        
        # Verify deletion and creation were called
        mock_client.delete_collection.assert_called_once_with('test_collection')
        mock_client.create_collection.assert_called_once()
        
        # Verify creation call arguments
        create_call = mock_client.create_collection.call_args
        assert create_call[1]['collection_name'] == 'test_collection'
        assert create_call[1]['vectors_config'].size == 1024


class TestBackwardCompatibility:
    """Test backward compatibility with existing OpenAI configurations."""
    
    def setup_method(self):
        """Setup for compatibility tests."""
        self.original_env = {
            key: os.environ.get(key) for key in [
                'EMBEDDINGS_MODEL', 'EMBEDDINGS_DIMENSIONS'
            ]
        }

    def teardown_method(self):
        """Cleanup after tests."""
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]
        
        for key, value in self.original_env.items():
            if value is not None:
                os.environ[key] = value

    def test_openai_model_detection(self):
        """Test OpenAI model dimension detection still works."""
        os.environ['EMBEDDINGS_MODEL'] = 'text-embedding-3-small'
        os.environ.pop('EMBEDDINGS_DIMENSIONS', None)
        assert get_embedding_dimensions() == 1536
        
        os.environ['EMBEDDINGS_MODEL'] = 'text-embedding-3-large'
        assert get_embedding_dimensions() == 3072

    def test_unknown_model_fallback(self):
        """Test fallback behavior for unknown models."""
        os.environ['EMBEDDINGS_MODEL'] = 'unknown-model'
        os.environ.pop('EMBEDDINGS_DIMENSIONS', None)
        assert get_embedding_dimensions() == 1536  # Default fallback

    @patch('src.utils.get_embeddings_client')
    def test_fallback_embedding_dimensions(self, mock_get_client):
        """Test fallback embedding creation uses correct dimensions."""
        # Configure for Qwen3
        os.environ['EMBEDDINGS_MODEL'] = 'Qwen/Qwen3-Embedding-0.6B'
        
        # Mock client failure
        mock_client = Mock()
        mock_client.embeddings.create.side_effect = Exception("API error")
        mock_get_client.return_value = mock_client
        
        # Test fallback embedding
        embedding = create_embedding("test text")
        
        # Should use Qwen3 dimensions (1024) for fallback
        assert len(embedding) == 1024
        assert all(v == 0.0 for v in embedding)


================================================
FILE: tests/test_device_manager.py
================================================
"""
Tests for device management functionality.

Tests device detection, fallback mechanisms, error handling, and GPU memory management
following the patterns established in conftest.py.
"""

import pytest
import os
from unittest.mock import Mock, patch

# Import the device manager functions
import sys
from pathlib import Path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from device_manager import (
    get_optimal_device, 
    device_detection_with_fallback, 
    cleanup_gpu_memory,
    get_device_info,
    get_model_kwargs_for_device,
    get_gpu_preference,
    DeviceConfig,
    DeviceInfo
)


class TestDeviceDetection:
    """Test device detection and selection logic."""
    
    def test_cpu_device_forced(self):
        """CPU device always works when explicitly requested."""
        with patch('device_manager.TORCH_AVAILABLE', True):
            with patch('device_manager.torch') as mock_torch:
                mock_torch.device.return_value = Mock()
                mock_torch.device.return_value.__str__ = Mock(return_value="cpu")
                
                device = get_optimal_device(preference="cpu")
                assert str(device) == "cpu"
    
    def test_cpu_fallback_when_torch_unavailable(self):
        """Falls back to CPU when PyTorch is not available."""
        with patch('device_manager.TORCH_AVAILABLE', False):
            with patch('device_manager.torch', None):
                device = get_optimal_device(preference="auto")
                # Should return a mock device or handle gracefully
                assert device is not None
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_gpu_detection_when_cuda_available(self, mock_torch):
        """GPU detection when CUDA is available and working."""
        # Mock CUDA availability
        mock_torch.cuda.is_available.return_value = True
        mock_torch.cuda.get_device_name.return_value = "Test GPU"
        
        # Mock device creation and tensor operations
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="cuda:0")
        mock_torch.device.return_value = mock_device
        
        # Mock successful tensor operations
        mock_tensor = Mock()
        mock_torch.randn.return_value = mock_tensor
        mock_tensor.__matmul__ = Mock(return_value=mock_tensor)
        
        device = get_optimal_device(preference="cuda")
        
        # Verify CUDA operations were tested
        mock_torch.randn.assert_called_once()
        assert str(device) == "cuda:0"
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_fallback_to_cpu_when_gpu_fails(self, mock_torch):
        """Fallback to CPU when GPU operations fail."""
        # Mock CUDA availability but operations fail
        mock_torch.cuda.is_available.return_value = True
        mock_torch.randn.side_effect = RuntimeError("GPU operation failed")
        
        # Mock CPU device as fallback
        cpu_device = Mock()
        cpu_device.__str__ = Mock(return_value="cpu")
        
        def device_side_effect(device_str):
            if "cuda" in device_str:
                raise RuntimeError("CUDA device failed")
            return cpu_device
        
        mock_torch.device.side_effect = device_side_effect
        
        device = get_optimal_device(preference="auto")
        assert str(device) == "cpu"
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_mps_detection_when_available(self, mock_torch):
        """MPS detection on Apple Silicon when available."""
        # Mock MPS availability
        mock_torch.cuda.is_available.return_value = False
        mock_torch.backends.mps.is_available.return_value = True
        
        # Mock MPS device and operations
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="mps")
        mock_torch.device.return_value = mock_device
        
        mock_tensor = Mock()
        mock_torch.randn.return_value = mock_tensor
        mock_tensor.sum.return_value = mock_tensor
        
        device = get_optimal_device(preference="mps")
        assert str(device) == "mps"


class TestDeviceConfiguration:
    """Test device configuration and environment variable handling."""
    
    def test_device_config_creation(self):
        """DeviceConfig dataclass creation works correctly."""
        config = DeviceConfig(
            device_type="cuda",
            device_index=0,
            precision="float16",
            memory_fraction=0.8
        )
        
        assert config.device_type == "cuda"
        assert config.device_index == 0
        assert config.precision == "float16"
        assert config.memory_fraction == 0.8
    
    def test_device_info_creation(self):
        """DeviceInfo dataclass creation works correctly."""
        info = DeviceInfo(
            device="cuda:0",
            name="Test GPU",
            memory_total=8.0,
            is_available=True
        )
        
        assert info.device == "cuda:0"
        assert info.name == "Test GPU"
        assert info.memory_total == 8.0
        assert info.is_available is True
    
    def test_gpu_preference_from_env(self):
        """GPU preference correctly reads from environment variables."""
        test_cases = [
            ("true", "auto"),
            ("false", "cpu"),
            ("auto", "auto"),
            ("cuda", "cuda"),
            ("mps", "mps"),
        ]
        
        for env_value, expected in test_cases:
            with patch.dict(os.environ, {"USE_GPU_ACCELERATION": env_value}):
                preference = get_gpu_preference()
                assert preference == expected


class TestModelKwargs:
    """Test model_kwargs generation for different devices and precisions."""
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_float32_precision_no_kwargs(self, mock_torch):
        """Float32 precision returns empty model_kwargs."""
        mock_device = Mock()
        mock_device.type = "cuda"
        
        kwargs = get_model_kwargs_for_device(mock_device, "float32")
        assert kwargs == {}
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_float16_precision_on_gpu(self, mock_torch):
        """Float16 precision on GPU returns correct torch_dtype."""
        mock_device = Mock()
        mock_device.type = "cuda"
        mock_torch.float16 = "float16_value"
        
        kwargs = get_model_kwargs_for_device(mock_device, "float16")
        assert kwargs == {"torch_dtype": "float16_value"}
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_bfloat16_precision_on_gpu(self, mock_torch):
        """BFloat16 precision on GPU returns correct torch_dtype."""
        mock_device = Mock()
        mock_device.type = "cuda"
        mock_torch.bfloat16 = "bfloat16_value"
        
        kwargs = get_model_kwargs_for_device(mock_device, "bfloat16")
        assert kwargs == {"torch_dtype": "bfloat16_value"}
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_precision_on_cpu_ignored(self, mock_torch):
        """Precision settings ignored on CPU device."""
        mock_device = Mock()
        mock_device.type = "cpu"
        
        kwargs = get_model_kwargs_for_device(mock_device, "float16")
        assert kwargs == {}


class TestMemoryManagement:
    """Test GPU memory management and cleanup."""
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_cleanup_gpu_memory_when_available(self, mock_torch):
        """GPU memory cleanup called when CUDA available."""
        mock_torch.cuda.is_available.return_value = True
        
        cleanup_gpu_memory()
        
        mock_torch.cuda.empty_cache.assert_called_once()
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_cleanup_gpu_memory_when_unavailable(self, mock_torch):
        """GPU memory cleanup safe when CUDA unavailable."""
        mock_torch.cuda.is_available.return_value = False
        
        # Should not raise exception
        cleanup_gpu_memory()
        
        mock_torch.cuda.empty_cache.assert_not_called()
    
    @patch('device_manager.TORCH_AVAILABLE', False)
    def test_cleanup_gpu_memory_no_torch(self):
        """GPU memory cleanup safe when PyTorch unavailable."""
        # Should not raise exception
        cleanup_gpu_memory()


class TestDeviceInfo:
    """Test comprehensive device information gathering."""
    
    @patch('device_manager.TORCH_AVAILABLE', False)
    def test_device_info_no_torch(self):
        """Device info when PyTorch not available."""
        info = get_device_info()
        
        expected = {
            "torch_available": False,
            "cuda_available": False,
            "mps_available": False,
            "device_count": 0,
            "devices": []
        }
        
        assert info == expected
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_device_info_cuda_available(self, mock_torch):
        """Device info when CUDA is available."""
        # Mock CUDA availability, no MPS
        mock_torch.cuda.is_available.return_value = True
        mock_torch.cuda.device_count.return_value = 2
        mock_torch.cuda.current_device.return_value = 0
        
        # Mock no MPS availability
        mock_torch.backends.mps.is_available.return_value = False
        
        # Mock device properties for two GPUs
        mock_props = [
            Mock(name="GPU 1", total_memory=8 * 1024**3),
            Mock(name="GPU 2", total_memory=16 * 1024**3)
        ]
        mock_props[0].name = "GPU 1"
        mock_props[1].name = "GPU 2"
        mock_torch.cuda.get_device_properties.side_effect = mock_props
        mock_torch.cuda.memory_allocated.side_effect = [1024**3, 2 * 1024**3]
        
        info = get_device_info()
        
        assert info["torch_available"] is True
        assert info["cuda_available"] is True
        assert info["device_count"] == 2
        assert len(info["devices"]) == 2
        
        # Check first device info
        device_0 = info["devices"][0]
        assert device_0["index"] == 0
        assert device_0["name"] == "GPU 1"
        assert device_0["memory_total_gb"] == 8.0
        assert device_0["is_current"] is True
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_device_info_mps_available(self, mock_torch):
        """Device info when MPS is available."""
        # Mock MPS availability, no CUDA
        mock_torch.cuda.is_available.return_value = False
        mock_torch.backends.mps.is_available.return_value = True
        
        info = get_device_info()
        
        assert info["torch_available"] is True
        assert info["cuda_available"] is False
        assert info["mps_available"] is True
        assert len(info["devices"]) == 1
        
        mps_device = info["devices"][0]
        assert mps_device["name"] == "Apple Silicon GPU (MPS)"
        assert mps_device["type"] == "mps"


class TestDeviceDetectionWithFallback:
    """Test comprehensive device detection with fallback strategy."""
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_device_detection_with_config(self, mock_torch):
        """Device detection with custom configuration."""
        # Mock CUDA availability
        mock_torch.cuda.is_available.return_value = True
        mock_torch.cuda.get_device_name.return_value = "Test GPU"
        mock_torch.cuda.get_device_properties.return_value = Mock(total_memory=8 * 1024**3)
        
        # Mock device and tensor operations
        mock_device = Mock()
        mock_device.type = "cuda"
        mock_device.__str__ = Mock(return_value="cuda:0")
        mock_torch.device.return_value = mock_device
        
        mock_tensor = Mock()
        mock_torch.randn.return_value = mock_tensor
        mock_tensor.__matmul__ = Mock(return_value=mock_tensor)
        
        # Test with custom config
        config = DeviceConfig(
            device_type="cuda",
            device_index=0,
            precision="float16",
            memory_fraction=0.8
        )
        
        device, device_info = device_detection_with_fallback(config)
        
        assert str(device) == "cuda:0"
        assert device_info.name == "Test GPU"
        assert device_info.memory_total == 8.0
        assert device_info.is_available is True
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    @patch.dict(os.environ, {
        "USE_GPU_ACCELERATION": "auto",
        "GPU_DEVICE_INDEX": "1",
        "GPU_PRECISION": "float16",
        "GPU_MEMORY_FRACTION": "0.9"
    })
    def test_device_detection_from_env(self, mock_torch):
        """Device detection using environment variables."""
        # Mock CPU fallback
        mock_torch.cuda.is_available.return_value = False
        cpu_device = Mock()
        cpu_device.type = "cpu"
        cpu_device.__str__ = Mock(return_value="cpu")
        mock_torch.device.return_value = cpu_device
        
        device, device_info = device_detection_with_fallback()
        
        assert str(device) == "cpu"
        assert device_info.name == "CPU"
        assert device_info.is_available is True


class TestErrorHandling:
    """Test error handling and edge cases."""
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_gpu_operations_exception_handling(self, mock_torch):
        """GPU operations exceptions are handled gracefully."""
        mock_torch.cuda.is_available.return_value = True
        mock_torch.randn.side_effect = RuntimeError("Out of memory")
        
        # Mock MPS not available
        mock_torch.backends.mps.is_available.return_value = False
        
        # Mock CPU device for fallback
        cpu_device = Mock()
        cpu_device.__str__ = Mock(return_value="cpu")
        mock_torch.device.return_value = cpu_device
        
        # Should not raise exception, should return CPU
        device = get_optimal_device(preference="auto")
        assert str(device) == "cpu"
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_device_info_partial_failure(self, mock_torch):
        """Device info gathering handles partial failures."""
        # Mock CUDA available but some operations fail, no MPS
        mock_torch.cuda.is_available.return_value = True
        mock_torch.cuda.device_count.return_value = 2
        mock_torch.backends.mps.is_available.return_value = False
        
        # First device works, second fails
        working_gpu_props = Mock(name="Working GPU", total_memory=8 * 1024**3)
        working_gpu_props.name = "Working GPU"
        mock_torch.cuda.get_device_properties.side_effect = [
            working_gpu_props,
            RuntimeError("Device error")
        ]
        mock_torch.cuda.memory_allocated.side_effect = [1024**3, RuntimeError("Memory error")]
        mock_torch.cuda.current_device.return_value = 0
        
        info = get_device_info()
        
        # Should still return info for working device
        assert info["cuda_available"] is True
        assert len(info["devices"]) == 1  # Only successful device included
        assert info["devices"][0]["name"] == "Working GPU"
    
    def test_unknown_precision_warning(self):
        """Unknown precision values are handled with warning."""
        mock_device = Mock()
        mock_device.type = "cuda"
        
        with patch('device_manager.logging') as mock_logging:
            kwargs = get_model_kwargs_for_device(mock_device, "unknown_precision")
            
            # Should return empty dict and log warning
            assert kwargs == {}
            mock_logging.warning.assert_called_once()


if __name__ == "__main__":
    pytest.main([__file__])


================================================
FILE: tests/test_embedding_cache.py
================================================
"""
Unit tests for EmbeddingCache with Redis integration.

Tests the core functionality of the Redis-based embedding cache with mocked Redis operations.
"""
import pytest
import os
import sys
import pickle
from pathlib import Path
from unittest.mock import Mock, patch

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from embedding_cache import (
    EmbeddingCache, 
    CircuitBreaker, 
    CircuitState, 
    get_embedding_cache,
    validate_redis_config
)


class TestCircuitBreaker:
    """Test cases for CircuitBreaker class."""

    def test_init_default_values(self):
        """Test circuit breaker initialization with default values."""
        breaker = CircuitBreaker()
        
        assert breaker.failure_threshold == 5
        assert breaker.recovery_timeout == 60
        assert breaker.failure_count == 0
        assert breaker.last_failure_time is None
        assert breaker.state == CircuitState.CLOSED

    def test_init_custom_values(self):
        """Test circuit breaker initialization with custom values."""
        breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)
        
        assert breaker.failure_threshold == 3
        assert breaker.recovery_timeout == 30
        assert breaker.state == CircuitState.CLOSED

    def test_successful_call_closed_state(self):
        """Test successful function call in closed state."""
        breaker = CircuitBreaker()
        mock_func = Mock(return_value="success")
        
        result = breaker.call(mock_func, "arg1", key="value")
        
        assert result == "success"
        assert breaker.failure_count == 0
        assert breaker.state == CircuitState.CLOSED
        mock_func.assert_called_once_with("arg1", key="value")

    def test_failed_call_threshold_not_reached(self):
        """Test failed call that doesn't reach failure threshold."""
        breaker = CircuitBreaker(failure_threshold=3)
        mock_func = Mock(side_effect=Exception("Redis error"))
        
        result = breaker.call(mock_func)
        
        assert result is None
        assert breaker.failure_count == 1
        assert breaker.state == CircuitState.CLOSED
        assert breaker.last_failure_time is not None

    def test_failed_call_opens_circuit(self):
        """Test that circuit opens after reaching failure threshold."""
        breaker = CircuitBreaker(failure_threshold=2)
        mock_func = Mock(side_effect=Exception("Redis error"))
        
        # First failure
        breaker.call(mock_func)
        assert breaker.state == CircuitState.CLOSED
        
        # Second failure - should open circuit
        breaker.call(mock_func)
        assert breaker.state == CircuitState.OPEN
        assert breaker.failure_count == 2

    def test_call_in_open_state_without_recovery(self):
        """Test call in open state before recovery timeout."""
        breaker = CircuitBreaker(failure_threshold=1, recovery_timeout=60)
        mock_func = Mock(side_effect=Exception("Redis error"))
        
        # Trigger circuit opening
        breaker.call(mock_func)
        assert breaker.state == CircuitState.OPEN
        
        # Try again immediately - should fail fast
        mock_func.reset_mock()
        result = breaker.call(mock_func)
        
        assert result is None
        assert breaker.state == CircuitState.OPEN
        mock_func.assert_not_called()

    @patch('embedding_cache.time.time')
    def test_call_in_open_state_with_recovery(self, mock_time):
        """Test call in open state after recovery timeout."""
        breaker = CircuitBreaker(failure_threshold=1, recovery_timeout=60)
        mock_func = Mock(return_value="success")
        
        # Set initial time and trigger circuit opening
        mock_time.return_value = 1000
        breaker.call(Mock(side_effect=Exception("Redis error")))
        assert breaker.state == CircuitState.OPEN
        
        # Set time after recovery timeout
        mock_time.return_value = 1070  # 70 seconds later
        
        result = breaker.call(mock_func)
        
        assert result == "success"
        assert breaker.state == CircuitState.CLOSED
        assert breaker.failure_count == 0

    def test_half_open_to_closed_on_success(self):
        """Test transition from half-open to closed on successful call."""
        breaker = CircuitBreaker()
        breaker.state = CircuitState.HALF_OPEN
        mock_func = Mock(return_value="success")
        
        result = breaker.call(mock_func)
        
        assert result == "success"
        assert breaker.state == CircuitState.CLOSED
        assert breaker.failure_count == 0

    def test_half_open_to_open_on_failure(self):
        """Test transition from half-open to open on failed call."""
        breaker = CircuitBreaker(failure_threshold=1)
        breaker.state = CircuitState.HALF_OPEN
        breaker.failure_count = 0  # Reset for half-open state
        mock_func = Mock(side_effect=Exception("Redis error"))
        
        result = breaker.call(mock_func)
        
        assert result is None
        assert breaker.state == CircuitState.OPEN
        assert breaker.failure_count == 1


class TestEmbeddingCache:
    """Test cases for EmbeddingCache class."""

    @patch('embedding_cache.redis.Redis')
    def test_init_default_config(self, mock_redis_class):
        """Test cache initialization with default configuration."""
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_redis_class.return_value = mock_redis_instance
        
        cache = EmbeddingCache()
        
        assert cache.host == "localhost"
        assert cache.port == 6379
        assert cache.db == 0
        assert cache.default_ttl == 86400
        assert cache.redis == mock_redis_instance
        mock_redis_instance.ping.assert_called_once()

    @patch('embedding_cache.redis.Redis')
    def test_init_custom_config(self, mock_redis_class):
        """Test cache initialization with custom configuration."""
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_redis_class.return_value = mock_redis_instance
        
        cache = EmbeddingCache(host="custom-host", port=1234, db=2, password="secret")
        
        assert cache.host == "custom-host"
        assert cache.port == 1234
        assert cache.db == 2
        assert cache.password == "secret"

    @patch('embedding_cache.redis.Redis')
    def test_init_redis_connection_failure(self, mock_redis_class):
        """Test graceful handling of Redis connection failure."""
        mock_redis_instance = Mock()
        mock_redis_instance.ping.side_effect = Exception("Connection failed")
        mock_redis_class.return_value = mock_redis_instance
        
        cache = EmbeddingCache()
        
        assert cache.redis is None  # Should set to None on failure

    def test_generate_cache_key(self):
        """Test cache key generation."""
        cache = EmbeddingCache.__new__(EmbeddingCache)  # Skip __init__
        
        key1 = cache._generate_cache_key("hello world", "text-embedding-3-small")
        key2 = cache._generate_cache_key("hello world", "text-embedding-3-small")
        key3 = cache._generate_cache_key("different text", "text-embedding-3-small")
        key4 = cache._generate_cache_key("hello world", "different-model")
        
        # Same inputs should generate same key
        assert key1 == key2
        
        # Different inputs should generate different keys
        assert key1 != key3
        assert key1 != key4
        
        # Key format should be correct
        assert key1.startswith("embedding:text-embedding-3-small:")
        assert len(key1.split(":")[2]) == 16  # SHA256 hash truncated to 16 chars

    @patch('embedding_cache.redis.Redis')
    def test_get_batch_no_redis(self, mock_redis_class):
        """Test get_batch when Redis is not available."""
        mock_redis_class.return_value.ping.side_effect = Exception("No Redis")
        cache = EmbeddingCache()
        
        result = cache.get_batch(["text1", "text2"], "model")
        
        assert result == {}

    @patch('embedding_cache.redis.Redis')
    def test_get_batch_empty_texts(self, mock_redis_class):
        """Test get_batch with empty text list."""
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_redis_class.return_value = mock_redis_instance
        cache = EmbeddingCache()
        
        result = cache.get_batch([], "model")
        
        assert result == {}

    @patch('embedding_cache.redis.Redis')
    def test_get_batch_cache_hits(self, mock_redis_class):
        """Test get_batch with cache hits."""
        # Setup mock Redis
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_pipeline = Mock()
        mock_redis_instance.pipeline.return_value = mock_pipeline
        mock_redis_class.return_value = mock_redis_instance
        
        # Setup cached embeddings
        embedding1 = [0.1, 0.2, 0.3]
        embedding2 = [0.4, 0.5, 0.6]
        mock_pipeline.execute.return_value = [
            pickle.dumps(embedding1),
            pickle.dumps(embedding2)
        ]
        
        cache = EmbeddingCache()
        result = cache.get_batch(["text1", "text2"], "model")
        
        expected = {
            "text1": embedding1,
            "text2": embedding2
        }
        assert result == expected

    @patch('embedding_cache.redis.Redis')
    def test_get_batch_cache_misses(self, mock_redis_class):
        """Test get_batch with cache misses."""
        # Setup mock Redis
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_pipeline = Mock()
        mock_redis_instance.pipeline.return_value = mock_pipeline
        mock_redis_class.return_value = mock_redis_instance
        
        # Setup cache misses (None values)
        mock_pipeline.execute.return_value = [None, None]
        
        cache = EmbeddingCache()
        result = cache.get_batch(["text1", "text2"], "model")
        
        assert result == {}

    @patch('embedding_cache.redis.Redis')
    def test_get_batch_mixed_hits_misses(self, mock_redis_class):
        """Test get_batch with mixed cache hits and misses."""
        # Setup mock Redis
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_pipeline = Mock()
        mock_redis_instance.pipeline.return_value = mock_pipeline
        mock_redis_class.return_value = mock_redis_instance
        
        # Setup mixed results
        embedding1 = [0.1, 0.2, 0.3]
        mock_pipeline.execute.return_value = [
            pickle.dumps(embedding1),
            None  # Cache miss
        ]
        
        cache = EmbeddingCache()
        result = cache.get_batch(["text1", "text2"], "model")
        
        expected = {"text1": embedding1}
        assert result == expected

    @patch('embedding_cache.redis.Redis')
    def test_get_batch_deserialization_error(self, mock_redis_class):
        """Test get_batch handling deserialization errors gracefully."""
        # Setup mock Redis
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_pipeline = Mock()
        mock_redis_instance.pipeline.return_value = mock_pipeline
        mock_redis_class.return_value = mock_redis_instance
        
        # Setup corrupted data
        mock_pipeline.execute.return_value = [b"corrupted_data"]
        
        cache = EmbeddingCache()
        result = cache.get_batch(["text1"], "model")
        
        assert result == {}  # Should handle error gracefully

    @patch('embedding_cache.redis.Redis')
    def test_set_batch_no_redis(self, mock_redis_class):
        """Test set_batch when Redis is not available."""
        mock_redis_class.return_value.ping.side_effect = Exception("No Redis")
        cache = EmbeddingCache()
        
        # Should not raise exception
        cache.set_batch({"text": [0.1, 0.2]}, "model")

    @patch('embedding_cache.redis.Redis')
    def test_set_batch_empty_embeddings(self, mock_redis_class):
        """Test set_batch with empty embeddings."""
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_redis_class.return_value = mock_redis_instance
        cache = EmbeddingCache()
        
        cache.set_batch({}, "model")
        
        # Should not call pipeline if no embeddings
        mock_redis_instance.pipeline.assert_not_called()

    @patch('embedding_cache.redis.Redis')
    def test_set_batch_success(self, mock_redis_class):
        """Test successful set_batch operation."""
        # Setup mock Redis
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_pipeline = Mock()
        mock_redis_instance.pipeline.return_value = mock_pipeline
        mock_redis_class.return_value = mock_redis_instance
        
        cache = EmbeddingCache()
        embeddings = {
            "text1": [0.1, 0.2, 0.3],
            "text2": [0.4, 0.5, 0.6]
        }
        
        cache.set_batch(embeddings, "model", ttl=3600)
        
        # Verify pipeline operations
        mock_redis_instance.pipeline.assert_called_once()
        assert mock_pipeline.setex.call_count == 2
        mock_pipeline.execute.assert_called_once()

    @patch('embedding_cache.redis.Redis')
    def test_set_batch_serialization_error(self, mock_redis_class):
        """Test set_batch handling serialization errors gracefully."""
        # Setup mock Redis
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_pipeline = Mock()
        mock_redis_instance.pipeline.return_value = mock_pipeline
        mock_redis_class.return_value = mock_redis_instance
        
        cache = EmbeddingCache()
        
        # Create an object that can't be pickled
        class UnpicklableObject:
            def __reduce__(self):
                raise TypeError("Cannot pickle this object")
        
        embeddings = {"text": UnpicklableObject()}
        
        # Should not raise exception
        cache.set_batch(embeddings, "model")

    @patch('embedding_cache.redis.Redis')
    def test_health_check_no_redis(self, mock_redis_class):
        """Test health check when Redis client is not initialized."""
        mock_redis_class.return_value.ping.side_effect = Exception("No Redis")
        cache = EmbeddingCache()
        
        health = cache.health_check()
        
        assert health['status'] == 'unhealthy'
        assert 'No Redis' in health['error']  # Should contain the actual error message
        assert 'circuit_breaker_state' in health

    @patch('embedding_cache.redis.Redis')
    def test_health_check_success(self, mock_redis_class):
        """Test successful health check."""
        # Setup mock Redis
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_redis_instance.setex.return_value = True
        mock_redis_instance.get.return_value = b'test'
        mock_redis_instance.delete.return_value = 1
        mock_redis_class.return_value = mock_redis_instance
        
        cache = EmbeddingCache()
        health = cache.health_check()
        
        assert health['status'] == 'healthy'
        assert health['ping'] is True
        assert health['read_write'] is True
        assert 'latency_ms' in health
        assert health['latency_ms'] >= 0
        assert 'connection_info' in health

    @patch('embedding_cache.redis.Redis')
    def test_health_check_failure(self, mock_redis_class):
        """Test health check with Redis operation failure."""
        # Setup mock Redis
        mock_redis_instance = Mock()
        mock_redis_instance.ping.side_effect = Exception("Redis down")
        mock_redis_class.return_value = mock_redis_instance
        
        cache = EmbeddingCache()
        health = cache.health_check()
        
        assert health['status'] == 'unhealthy'
        assert 'Redis down' in health['error']

    @patch('embedding_cache.redis.Redis')
    def test_circuit_breaker_integration(self, mock_redis_class):
        """Test circuit breaker integration with Redis operations."""
        # Setup mock Redis that fails
        mock_redis_instance = Mock()
        mock_redis_instance.ping.return_value = True
        mock_pipeline = Mock()
        mock_pipeline.execute.side_effect = Exception("Redis error")
        mock_redis_instance.pipeline.return_value = mock_pipeline
        mock_redis_class.return_value = mock_redis_instance
        
        cache = EmbeddingCache()
        cache.circuit_breaker.failure_threshold = 2
        
        # First failure
        result1 = cache.get_batch(["text"], "model")
        assert result1 == {}
        assert cache.circuit_breaker.state == CircuitState.CLOSED
        
        # Second failure - should open circuit
        result2 = cache.get_batch(["text"], "model")
        assert result2 == {}
        assert cache.circuit_breaker.state == CircuitState.OPEN
        
        # Third call should fail fast without calling Redis
        mock_pipeline.execute.reset_mock()
        result3 = cache.get_batch(["text"], "model")
        assert result3 == {}
        mock_pipeline.execute.assert_not_called()


class TestGlobalFunctions:
    """Test cases for global functions."""

    @patch.dict(os.environ, {'USE_REDIS_CACHE': 'false'})
    def test_get_embedding_cache_disabled(self):
        """Test get_embedding_cache when caching is disabled."""
        result = get_embedding_cache()
        assert result is None

    @patch.dict(os.environ, {'USE_REDIS_CACHE': 'true'})
    @patch('embedding_cache.EmbeddingCache')
    def test_get_embedding_cache_enabled_success(self, mock_cache_class):
        """Test get_embedding_cache when caching is enabled and succeeds."""
        mock_cache_instance = Mock()
        mock_cache_class.return_value = mock_cache_instance
        
        # Clear global cache
        import embedding_cache
        embedding_cache._embedding_cache = None
        
        result = get_embedding_cache()
        
        assert result == mock_cache_instance
        mock_cache_class.assert_called_once()

    @patch.dict(os.environ, {'USE_REDIS_CACHE': 'true'})
    @patch('embedding_cache.EmbeddingCache')
    def test_get_embedding_cache_initialization_failure(self, mock_cache_class):
        """Test get_embedding_cache when cache initialization fails."""
        mock_cache_class.side_effect = Exception("Redis unavailable")
        
        # Clear global cache
        import embedding_cache
        embedding_cache._embedding_cache = None
        
        result = get_embedding_cache()
        
        assert result is None
        mock_cache_class.assert_called_once()

    @patch.dict(os.environ, {'REDIS_HOST': 'localhost', 'REDIS_PORT': '6379', 'REDIS_EMBEDDING_TTL': '86400'})
    def test_validate_redis_config_success(self):
        """Test successful Redis configuration validation."""
        # Should not raise exception
        validate_redis_config()

    @patch.dict(os.environ, {'REDIS_PORT': 'invalid'})
    def test_validate_redis_config_invalid_port(self):
        """Test Redis configuration validation with invalid port."""
        with pytest.raises(ValueError, match="Invalid Redis port"):
            validate_redis_config()

    @patch.dict(os.environ, {'REDIS_PORT': '70000'})
    def test_validate_redis_config_port_out_of_range(self):
        """Test Redis configuration validation with port out of range."""
        with pytest.raises(ValueError, match="Invalid Redis port"):
            validate_redis_config()

    @patch.dict(os.environ, {'REDIS_EMBEDDING_TTL': '-1'})
    def test_validate_redis_config_invalid_ttl(self):
        """Test Redis configuration validation with invalid TTL."""
        with pytest.raises(ValueError, match="Invalid Redis TTL"):
            validate_redis_config()

    @patch.dict(os.environ, {'REDIS_EMBEDDING_TTL': 'not_a_number'})
    def test_validate_redis_config_non_numeric_ttl(self):
        """Test Redis configuration validation with non-numeric TTL."""
        with pytest.raises(ValueError, match="Invalid Redis TTL"):
            validate_redis_config()


if __name__ == "__main__":
    pytest.main([__file__])


================================================
FILE: tests/test_event_loop_fix.py
================================================
"""
Tests for the Windows ConnectionResetError event loop fix.

These tests validate the event loop configuration utility that fixes
ConnectionResetError [WinError 10054] on Windows by using SelectorEventLoop
instead of the default ProactorEventLoop.
"""

import asyncio
import platform
import pytest
import sys
from unittest.mock import patch, MagicMock

# Import the module under test
try:
    from src.event_loop_fix import (
        is_windows,
        has_selector_event_loop_policy,
        is_playwright_imported,
        should_use_selector_loop,
        get_current_event_loop_policy,
        setup_event_loop,
        validate_event_loop_setup,
        print_event_loop_info,
    )
except ImportError:
    from event_loop_fix import (
        is_windows,
        has_selector_event_loop_policy,
        is_playwright_imported,
        should_use_selector_loop,
        get_current_event_loop_policy,
        setup_event_loop,
        validate_event_loop_setup,
        print_event_loop_info,
    )


class TestPlatformDetection:
    """Test platform detection functions."""

    def test_is_windows_actual_platform(self):
        """Test is_windows returns correct value for current platform."""
        expected = platform.system().lower() == "windows"
        assert is_windows() == expected

    @patch("event_loop_fix.platform.system")
    def test_is_windows_mocked_windows(self, mock_system):
        """Test is_windows returns True when platform is Windows."""
        mock_system.return_value = "Windows"
        assert is_windows() is True

    @patch("event_loop_fix.platform.system")
    def test_is_windows_mocked_linux(self, mock_system):
        """Test is_windows returns False when platform is Linux."""
        mock_system.return_value = "Linux"
        assert is_windows() is False

    @patch("event_loop_fix.platform.system")
    def test_is_windows_mocked_darwin(self, mock_system):
        """Test is_windows returns False when platform is Darwin (macOS)."""
        mock_system.return_value = "Darwin"
        assert is_windows() is False


class TestPlaywrightDetection:
    """Test Playwright detection functions."""

    def test_is_playwright_imported_no_modules(self):
        """Test is_playwright_imported returns False when no Playwright modules are imported."""
        # Ensure no playwright modules are in sys.modules
        original_modules = {}
        playwright_modules = ['playwright', 'playwright.async_api', 'playwright._impl', 'crawl4ai']
        
        for module in playwright_modules:
            if module in sys.modules:
                original_modules[module] = sys.modules[module]
                del sys.modules[module]
        
        try:
            assert is_playwright_imported() is False
        finally:
            # Restore original modules
            for module, mod_obj in original_modules.items():
                sys.modules[module] = mod_obj

    def test_is_playwright_imported_with_playwright(self):
        """Test is_playwright_imported returns True when playwright is imported."""
        import sys
        from types import ModuleType
        
        # Mock playwright module
        sys.modules['playwright'] = ModuleType('playwright')
        
        try:
            assert is_playwright_imported() is True
        finally:
            # Clean up
            if 'playwright' in sys.modules:
                del sys.modules['playwright']

    def test_is_playwright_imported_with_crawl4ai(self):
        """Test is_playwright_imported returns True when crawl4ai is imported."""
        import sys
        from types import ModuleType
        
        # Mock crawl4ai module
        sys.modules['crawl4ai'] = ModuleType('crawl4ai')
        
        try:
            assert is_playwright_imported() is True
        finally:
            # Clean up
            if 'crawl4ai' in sys.modules:
                del sys.modules['crawl4ai']


class TestEventLoopPolicyDetection:
    """Test event loop policy detection functions."""

    def test_has_selector_event_loop_policy_actual(self):
        """Test has_selector_event_loop_policy with actual platform."""
        expected = platform.system().lower() == "windows" and hasattr(
            asyncio, "WindowsSelectorEventLoopPolicy"
        )
        assert has_selector_event_loop_policy() == expected

    @patch("src.event_loop_fix.is_windows")
    def test_has_selector_event_loop_policy_non_windows(self, mock_is_windows):
        """Test has_selector_event_loop_policy returns False on non-Windows."""
        mock_is_windows.return_value = False
        assert has_selector_event_loop_policy() is False

    @patch("src.event_loop_fix.is_windows")
    @patch("src.event_loop_fix.asyncio")
    def test_has_selector_event_loop_policy_windows_with_policy(
        self, mock_asyncio, mock_is_windows
    ):
        """Test has_selector_event_loop_policy returns True on Windows with policy."""
        mock_is_windows.return_value = True
        mock_asyncio.WindowsSelectorEventLoopPolicy = MagicMock()

        # Mock hasattr to return True
        with patch("builtins.hasattr", return_value=True):
            assert has_selector_event_loop_policy() is True

    @patch("src.event_loop_fix.is_windows")
    @patch("src.event_loop_fix.asyncio")
    def test_has_selector_event_loop_policy_windows_without_policy(
        self, mock_asyncio, mock_is_windows
    ):
        """Test has_selector_event_loop_policy returns False on Windows without policy."""
        mock_is_windows.return_value = True

        # Mock hasattr to return False (policy not available)
        with patch("builtins.hasattr", return_value=False):
            assert has_selector_event_loop_policy() is False

    def test_should_use_selector_loop_no_playwright(self):
        """Test should_use_selector_loop when Playwright is not imported."""
        with patch("src.event_loop_fix.is_playwright_imported", return_value=False):
            assert should_use_selector_loop() == has_selector_event_loop_policy()

    def test_should_use_selector_loop_with_playwright(self):
        """Test should_use_selector_loop returns False when Playwright is imported."""
        with patch("src.event_loop_fix.is_playwright_imported", return_value=True):
            with patch("src.event_loop_fix.has_selector_event_loop_policy", return_value=True):
                assert should_use_selector_loop() is False


class TestEventLoopPolicyConfiguration:
    """Test event loop policy configuration functions."""

    def test_get_current_event_loop_policy_normal(self):
        """Test get_current_event_loop_policy returns policy name."""
        policy_name = get_current_event_loop_policy()
        assert isinstance(policy_name, str)
        assert len(policy_name) > 0
        assert "Policy" in policy_name or policy_name == "Unknown"

    @patch("src.event_loop_fix.asyncio.get_event_loop_policy")
    def test_get_current_event_loop_policy_exception(self, mock_get_policy):
        """Test get_current_event_loop_policy handles exceptions."""
        mock_get_policy.side_effect = Exception("Test exception")
        assert get_current_event_loop_policy() == "Unknown"

    def test_setup_event_loop_preserves_functionality(self):
        """Test setup_event_loop doesn't break asyncio functionality."""
        # Store original policy
        original_policy = asyncio.get_event_loop_policy()

        try:
            # Apply fix
            result = setup_event_loop()

            # Test that asyncio still works
            async def test_async():
                return "test_result"

            # This should work regardless of event loop policy
            loop_result = asyncio.run(test_async())
            assert loop_result == "test_result"

            # Verify return value is appropriate
            if platform.system().lower() == "windows":
                if hasattr(asyncio, "WindowsSelectorEventLoopPolicy"):
                    assert result == "WindowsSelectorEventLoopPolicy"
                else:
                    assert result is None
            else:
                assert result is None

        finally:
            # Restore original policy
            asyncio.set_event_loop_policy(original_policy)

    @patch("src.event_loop_fix.is_windows")
    @patch("src.event_loop_fix.is_playwright_imported")
    @patch("src.event_loop_fix.should_use_selector_loop")
    @patch("src.event_loop_fix.get_current_event_loop_policy")
    @patch("src.event_loop_fix.asyncio.set_event_loop_policy")
    def test_setup_event_loop_windows_success(self, mock_set_policy, mock_get_policy, mock_should_use, mock_playwright, mock_is_windows):
        """Test setup_event_loop success path on Windows."""
        mock_is_windows.return_value = True
        mock_playwright.return_value = False
        mock_should_use.return_value = True
        mock_get_policy.side_effect = ["WindowsProactorEventLoopPolicy", "WindowsSelectorEventLoopPolicy"]

        with patch(
            "src.event_loop_fix.asyncio.WindowsSelectorEventLoopPolicy"
        ) as mock_policy_class:
            result = setup_event_loop()

            mock_set_policy.assert_called_once_with(mock_policy_class())
            assert result == "WindowsSelectorEventLoopPolicy"

    @patch("src.event_loop_fix.should_use_selector_loop")
    def test_setup_event_loop_non_windows(self, mock_should_use):
        """Test setup_event_loop on non-Windows platforms."""
        mock_should_use.return_value = False

        result = setup_event_loop()
        assert result is None

    @patch("src.event_loop_fix.is_windows")
    @patch("src.event_loop_fix.is_playwright_imported")
    @patch("src.event_loop_fix.should_use_selector_loop")
    @patch("src.event_loop_fix.asyncio.set_event_loop_policy")
    def test_setup_event_loop_windows_with_playwright(
        self, mock_set_policy, mock_should_use, mock_playwright, mock_is_windows
    ):
        """Test setup_event_loop on Windows with Playwright detected."""
        mock_is_windows.return_value = True
        mock_playwright.return_value = True
        mock_should_use.return_value = False

        with patch("src.event_loop_fix.asyncio.WindowsProactorEventLoopPolicy") as mock_policy_class:
            result = setup_event_loop()

            mock_set_policy.assert_called_once_with(mock_policy_class())
            assert result == "WindowsProactorEventLoopPolicy"

    @patch("src.event_loop_fix.is_windows")
    @patch("src.event_loop_fix.is_playwright_imported")
    @patch("src.event_loop_fix.should_use_selector_loop")
    def test_setup_event_loop_windows_with_playwright_no_proactor_policy(
        self, mock_should_use, mock_playwright, mock_is_windows
    ):
        """Test setup_event_loop when Playwright detected but ProactorEventLoop unavailable."""
        mock_is_windows.return_value = True
        mock_playwright.return_value = True
        mock_should_use.return_value = False

        # Mock hasattr to return False for WindowsProactorEventLoopPolicy
        with patch("builtins.hasattr", return_value=False):
            result = setup_event_loop()
            assert result is None

    @patch("src.event_loop_fix.should_use_selector_loop")
    @patch("src.event_loop_fix.asyncio.set_event_loop_policy")
    def test_setup_event_loop_exception_handling(
        self, mock_set_policy, mock_should_use
    ):
        """Test setup_event_loop handles exceptions gracefully."""
        mock_should_use.return_value = True
        mock_set_policy.side_effect = Exception("Test exception")

        # Should not raise exception, but return None
        result = setup_event_loop()
        assert result is None


class TestValidationFunctions:
    """Test validation and information functions."""

    def test_validate_event_loop_setup_structure(self):
        """Test validate_event_loop_setup returns proper structure."""
        info = validate_event_loop_setup()

        required_keys = [
            "platform",
            "python_version",
            "current_policy",
            "is_windows",
            "has_selector_policy",
            "should_use_selector",
            "playwright_detected",
            "fix_applied",
            "recommendations",
        ]

        for key in required_keys:
            assert key in info

        assert isinstance(info["platform"], str)
        assert isinstance(info["python_version"], str)
        assert isinstance(info["current_policy"], str)
        assert isinstance(info["is_windows"], bool)
        assert isinstance(info["has_selector_policy"], bool)
        assert isinstance(info["should_use_selector"], bool)
        assert isinstance(info["playwright_detected"], bool)
        assert isinstance(info["fix_applied"], bool)
        assert isinstance(info["recommendations"], list)

    def test_validate_event_loop_setup_windows_detection(self):
        """Test validate_event_loop_setup properly detects Windows scenarios."""
        info = validate_event_loop_setup()

        if platform.system().lower() == "windows":
            assert info["is_windows"] is True

            if info["current_policy"] == "WindowsSelectorEventLoopPolicy":
                assert info["fix_applied"] is True
                assert any("OK" in rec for rec in info["recommendations"])
            else:
                assert info["fix_applied"] is False
                assert any("WARNING" in rec for rec in info["recommendations"])
        else:
            assert info["is_windows"] is False
            assert any("INFO" in rec for rec in info["recommendations"])

    def test_validate_event_loop_setup_playwright_detection(self):
        """Test validate_event_loop_setup properly detects Playwright scenarios."""
        with patch("src.event_loop_fix.is_playwright_imported", return_value=True):
            info = validate_event_loop_setup()
            
            assert info["playwright_detected"] is True
            
            if platform.system().lower() == "windows":
                if info["current_policy"] == "WindowsProactorEventLoopPolicy":
                    assert info["fix_applied"] is True
                    assert any("OK" in rec and "Playwright detected" in rec for rec in info["recommendations"])
                    assert any("WARNING" in rec and "ConnectionResetError may still occur" in rec for rec in info["recommendations"])
                else:
                    assert any("WARNING" in rec and "Playwright detected" in rec for rec in info["recommendations"])

    def test_validate_event_loop_setup_no_playwright_detection(self):
        """Test validate_event_loop_setup when no Playwright is detected."""
        with patch("src.event_loop_fix.is_playwright_imported", return_value=False):
            info = validate_event_loop_setup()
            
            assert info["playwright_detected"] is False
            
            if platform.system().lower() == "windows":
                if info["current_policy"] == "WindowsSelectorEventLoopPolicy":
                    assert info["fix_applied"] is True
                    assert any("OK" in rec and "ConnectionResetError fix is active" in rec for rec in info["recommendations"])
                else:
                    assert any("WARNING" in rec and "SelectorEventLoop not active" in rec for rec in info["recommendations"])

    def test_print_event_loop_info_no_exception(self, capsys):
        """Test print_event_loop_info runs without exception."""
        # Should not raise any exceptions
        print_event_loop_info()

        # Capture output to verify it produces some output
        captured = capsys.readouterr()
        assert len(captured.out) > 0
        assert "Event Loop Configuration Information" in captured.out


class TestIntegrationScenarios:
    """Test integration scenarios and edge cases."""

    def test_import_works_from_package(self):
        """Test that imports work when called from package context."""
        # This test validates that the import structure works correctly
        try:
            from src.event_loop_fix import setup_event_loop

            assert callable(setup_event_loop)
        except ImportError:
            # Fallback import should work
            from event_loop_fix import setup_event_loop

            assert callable(setup_event_loop)

    def test_entry_point_integration_crawl4ai_mcp(self):
        """Test that the fix integrates properly with crawl4ai_mcp entry point."""
        # Test the import pattern used in crawl4ai_mcp.py
        try:
            from src.event_loop_fix import setup_event_loop
        except ImportError:
            from event_loop_fix import setup_event_loop

        # Should be callable
        assert callable(setup_event_loop)

        # Should return appropriate value
        result = setup_event_loop()
        assert result is None or isinstance(result, str)

    def test_entry_point_integration_main(self):
        """Test that the fix integrates properly with __main__ entry point."""
        # Test the import pattern used in __main__.py
        from src.event_loop_fix import setup_event_loop

        # Should be callable
        assert callable(setup_event_loop)

        # Should return appropriate value
        result = setup_event_loop()
        assert result is None or isinstance(result, str)

    @pytest.mark.asyncio
    async def test_http_operations_still_work(self):
        """Test that HTTP operations work after applying the fix."""
        # Apply the fix
        setup_event_loop()

        # Test basic async operation (simulating HTTP operation pattern)
        async def mock_http_operation():
            # Simulate some async work like HTTP request
            await asyncio.sleep(0.01)
            return {"success": True, "data": "test"}

        result = await mock_http_operation()
        assert result["success"] is True
        assert result["data"] == "test"

    def test_multiple_setup_calls_safe(self):
        """Test that calling setup_event_loop multiple times is safe."""
        # Store original policy
        original_policy = get_current_event_loop_policy()

        try:
            # Call setup multiple times
            result1 = setup_event_loop()
            result2 = setup_event_loop()
            result3 = setup_event_loop()

            # All should return same result or None
            assert result1 == result2 == result3

            # Policy should be consistent
            final_policy = get_current_event_loop_policy()
            assert isinstance(final_policy, str)

        finally:
            # Restore original policy for other tests
            if original_policy != "Unknown":
                # Try to restore, but don't fail if we can't
                try:
                    if original_policy == "DefaultEventLoopPolicy":
                        asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())
                    elif (
                        hasattr(asyncio, "WindowsProactorEventLoopPolicy")
                        and original_policy == "WindowsProactorEventLoopPolicy"
                    ):
                        asyncio.set_event_loop_policy(
                            asyncio.WindowsProactorEventLoopPolicy()
                        )
                except Exception:
                    pass  # Best effort restore


if __name__ == "__main__":
    # Run tests when executed directly
    pytest.main([__file__, "-v"])



================================================
FILE: tests/test_fallback_api_config.py
================================================
"""
Integration tests for fallback API configuration system.

Tests the fallback API configuration with inheritance patterns,
mixed provider scenarios, and adaptive client functionality.
"""
import pytest
import os
import time
from unittest.mock import Mock, patch
from pathlib import Path
import sys

# Add src to path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))


class TestFallbackAPIConfiguration:
    """Test the fallback API configuration system with inheritance."""

    def setup_method(self):
        """Set up test environment for each test."""
        # Save original environment (including conftest values)
        self.original_env = {
            key: os.environ.get(key) for key in [
                'CHAT_MODEL', 'CHAT_API_KEY', 'CHAT_API_BASE',
                'CHAT_FALLBACK_MODEL', 'CHAT_FALLBACK_API_KEY', 'CHAT_FALLBACK_API_BASE',
                'EMBEDDINGS_MODEL', 'EMBEDDINGS_API_KEY', 'EMBEDDINGS_API_BASE',
                'EMBEDDINGS_FALLBACK_MODEL', 'EMBEDDINGS_FALLBACK_API_KEY', 'EMBEDDINGS_FALLBACK_API_BASE',
            ]
        }
        
        # Clear environment for clean testing (remove conftest interference)
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]

    def teardown_method(self):
        """Restore original environment after each test."""
        # Clear test environment
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]
        
        # Restore original values
        for key, value in self.original_env.items():
            if value is not None:
                os.environ[key] = value

    def test_chat_fallback_api_key_inheritance(self):
        """Test that chat fallback inherits primary API key when not specified."""
        # Set primary configuration only
        os.environ['CHAT_API_KEY'] = 'primary-chat-key'
        os.environ['CHAT_API_BASE'] = 'https://primary.api.com/v1'
        
        from src.utils import get_chat_fallback_client
        
        client = get_chat_fallback_client()
        assert client.api_key == 'primary-chat-key'
        assert str(client.base_url).rstrip('/') == 'https://primary.api.com/v1'

    def test_chat_fallback_api_key_override(self):
        """Test that explicit fallback API key overrides inheritance."""
        # Set both primary and fallback configuration
        os.environ['CHAT_API_KEY'] = 'primary-chat-key'
        os.environ['CHAT_API_BASE'] = 'https://primary.api.com/v1'
        os.environ['CHAT_FALLBACK_API_KEY'] = 'fallback-chat-key'
        os.environ['CHAT_FALLBACK_API_BASE'] = 'https://fallback.api.com/v1'
        
        from src.utils import get_chat_fallback_client
        
        client = get_chat_fallback_client()
        assert client.api_key == 'fallback-chat-key'
        assert str(client.base_url).rstrip('/') == 'https://fallback.api.com/v1'

    def test_embeddings_fallback_api_key_inheritance(self):
        """Test that embeddings fallback inherits primary API key when not specified."""
        # Set primary configuration only
        os.environ['EMBEDDINGS_API_KEY'] = 'primary-embeddings-key'
        os.environ['EMBEDDINGS_API_BASE'] = 'https://primary.api.com/v1'
        
        from src.utils import get_embeddings_fallback_client
        
        client = get_embeddings_fallback_client()
        assert client.api_key == 'primary-embeddings-key'
        assert str(client.base_url).rstrip('/') == 'https://primary.api.com/v1'

    def test_embeddings_fallback_api_key_override(self):
        """Test that explicit fallback API key overrides inheritance."""
        # Set both primary and fallback configuration
        os.environ['EMBEDDINGS_API_KEY'] = 'primary-embeddings-key'
        os.environ['EMBEDDINGS_API_BASE'] = 'https://primary.api.com/v1'
        os.environ['EMBEDDINGS_FALLBACK_API_KEY'] = 'fallback-embeddings-key'
        os.environ['EMBEDDINGS_FALLBACK_API_BASE'] = 'https://fallback.api.com/v1'
        
        from src.utils import get_embeddings_fallback_client
        
        client = get_embeddings_fallback_client()
        assert client.api_key == 'fallback-embeddings-key'
        assert str(client.base_url).rstrip('/') == 'https://fallback.api.com/v1'

    def test_mixed_provider_configuration(self):
        """Test using different providers for primary and fallback."""
        # Primary via OpenRouter, fallback via OpenAI (explicitly configured)
        os.environ['CHAT_API_KEY'] = 'openrouter-key'
        os.environ['CHAT_API_BASE'] = 'https://openrouter.ai/api/v1'
        os.environ['CHAT_FALLBACK_API_KEY'] = 'openai-key'
        os.environ['CHAT_FALLBACK_API_BASE'] = 'https://api.openai.com/v1'  # Explicitly set for different provider
        
        from src.utils import get_chat_client, get_chat_fallback_client
        
        primary_client = get_chat_client()
        fallback_client = get_chat_fallback_client()
        
        # Primary should use OpenRouter
        assert primary_client.api_key == 'openrouter-key'
        assert str(primary_client.base_url).rstrip('/') == 'https://openrouter.ai/api/v1'
        
        # Fallback should use OpenAI (explicitly configured)
        assert fallback_client.api_key == 'openai-key'
        assert str(fallback_client.base_url).rstrip('/') == 'https://api.openai.com/v1'

    def test_adaptive_chat_client_primary_success(self):
        """Test adaptive client uses primary when available."""
        # Set primary configuration
        os.environ['CHAT_MODEL'] = 'gpt-4'
        os.environ['CHAT_API_KEY'] = 'primary-key'
        os.environ['CHAT_API_BASE'] = 'https://primary.api.com/v1'
        
        from src.utils import get_adaptive_chat_client
        
        client, model_used, is_fallback = get_adaptive_chat_client()
        
        assert client.api_key == 'primary-key'
        assert str(client.base_url).rstrip('/') == 'https://primary.api.com/v1'
        assert model_used == 'gpt-4'
        assert is_fallback is False

    def test_adaptive_chat_client_fallback_when_no_primary(self):
        """Test adaptive client uses fallback when primary not available."""
        # Set only fallback configuration
        os.environ['CHAT_FALLBACK_MODEL'] = 'gpt-3.5-turbo'
        os.environ['CHAT_FALLBACK_API_KEY'] = 'fallback-key'
        os.environ['CHAT_FALLBACK_API_BASE'] = 'https://fallback.api.com/v1'
        
        from src.utils import get_adaptive_chat_client
        
        client, model_used, is_fallback = get_adaptive_chat_client()
        
        assert client.api_key == 'fallback-key'
        assert str(client.base_url).rstrip('/') == 'https://fallback.api.com/v1'
        assert model_used == 'gpt-3.5-turbo'
        assert is_fallback is True

    def test_adaptive_chat_client_with_inheritance(self):
        """Test adaptive client fallback with inheritance from primary."""
        # Set primary configuration that fallback will inherit
        os.environ['CHAT_API_KEY'] = 'inherited-key'
        os.environ['CHAT_API_BASE'] = 'https://inherited.api.com/v1'
        os.environ['CHAT_FALLBACK_MODEL'] = 'gpt-3.5-turbo'
        # No primary model set, so should use fallback
        
        from src.utils import get_adaptive_chat_client
        
        client, model_used, is_fallback = get_adaptive_chat_client()
        
        assert client.api_key == 'inherited-key'
        assert str(client.base_url).rstrip('/') == 'https://inherited.api.com/v1'
        assert model_used == 'gpt-3.5-turbo'
        assert is_fallback is True

    def test_adaptive_chat_client_model_preference(self):
        """Test adaptive client with specific model preference."""
        # Set fallback configuration
        os.environ['CHAT_FALLBACK_API_KEY'] = 'fallback-key'
        
        from src.utils import get_adaptive_chat_client
        
        client, model_used, is_fallback = get_adaptive_chat_client(model_preference='custom-model')
        
        assert client.api_key == 'fallback-key'
        assert model_used == 'custom-model'
        # is_fallback might be True because we're using fallback client

    def test_validation_chat_fallback_config_success(self):
        """Test validation of valid chat fallback configuration."""
        # Set valid configuration
        os.environ['CHAT_API_KEY'] = 'valid-key'
        os.environ['CHAT_API_BASE'] = 'https://valid.api.com/v1'
        
        from src.utils import validate_chat_fallback_config
        
        result = validate_chat_fallback_config()
        assert result is True

    def test_validation_chat_fallback_config_no_key(self):
        """Test validation fails when no API key is available."""
        # No API keys set
        
        from src.utils import validate_chat_fallback_config
        
        with pytest.raises(ValueError, match="No API key configured for chat fallback"):
            validate_chat_fallback_config()

    def test_validation_embeddings_fallback_config_success(self):
        """Test validation of valid embeddings fallback configuration."""
        # Set valid configuration
        os.environ['EMBEDDINGS_API_KEY'] = 'valid-key'
        os.environ['EMBEDDINGS_API_BASE'] = 'https://valid.api.com/v1'
        
        from src.utils import validate_embeddings_fallback_config
        
        result = validate_embeddings_fallback_config()
        assert result is True

    def test_validation_embeddings_fallback_config_no_key(self):
        """Test validation fails when no API key is available."""
        # No API keys set
        
        from src.utils import validate_embeddings_fallback_config
        
        with pytest.raises(ValueError, match="No API key configured for embeddings fallback"):
            validate_embeddings_fallback_config()

    def test_get_effective_fallback_config(self):
        """Test effective fallback configuration resolution."""
        # Set mixed configuration with inheritance
        os.environ['CHAT_API_KEY'] = 'primary-chat-key'
        os.environ['CHAT_FALLBACK_MODEL'] = 'gpt-3.5-turbo'
        os.environ['EMBEDDINGS_FALLBACK_API_KEY'] = 'explicit-embeddings-key'
        os.environ['EMBEDDINGS_FALLBACK_API_BASE'] = 'https://explicit.api.com/v1'
        
        from src.utils import get_effective_fallback_config
        
        config = get_effective_fallback_config()
        
        # Chat should inherit primary key
        assert config['chat_fallback']['model'] == 'gpt-3.5-turbo'
        assert config['chat_fallback']['api_key_source'] == 'CHAT_API_KEY (inherited)'
        
        # Embeddings should use explicit fallback configuration
        assert config['embeddings_fallback']['api_key_source'] == 'EMBEDDINGS_FALLBACK_API_KEY'
        assert config['embeddings_fallback']['base_url'] == 'https://explicit.api.com/v1'

    def test_no_configuration_error_handling(self):
        """Test error handling when no configuration is available."""
        # No environment variables set
        
        from src.utils import get_adaptive_chat_client
        
        with pytest.raises(ValueError, match="No valid API configuration available"):
            get_adaptive_chat_client()

    @patch('src.utils.get_chat_client')
    @patch('src.utils.get_chat_fallback_client')
    def test_performance_fallback_switching_time(self, mock_fallback_client, mock_primary_client):
        """Test that fallback switching is fast."""
        # Mock clients
        mock_primary_client.side_effect = ValueError("Primary failed")
        mock_fallback_client.return_value = Mock()
        
        # Set fallback configuration
        os.environ['CHAT_FALLBACK_API_KEY'] = 'fallback-key'
        os.environ['CHAT_FALLBACK_MODEL'] = 'gpt-3.5-turbo'
        os.environ['CHAT_API_KEY'] = 'primary-key'  # This will fail
        os.environ['CHAT_MODEL'] = 'gpt-4'
        
        from src.utils import get_adaptive_chat_client
        
        # Measure fallback switching time
        start_time = time.time()
        client, model_used, is_fallback = get_adaptive_chat_client()
        end_time = time.time()
        
        # Should be very fast (under 100ms)
        elapsed = end_time - start_time
        assert elapsed < 0.1, f"Fallback switching too slow: {elapsed:.3f}s"
        assert is_fallback is True
        assert model_used == 'gpt-3.5-turbo'

    def test_base_url_validation(self):
        """Test base URL validation in fallback configuration."""
        # Set invalid base URL
        os.environ['CHAT_API_KEY'] = 'valid-key'
        os.environ['CHAT_FALLBACK_API_BASE'] = 'not-a-valid-url'
        
        from src.utils import validate_chat_fallback_config
        
        with pytest.raises(ValueError, match="Invalid"):
            validate_chat_fallback_config()

    def test_partial_inheritance_scenarios(self):
        """Test various partial inheritance scenarios."""
        # Scenario 1: Inherit key but override base URL
        os.environ['CHAT_API_KEY'] = 'inherited-key'
        os.environ['CHAT_FALLBACK_API_BASE'] = 'https://fallback.api.com/v1'
        
        from src.utils import get_chat_fallback_client
        
        client = get_chat_fallback_client()
        assert client.api_key == 'inherited-key'
        assert str(client.base_url).rstrip('/') == 'https://fallback.api.com/v1'

    def test_real_world_provider_combinations(self):
        """Test realistic provider combination scenarios."""
        # Scenario: Expensive primary (GPT-4 via OpenAI), cheap fallback (GPT-3.5 via OpenRouter)
        os.environ['CHAT_MODEL'] = 'gpt-4'
        os.environ['CHAT_API_KEY'] = 'openai-key'
        # Primary uses default OpenAI
        
        os.environ['CHAT_FALLBACK_MODEL'] = 'openai/gpt-3.5-turbo'
        os.environ['CHAT_FALLBACK_API_KEY'] = 'openrouter-key'
        os.environ['CHAT_FALLBACK_API_BASE'] = 'https://openrouter.ai/api/v1'
        
        from src.utils import get_chat_client, get_chat_fallback_client
        
        primary_client = get_chat_client()
        fallback_client = get_chat_fallback_client()
        
        # Primary: OpenAI with expensive model
        assert primary_client.api_key == 'openai-key'
        assert str(primary_client.base_url).rstrip('/') == 'https://api.openai.com/v1'
        
        # Fallback: OpenRouter with cheaper model
        assert fallback_client.api_key == 'openrouter-key'
        assert str(fallback_client.base_url).rstrip('/') == 'https://openrouter.ai/api/v1'


================================================
FILE: tests/test_flexible_api_config.py
================================================
"""
Integration tests for flexible API configuration system.

Tests the new configurable OpenAI client system with different providers,
backward compatibility, error handling, and performance.
"""
import pytest
import os
import time
from unittest.mock import Mock, patch
from pathlib import Path
import sys

# Add src to path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))


class TestFlexibleAPIConfiguration:
    """Test the new flexible API configuration system."""

    def setup_method(self):
        """Set up test environment for each test."""
        # Save original environment (including conftest values)
        self.original_env = {
            key: os.environ.get(key) for key in [
                'CHAT_MODEL', 'CHAT_API_KEY', 'CHAT_API_BASE',
                'EMBEDDINGS_MODEL', 'EMBEDDINGS_API_KEY', 'EMBEDDINGS_API_BASE',
            ]
        }
        
        # Clear environment for clean testing (remove conftest interference)
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]

    def teardown_method(self):
        """Restore original environment after each test."""
        # Clear test environment
        for key in self.original_env:
            if key in os.environ:
                del os.environ[key]
        
        # Restore original values
        for key, value in self.original_env.items():
            if value is not None:
                os.environ[key] = value

    def test_chat_client_new_configuration(self):
        """Test chat client with new CHAT_* environment variables."""
        # Set new configuration
        os.environ['CHAT_MODEL'] = 'gpt-4'
        os.environ['CHAT_API_KEY'] = 'test-chat-key'
        os.environ['CHAT_API_BASE'] = 'https://api.openai.com/v1'
        
        from src.utils import get_chat_client
        
        client = get_chat_client()
        assert client.api_key == 'test-chat-key'
        assert str(client.base_url).rstrip('/') == 'https://api.openai.com/v1'

    def test_embeddings_client_new_configuration(self):
        """Test embeddings client with new EMBEDDINGS_* environment variables."""
        # Set new configuration
        os.environ['EMBEDDINGS_MODEL'] = 'text-embedding-3-large'
        os.environ['EMBEDDINGS_API_KEY'] = 'test-embeddings-key'
        os.environ['EMBEDDINGS_API_BASE'] = 'https://api.openai.com/v1'
        
        from src.utils import get_embeddings_client
        
        client = get_embeddings_client()
        assert client.api_key == 'test-embeddings-key'
        assert str(client.base_url).rstrip('/') == 'https://api.openai.com/v1'


    def test_azure_openai_configuration(self):
        """Test configuration for Azure OpenAI service."""
        os.environ['CHAT_API_KEY'] = 'azure-api-key'
        os.environ['CHAT_API_BASE'] = 'https://my-resource.openai.azure.com/'
        os.environ['EMBEDDINGS_API_KEY'] = 'azure-embeddings-key'
        os.environ['EMBEDDINGS_API_BASE'] = 'https://my-resource.openai.azure.com/'
        
        from src.utils import get_chat_client, get_embeddings_client
        
        chat_client = get_chat_client()
        embeddings_client = get_embeddings_client()
        
        assert chat_client.api_key == 'azure-api-key'
        assert str(chat_client.base_url).rstrip('/') == 'https://my-resource.openai.azure.com'
        assert embeddings_client.api_key == 'azure-embeddings-key'
        assert str(embeddings_client.base_url).rstrip('/') == 'https://my-resource.openai.azure.com'

    def test_localai_configuration(self):
        """Test configuration for LocalAI service."""
        os.environ['CHAT_API_KEY'] = 'not-needed'
        os.environ['CHAT_API_BASE'] = 'http://localhost:8080/v1'
        os.environ['EMBEDDINGS_API_KEY'] = 'not-needed'
        os.environ['EMBEDDINGS_API_BASE'] = 'http://localhost:8080/v1'
        
        from src.utils import get_chat_client, get_embeddings_client
        
        chat_client = get_chat_client()
        embeddings_client = get_embeddings_client()
        
        assert str(chat_client.base_url).rstrip('/') == 'http://localhost:8080/v1'
        assert str(embeddings_client.base_url).rstrip('/') == 'http://localhost:8080/v1'

    def test_error_handling_missing_api_key(self):
        """Test error handling when no API key is configured."""
        # Don't set any API keys
        
        from src.utils import get_chat_client, get_embeddings_client
        
        with pytest.raises(ValueError, match="No API key configured for chat model"):
            get_chat_client()
        
        with pytest.raises(ValueError, match="No API key configured for embeddings"):
            get_embeddings_client()

    def test_validation_functions(self):
        """Test configuration validation functions."""
        from src.utils import validate_chat_config, validate_embeddings_config
        
        # Test with missing configuration
        with pytest.raises(ValueError, match="No API key configured"):
            validate_chat_config()
        
        with pytest.raises(ValueError, match="No API key configured"):
            validate_embeddings_config()
        
        # Test with valid configuration
        os.environ['CHAT_API_KEY'] = 'test-key'
        os.environ['EMBEDDINGS_API_KEY'] = 'test-key'
        
        assert validate_chat_config() is True
        assert validate_embeddings_config() is True


    def test_embeddings_model_configuration(self):
        """Test that EMBEDDINGS_MODEL is properly used."""
        os.environ['EMBEDDINGS_MODEL'] = 'text-embedding-3-large'
        os.environ['EMBEDDINGS_API_KEY'] = 'test-key'
        
        from src.utils import create_embeddings_batch
        
        # Mock the embeddings client
        with patch('src.utils.get_embeddings_client') as mock_get_client:
            mock_client = Mock()
            mock_response = Mock()
            mock_response.data = [Mock(embedding=[0.1] * 1536)]
            mock_client.embeddings.create.return_value = mock_response
            mock_get_client.return_value = mock_client
            
            # Call function that creates embeddings
            create_embeddings_batch(['test text'])
            
            # Should use EMBEDDINGS_MODEL
            call_args = mock_client.embeddings.create.call_args
            assert call_args[1]['model'] == 'text-embedding-3-large'

    @patch('src.utils.get_chat_client')
    @patch('src.utils.get_embeddings_client')
    def test_performance_no_regression(self, mock_embeddings_client, mock_chat_client):
        """Test that new configuration doesn't introduce performance regression."""
        # Set up mocks
        mock_chat_client.return_value = Mock()
        mock_embeddings_client.return_value = Mock()
        
        # Set configuration
        os.environ['CHAT_MODEL'] = 'gpt-3.5-turbo'
        os.environ['CHAT_API_KEY'] = 'test-key'
        os.environ['EMBEDDINGS_MODEL'] = 'text-embedding-3-small'
        os.environ['EMBEDDINGS_API_KEY'] = 'test-key'
        
        from src.utils import get_chat_client, get_embeddings_client
        
        # Measure client creation time
        start_time = time.time()
        for _ in range(100):
            get_chat_client()
            get_embeddings_client()
        end_time = time.time()
        
        # Should be very fast (under 100ms for 100 iterations)
        elapsed = end_time - start_time
        assert elapsed < 0.1, f"Client creation too slow: {elapsed:.3f}s"

    def test_mixed_provider_configuration(self):
        """Test using different providers for chat and embeddings."""
        # Chat via Azure OpenAI
        os.environ['CHAT_API_KEY'] = 'azure-chat-key'
        os.environ['CHAT_API_BASE'] = 'https://azure.openai.azure.com/'
        
        # Embeddings via regular OpenAI
        os.environ['EMBEDDINGS_API_KEY'] = 'openai-embeddings-key'
        # No EMBEDDINGS_API_BASE, should use default OpenAI
        
        from src.utils import get_chat_client, get_embeddings_client
        
        chat_client = get_chat_client()
        embeddings_client = get_embeddings_client()
        
        # Chat should use Azure
        assert chat_client.api_key == 'azure-chat-key'
        assert str(chat_client.base_url).rstrip('/') == 'https://azure.openai.azure.com'
        
        # Embeddings should use regular OpenAI (default base_url)
        assert embeddings_client.api_key == 'openai-embeddings-key'
        assert str(embeddings_client.base_url).rstrip('/') == 'https://api.openai.com/v1'


    def test_configuration_isolation(self):
        """Test that chat and embeddings configurations are independent."""
        # Different providers for chat and embeddings
        os.environ['CHAT_API_KEY'] = 'chat-provider-key'
        os.environ['CHAT_API_BASE'] = 'https://chat-provider.com/v1'
        
        os.environ['EMBEDDINGS_API_KEY'] = 'embeddings-provider-key'
        os.environ['EMBEDDINGS_API_BASE'] = 'https://embeddings-provider.com/v1'
        
        from src.utils import get_chat_client, get_embeddings_client
        
        chat_client = get_chat_client()
        embeddings_client = get_embeddings_client()
        
        # Each should use its own configuration
        assert chat_client.api_key == 'chat-provider-key'
        assert str(chat_client.base_url).rstrip('/') == 'https://chat-provider.com/v1'
        
        assert embeddings_client.api_key == 'embeddings-provider-key'
        assert str(embeddings_client.base_url).rstrip('/') == 'https://embeddings-provider.com/v1'





================================================
FILE: tests/test_github_processor.py
================================================
"""
Unit tests for GitHub processor utilities.

Tests the GitHubRepoManager, MarkdownDiscovery, and GitHubMetadataExtractor classes.
"""
import pytest
import os
import sys
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch, mock_open

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from utils.github_processor import (
    GitHubRepoManager, MarkdownDiscovery, GitHubMetadataExtractor,
    MultiFileDiscovery, PythonProcessor, TypeScriptProcessor, 
    ConfigProcessor, MarkdownProcessor
)
from utils.validation import validate_github_url, normalize_github_url


class TestGitHubRepoManager:
    """Test cases for GitHubRepoManager class."""
    
    def test_init(self):
        """Test initialization of GitHubRepoManager."""
        manager = GitHubRepoManager()
        assert manager.temp_dirs == []
        assert manager.logger is not None
    
    def test_is_valid_github_url(self):
        """Test GitHub URL validation."""
        manager = GitHubRepoManager()
        
        # Valid URLs
        assert manager._is_valid_github_url("https://github.com/user/repo") is True
        assert manager._is_valid_github_url("https://github.com/user/repo.git") is True
        assert manager._is_valid_github_url("https://www.github.com/user/repo") is True
        
        # Invalid URLs
        assert manager._is_valid_github_url("https://gitlab.com/user/repo") is False
        assert manager._is_valid_github_url("https://github.com/user") is False
        assert manager._is_valid_github_url("https://github.com/") is False
        assert manager._is_valid_github_url("invalid-url") is False
    
    def test_normalize_clone_url(self):
        """Test URL normalization for git clone."""
        manager = GitHubRepoManager()
        
        assert manager._normalize_clone_url("https://github.com/user/repo") == "https://github.com/user/repo.git"
        assert manager._normalize_clone_url("https://github.com/user/repo.git") == "https://github.com/user/repo.git"
        assert manager._normalize_clone_url("https://github.com/user/repo/") == "https://github.com/user/repo.git"
    
    @patch('utils.github_processor.tempfile.mkdtemp')
    @patch('utils.github_processor.subprocess.run')
    @patch('utils.github_processor.os.walk')
    @patch('utils.github_processor.os.path.getsize')
    def test_clone_repository_success(self, mock_getsize, mock_walk, mock_subprocess, mock_mkdtemp):
        """Test successful repository cloning."""
        # Setup mocks
        mock_mkdtemp.return_value = "/tmp/test_clone"
        mock_subprocess.return_value = Mock(returncode=0, stderr="")
        mock_walk.return_value = [("/tmp/test_clone", [], ["file1.md", "file2.txt"])]
        mock_getsize.side_effect = [1024, 2048]  # File sizes
        
        manager = GitHubRepoManager()
        
        # Test
        result = manager.clone_repository("https://github.com/user/repo")
        
        # Verify
        assert result == "/tmp/test_clone"
        assert "/tmp/test_clone" in manager.temp_dirs
        mock_subprocess.assert_called_once_with(
            ["git", "clone", "--depth", "1", "https://github.com/user/repo.git", "/tmp/test_clone"],
            capture_output=True,
            text=True,
            timeout=300
        )
    
    @patch('utils.github_processor.tempfile.mkdtemp')
    @patch('utils.github_processor.subprocess.run')
    def test_clone_repository_invalid_url(self, mock_subprocess, mock_mkdtemp):
        """Test cloning with invalid URL."""
        mock_mkdtemp.return_value = "/tmp/test_clone"
        
        manager = GitHubRepoManager()
        
        with pytest.raises(ValueError, match="Invalid GitHub repository URL"):
            manager.clone_repository("https://gitlab.com/user/repo")
    
    @patch('utils.github_processor.tempfile.mkdtemp')
    @patch('utils.github_processor.subprocess.run')
    def test_clone_repository_git_failure(self, mock_subprocess, mock_mkdtemp):
        """Test cloning when git command fails."""
        mock_mkdtemp.return_value = "/tmp/test_clone"
        mock_subprocess.return_value = Mock(returncode=1, stderr="Repository not found")
        
        manager = GitHubRepoManager()
        
        with pytest.raises(RuntimeError, match="Git clone failed"):
            manager.clone_repository("https://github.com/user/nonexistent")
    
    @patch('utils.github_processor.tempfile.mkdtemp')
    @patch('utils.github_processor.subprocess.run')
    @patch('utils.github_processor.os.walk')
    @patch('utils.github_processor.os.path.getsize')
    def test_clone_repository_too_large(self, mock_getsize, mock_walk, mock_subprocess, mock_mkdtemp):
        """Test cloning when repository is too large."""
        # Setup mocks
        mock_mkdtemp.return_value = "/tmp/test_clone"
        mock_subprocess.return_value = Mock(returncode=0, stderr="")
        mock_walk.return_value = [("/tmp/test_clone", [], ["large_file.bin"])]
        mock_getsize.return_value = 600 * 1024 * 1024  # 600MB file
        
        manager = GitHubRepoManager()
        
        with pytest.raises(RuntimeError, match="Repository too large"):
            manager.clone_repository("https://github.com/user/large-repo", max_size_mb=500)
    
    @patch('shutil.rmtree')
    @patch('os.path.exists')
    def test_cleanup(self, mock_exists, mock_rmtree):
        """Test cleanup of temporary directories."""
        mock_exists.return_value = True
        
        manager = GitHubRepoManager()
        manager.temp_dirs = ["/tmp/dir1", "/tmp/dir2"]
        
        manager.cleanup()
        
        assert manager.temp_dirs == []
        assert mock_rmtree.call_count == 2


class TestMarkdownDiscovery:
    """Test cases for MarkdownDiscovery class."""
    
    def test_init(self):
        """Test initialization of MarkdownDiscovery."""
        discovery = MarkdownDiscovery()
        assert discovery.logger is not None
        assert discovery.EXCLUDED_DIRS is not None
        assert discovery.EXCLUDED_PATTERNS is not None
    
    def test_is_markdown_file(self):
        """Test markdown file detection."""
        discovery = MarkdownDiscovery()
        
        # Valid markdown files
        assert discovery._is_markdown_file("README.md") is True
        assert discovery._is_markdown_file("doc.markdown") is True
        assert discovery._is_markdown_file("guide.mdown") is True
        assert discovery._is_markdown_file("notes.mkd") is True
        
        # Non-markdown files
        assert discovery._is_markdown_file("script.py") is False
        assert discovery._is_markdown_file("data.json") is False
        assert discovery._is_markdown_file("image.png") is False
    
    def test_should_exclude_dir(self):
        """Test directory exclusion logic."""
        discovery = MarkdownDiscovery()
        
        # Excluded directories
        assert discovery._should_exclude_dir(".git") is True
        assert discovery._should_exclude_dir("node_modules") is True
        assert discovery._should_exclude_dir(".hidden") is True
        
        # Included directories
        assert discovery._should_exclude_dir("docs") is False
        assert discovery._should_exclude_dir("src") is False
        assert discovery._should_exclude_dir("examples") is False
    
    def test_should_exclude_file(self):
        """Test file exclusion logic."""
        discovery = MarkdownDiscovery()
        
        # Excluded files
        assert discovery._should_exclude_file("CHANGELOG.md") is True
        assert discovery._should_exclude_file("package-lock.json") is True
        assert discovery._should_exclude_file("script.min.js") is True
        
        # Included files
        assert discovery._should_exclude_file("README.md") is False
        assert discovery._should_exclude_file("guide.md") is False
    
    def test_is_readme_file(self):
        """Test README file detection."""
        discovery = MarkdownDiscovery()
        
        assert discovery._is_readme_file("README.md") is True
        assert discovery._is_readme_file("readme.txt") is True
        assert discovery._is_readme_file("ReadMe.markdown") is True
        assert discovery._is_readme_file("guide.md") is False
    
    def test_file_priority_key(self):
        """Test file priority calculation."""
        discovery = MarkdownDiscovery()
        
        readme_file = {
            'is_readme': True,
            'word_count': 1000
        }
        
        regular_file = {
            'is_readme': False,
            'word_count': 2000
        }
        
        readme_priority = discovery._file_priority_key(readme_file)
        regular_priority = discovery._file_priority_key(regular_file)
        
        # README files should have higher priority
        assert readme_priority[0] > regular_priority[0]
    
    @patch('utils.github_processor.os.walk')
    @patch('utils.github_processor.os.stat')
    @patch('builtins.open', new_callable=mock_open)
    def test_discover_markdown_files(self, mock_file, mock_stat, mock_walk):
        """Test markdown file discovery."""
        # Setup mocks
        mock_walk.return_value = [
            ("/repo", ["docs", ".git"], ["README.md", "script.py"]),
            ("/repo/docs", [], ["guide.md", "api.md"])
        ]
        
        mock_stat.return_value = Mock(st_size=5000)
        mock_file.return_value.read.return_value = "# Test\nThis is test content with enough words to pass the minimum."
        
        discovery = MarkdownDiscovery()
        
        # Test
        result = discovery.discover_markdown_files("/repo", max_files=10)
        
        # Verify
        assert len(result) > 0
        assert all(f['filename'].endswith('.md') for f in result)
        assert all('content' in f for f in result)
        assert all('relative_path' in f for f in result)


class TestGitHubMetadataExtractor:
    """Test cases for GitHubMetadataExtractor class."""
    
    def test_init(self):
        """Test initialization of GitHubMetadataExtractor."""
        extractor = GitHubMetadataExtractor()
        assert extractor.logger is not None
    
    def test_parse_repo_info(self):
        """Test repository information parsing."""
        extractor = GitHubMetadataExtractor()
        
        # Test various URL formats
        owner, repo = extractor._parse_repo_info("https://github.com/user/repo")
        assert owner == "user"
        assert repo == "repo"
        
        owner, repo = extractor._parse_repo_info("https://github.com/org/project.git")
        assert owner == "org"
        assert repo == "project"
        
        # Test invalid URL
        with pytest.raises(ValueError):
            extractor._parse_repo_info("https://github.com/user")
    
    @patch('builtins.open', new_callable=mock_open)
    @patch('utils.github_processor.os.path.exists')
    def test_extract_package_info_nodejs(self, mock_exists, mock_file):
        """Test package.json extraction."""
        # Setup mocks
        mock_exists.side_effect = lambda path: path.endswith('package.json')
        package_json = {
            "name": "test-package",
            "description": "A test package",
            "version": "1.0.0",
            "license": "MIT"
        }
        mock_file.return_value.read.return_value = str(package_json).replace("'", '"')
        
        extractor = GitHubMetadataExtractor()
        
        with patch('json.load', return_value=package_json):
            result = extractor._extract_package_info("/repo")
        
        assert result['language'] == 'javascript'
        assert result['package_name'] == 'test-package'
        assert result['description'] == 'A test package'
        assert result['version'] == '1.0.0'
        assert result['license'] == 'MIT'
    
    @patch('builtins.open', new_callable=mock_open)
    @patch('utils.github_processor.os.path.exists')
    def test_extract_package_info_python(self, mock_exists, mock_file):
        """Test pyproject.toml extraction."""
        # Setup mocks
        mock_exists.side_effect = lambda path: path.endswith('pyproject.toml')
        toml_content = """
        [project]
        name = "test-package"
        description = "A Python test package"
        """
        mock_file.return_value.read.return_value = toml_content
        
        extractor = GitHubMetadataExtractor()
        result = extractor._extract_package_info("/repo")
        
        assert result['language'] == 'python'
        assert result['package_name'] == 'test-package'
        assert result['description'] == 'A Python test package'
    
    @patch('builtins.open', new_callable=mock_open)
    @patch('utils.github_processor.os.path.exists')
    def test_extract_readme_info(self, mock_exists, mock_file):
        """Test README extraction."""
        # Setup mocks
        mock_exists.side_effect = lambda path: path.endswith('README.md')
        readme_content = """# Test Project
        
        This is a test project for demonstration.
        
        ## Features
        - Feature 1
        - Feature 2
        """
        mock_file.return_value.read.return_value = readme_content
        
        extractor = GitHubMetadataExtractor()
        result = extractor._extract_readme_info("/repo")
        
        assert result['readme_title'] == 'Test Project'
    
    @patch('utils.github_processor.subprocess.run')
    def test_extract_git_info(self, mock_subprocess):
        """Test Git information extraction."""
        # Setup mock
        mock_subprocess.return_value = Mock(
            returncode=0,
            stdout="abc123|Initial commit|2024-01-15 10:30:00 +0000"
        )
        
        extractor = GitHubMetadataExtractor()
        result = extractor._extract_git_info("/repo")
        
        assert result['latest_commit_hash'] == 'abc123'
        assert result['latest_commit_message'] == 'Initial commit'
        assert result['latest_commit_date'] == '2024-01-15 10:30:00 +0000'
    
    def test_extract_repo_metadata_integration(self):
        """Test complete repository metadata extraction."""
        extractor = GitHubMetadataExtractor()
        
        with patch.object(extractor, '_parse_repo_info', return_value=('user', 'repo')), \
             patch.object(extractor, '_extract_package_info', return_value={'language': 'python'}), \
             patch.object(extractor, '_extract_readme_info', return_value={'readme_title': 'Test'}), \
             patch.object(extractor, '_extract_git_info', return_value={'latest_commit_hash': 'abc123'}):
            
            result = extractor.extract_repo_metadata("https://github.com/user/repo", "/repo")
            
            assert result['repo_url'] == "https://github.com/user/repo"
            assert result['owner'] == 'user'
            assert result['repo_name'] == 'repo'
            assert result['full_name'] == 'user/repo'
            assert result['source_type'] == 'github_repository'
            assert result['language'] == 'python'
            assert result['readme_title'] == 'Test'
            assert result['latest_commit_hash'] == 'abc123'


class TestValidationFunctions:
    """Test cases for validation functions."""
    
    def test_validate_github_url_valid(self):
        """Test validation of valid GitHub URLs."""
        valid_urls = [
            "https://github.com/user/repo",
            "https://github.com/user/repo.git",
            "https://www.github.com/user/repo",
            "http://github.com/org/project",
            "https://github.com/user/repo/tree/main",
            "https://github.com/user/repo/blob/main/README.md"
        ]
        
        for url in valid_urls:
            is_valid, error = validate_github_url(url)
            assert is_valid, f"URL should be valid: {url}, Error: {error}"
            assert error == ""
    
    def test_validate_github_url_invalid(self):
        """Test validation of invalid GitHub URLs."""
        invalid_urls = [
            ("", "URL must be a non-empty string"),
            ("not-a-url", "URL must use http or https scheme"),
            ("https://gitlab.com/user/repo", "URL must be from github.com"),
            ("https://github.com/user", "URL must include both owner and repository name"),
            ("https://github.com/", "URL must include repository path"),
            ("ftp://github.com/user/repo", "URL must use http or https scheme"),
            ("https://github.com/user-/repo", "Invalid GitHub owner/organization name format"),
            ("https://github.com/user/repo-", "Invalid GitHub repository name format")
        ]
        
        for url, expected_error in invalid_urls:
            is_valid, error = validate_github_url(url)
            assert not is_valid, f"URL should be invalid: {url}"
            assert expected_error in error, f"Expected error '{expected_error}' in '{error}'"
    
    def test_normalize_github_url(self):
        """Test GitHub URL normalization."""
        test_cases = [
            ("https://github.com/user/repo", "https://github.com/user/repo.git"),
            ("https://github.com/user/repo.git", "https://github.com/user/repo.git"),
            ("https://github.com/user/repo/tree/main", "https://github.com/user/repo.git"),
            ("https://www.github.com/user/repo", "https://github.com/user/repo.git")
        ]
        
        for input_url, expected in test_cases:
            result = normalize_github_url(input_url)
            assert result == expected, f"Expected {expected}, got {result}"
    
    def test_normalize_github_url_invalid(self):
        """Test normalization with invalid URLs."""
        with pytest.raises(ValueError, match="Invalid GitHub URL"):
            normalize_github_url("https://gitlab.com/user/repo")


class TestPythonProcessor:
    """Test cases for PythonProcessor class."""
    
    def test_process_file_with_docstrings(self):
        """Test processing Python file with docstrings."""
        processor = PythonProcessor()
        
        python_content = '''"""
Module docstring for testing.
This module contains test functions and classes.
"""

def test_function(arg1: str, arg2: int) -> bool:
    """
    Test function with parameters and return type.
    
    Args:
        arg1: First argument as string
        arg2: Second argument as integer
        
    Returns:
        Boolean result
    """
    return True

class TestClass:
    """
    Test class for demonstration.
    
    This class shows how docstrings are extracted.
    """
    
    def method(self, param: str) -> None:
        """Method with docstring."""
        pass

async def async_function():
    """Async function docstring."""
    pass
'''
        
        # Create temp file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(python_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "test.py")
            
            # Should extract module, function, class, and async function docstrings
            assert len(result) >= 4
            
            # Check module docstring
            module_docs = [item for item in result if item['type'] == 'module']
            assert len(module_docs) == 1
            assert 'Module docstring for testing' in module_docs[0]['content']
            assert module_docs[0]['language'] == 'python'
            
            # Check function docstring
            function_docs = [item for item in result if item['type'] == 'function']
            assert len(function_docs) >= 2  # test_function and async_function
            
            test_func = next(item for item in function_docs if item['name'] == 'test_function')
            assert 'Test function with parameters' in test_func['content']
            assert '(arg1: str, arg2: int) -> bool' in test_func['signature']
            
            # Check class docstring
            class_docs = [item for item in result if item['type'] == 'class']
            assert len(class_docs) == 1
            assert 'Test class for demonstration' in class_docs[0]['content']
            assert class_docs[0]['name'] == 'TestClass'
            
        finally:
            os.unlink(temp_path)
    
    def test_process_file_syntax_error(self):
        """Test processing Python file with syntax error."""
        processor = PythonProcessor()
        
        invalid_python = '''
def invalid_syntax(
    # Missing closing parenthesis
    pass
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(invalid_python)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "invalid.py")
            # Should return empty list for syntax errors
            assert result == []
        finally:
            os.unlink(temp_path)
    
    def test_process_file_no_docstrings(self):
        """Test processing Python file without docstrings."""
        processor = PythonProcessor()
        
        python_content = '''
def function_without_docstring():
    return True

class ClassWithoutDocstring:
    def method(self):
        pass
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(python_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "no_docs.py")
            # Should return empty list when no docstrings found
            assert result == []
        finally:
            os.unlink(temp_path)
    
    @patch('os.path.getsize')
    def test_process_file_too_large(self, mock_getsize):
        """Test processing Python file that's too large."""
        processor = PythonProcessor()
        mock_getsize.return_value = 2_000_000  # 2MB - over 1MB limit
        
        result = processor.process_file("/fake/path.py", "large.py")
        assert result == []


class TestTypeScriptProcessor:
    """Test cases for TypeScriptProcessor class."""
    
    def test_process_file_with_jsdoc(self):
        """Test processing TypeScript file with JSDoc comments."""
        processor = TypeScriptProcessor()
        
        typescript_content = '''
/**
 * Calculates the area of a rectangle.
 * @param width - The width of the rectangle
 * @param height - The height of the rectangle
 * @returns The area of the rectangle
 * @example
 * ```typescript
 * const area = calculateArea(5, 10);
 * console.log(area); // 50
 * ```
 */
function calculateArea(width: number, height: number): number {
    return width * height;
}

/**
 * A class representing a database connection.
 * @public
 */
export class DatabaseConnection {
    /**
     * Connects to the database using the provided configuration.
     * @param config - The database configuration object
     * @returns A promise that resolves when the connection is established
     */
    async connect(config: DatabaseConfig): Promise<void> {
        // Implementation
    }
}

/**
 * User interface definition.
 * @interface
 */
export interface User {
    id: number;
    name: string;
    email: string;
}
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.ts', delete=False) as f:
            f.write(typescript_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "test.ts")
            
            # Should extract function, class, and interface JSDoc
            assert len(result) >= 2
            
            # Check function JSDoc
            func_docs = [item for item in result if item['type'] == 'function']
            assert len(func_docs) >= 1
            
            calc_func = next((item for item in func_docs if 'calculateArea' in item['name']), None)
            assert calc_func is not None
            assert 'Calculates the area of a rectangle' in calc_func['content']
            assert calc_func['language'] == 'typescript'
            
            # Check class JSDoc
            class_docs = [item for item in result if item['type'] == 'class']
            if class_docs:  # JSDoc might not always associate correctly
                assert 'DatabaseConnection' in class_docs[0]['name']
                assert 'database connection' in class_docs[0]['content']
            
        finally:
            os.unlink(temp_path)
    
    def test_process_file_minified(self):
        """Test processing minified TypeScript file."""
        processor = TypeScriptProcessor()
        
        # Minified content (long single line)
        minified_content = 'function test(){return true;}' + 'x' * 1000  # Make it long
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.ts', delete=False) as f:
            f.write(minified_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "minified.ts")
            # Should return empty list for minified files
            assert result == []
        finally:
            os.unlink(temp_path)
    
    def test_process_file_no_jsdoc(self):
        """Test processing TypeScript file without JSDoc."""
        processor = TypeScriptProcessor()
        
        typescript_content = '''
function normalFunction(): void {
    console.log("No JSDoc here");
}

class RegularClass {
    method(): string {
        return "test";
    }
}
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.ts', delete=False) as f:
            f.write(typescript_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "no_jsdoc.ts")
            # Should return empty list when no JSDoc found
            assert result == []
        finally:
            os.unlink(temp_path)


class TestConfigProcessor:
    """Test cases for ConfigProcessor class."""
    
    def test_process_json_file(self):
        """Test processing JSON configuration file."""
        processor = ConfigProcessor()
        
        json_content = '''
{
    "name": "test-app",
    "version": "1.0.0",
    "scripts": {
        "start": "node server.js",
        "test": "jest"
    },
    "dependencies": {
        "express": "^4.18.0"
    }
}
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            f.write(json_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "package.json")
            
            assert len(result) == 1
            assert result[0]['type'] == 'configuration'
            assert result[0]['language'] == 'json'
            assert result[0]['name'] == os.path.basename(temp_path)
            assert 'test-app' in result[0]['content']
            
        finally:
            os.unlink(temp_path)
    
    def test_process_yaml_file(self):
        """Test processing YAML configuration file."""
        processor = ConfigProcessor()
        
        yaml_content = '''
version: '3.8'
services:
  web:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./html:/usr/share/nginx/html
  database:
    image: postgres:13
    environment:
      POSTGRES_DB: myapp
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
            f.write(yaml_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "docker-compose.yaml")
            
            assert len(result) == 1
            assert result[0]['type'] == 'configuration' 
            assert result[0]['language'] == 'yaml'
            assert 'services:' in result[0]['content']
            
        finally:
            os.unlink(temp_path)
    
    def test_process_toml_file(self):
        """Test processing TOML configuration file."""
        processor = ConfigProcessor()
        
        toml_content = '''
[project]
name = "my-python-project"
version = "0.1.0"
description = "A sample Python project"

[project.dependencies]
requests = "^2.28.0"
click = "^8.1.0"

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.toml', delete=False) as f:
            f.write(toml_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "pyproject.toml")
            
            assert len(result) == 1
            assert result[0]['type'] == 'configuration'
            assert result[0]['language'] == 'toml'
            assert 'my-python-project' in result[0]['content']
            
        finally:
            os.unlink(temp_path)
    
    @patch('os.path.getsize')
    def test_process_file_too_large(self, mock_getsize):
        """Test processing config file that's too large."""
        processor = ConfigProcessor()
        mock_getsize.return_value = 150_000  # 150KB - over 100KB limit
        
        result = processor.process_file("/fake/config.json", "large.json")
        assert result == []
    
    def test_process_empty_file(self):
        """Test processing empty configuration file."""
        processor = ConfigProcessor()
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            f.write("")  # Empty file
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "empty.json")
            assert result == []
        finally:
            os.unlink(temp_path)


class TestMarkdownProcessor:
    """Test cases for MarkdownProcessor class."""
    
    def test_process_markdown_file(self):
        """Test processing markdown file."""
        processor = MarkdownProcessor()
        
        markdown_content = '''# Test Project

This is a test markdown file with various content.

## Features

- Feature 1: Something awesome
- Feature 2: Something even better

## Code Example

```python
def hello():
    print("Hello, World!")
```

## Conclusion

This concludes our test markdown file.
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
            f.write(markdown_content)
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "test.md")
            
            assert len(result) == 1
            assert result[0]['type'] == 'markdown'
            assert result[0]['language'] == 'markdown'
            assert result[0]['name'] == os.path.basename(temp_path)
            assert 'Test Project' in result[0]['content']
            assert '```python' in result[0]['content']
            
        finally:
            os.unlink(temp_path)
    
    def test_process_empty_markdown(self):
        """Test processing empty or very short markdown file."""
        processor = MarkdownProcessor()
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
            f.write("# Short")  # Too short
            temp_path = f.name
        
        try:
            result = processor.process_file(temp_path, "short.md")
            assert result == []
        finally:
            os.unlink(temp_path)


class TestMultiFileDiscovery:
    """Test cases for MultiFileDiscovery class."""
    
    def test_init(self):
        """Test initialization of MultiFileDiscovery."""
        discovery = MultiFileDiscovery()
        assert discovery.logger is not None
        assert discovery.SUPPORTED_EXTENSIONS is not None
        assert discovery.FILE_SIZE_LIMITS is not None
        assert '.py' in discovery.SUPPORTED_EXTENSIONS
        assert '.ts' in discovery.SUPPORTED_EXTENSIONS
        assert '.json' in discovery.SUPPORTED_EXTENSIONS
    
    def test_is_supported_file(self):
        """Test file type support detection."""
        discovery = MultiFileDiscovery()
        
        # Test Python files
        assert discovery._is_supported_file("script.py", ['.py']) is True
        assert discovery._is_supported_file("script.py", ['.js']) is False
        
        # Test TypeScript files
        assert discovery._is_supported_file("component.ts", ['.ts']) is True
        assert discovery._is_supported_file("component.tsx", ['.tsx']) is True
        
        # Test config files
        assert discovery._is_supported_file("config.json", ['.json']) is True
        assert discovery._is_supported_file("docker.yaml", ['.yaml']) is True
        assert discovery._is_supported_file("pyproject.toml", ['.toml']) is True
        
        # Test markdown files
        assert discovery._is_supported_file("README.md", ['.md']) is True
        assert discovery._is_supported_file("guide.markdown", ['.markdown']) is True
        
        # Test unsupported files
        assert discovery._is_supported_file("image.png", ['.png']) is False
        assert discovery._is_supported_file("data.csv", ['.csv']) is False
    
    @patch('utils.github_processor.os.walk')
    @patch('utils.github_processor.os.stat')
    @patch('builtins.open', new_callable=mock_open)
    def test_discover_files_multi_type(self, mock_file, mock_stat, mock_walk):
        """Test discovering multiple file types."""
        # Setup mocks
        mock_walk.return_value = [
            ("/repo", ["src", "docs"], ["README.md", "package.json", "script.py"]),
            ("/repo/src", [], ["main.ts", "utils.py", "config.yaml"]),
            ("/repo/docs", [], ["guide.md", "api.md"])
        ]
        
        mock_stat.return_value = Mock(st_size=5000)
        mock_file.return_value.read.return_value = "test content"
        
        discovery = MultiFileDiscovery()
        
        # Test discovering multiple types
        result = discovery.discover_files("/repo", file_types=['.md', '.py', '.ts', '.json', '.yaml'])
        
        # Should find files of all requested types
        found_extensions = {os.path.splitext(f['filename'])[1] for f in result}
        expected_extensions = {'.md', '.py', '.ts', '.json', '.yaml'}
        assert expected_extensions.issubset(found_extensions)
        
        # Verify file info structure
        for file_info in result:
            assert 'path' in file_info
            assert 'relative_path' in file_info
            assert 'filename' in file_info
            assert 'size_bytes' in file_info
            assert 'file_type' in file_info
            assert 'is_readme' in file_info
    
    @patch('utils.github_processor.os.walk')
    def test_discover_files_empty_result(self, mock_walk):
        """Test discovering files with no matches."""
        mock_walk.return_value = [("/repo", [], ["script.js", "style.css"])]
        
        discovery = MultiFileDiscovery()
        result = discovery.discover_files("/repo", file_types=['.py'])
        
        assert result == []
    
    @patch('utils.github_processor.os.walk')
    @patch('utils.github_processor.os.stat')
    @patch('builtins.open')
    def test_discover_files_binary_filtering(self, mock_file, mock_stat, mock_walk):
        """Test filtering out binary files."""
        mock_walk.return_value = [("/repo", [], ["data.json"])]
        mock_stat.return_value = Mock(st_size=1000)
        
        # Mock binary content (contains null bytes)
        mock_file.return_value.__enter__.return_value.read.return_value = "text\x00binary"
        
        discovery = MultiFileDiscovery()
        result = discovery.discover_files("/repo", file_types=['.json'])
        
        # Should filter out binary files
        assert result == []
    
    @patch('utils.github_processor.os.walk')
    @patch('utils.github_processor.os.stat')
    def test_discover_files_size_limits(self, mock_stat, mock_walk):
        """Test file size limits by type."""
        mock_walk.return_value = [("/repo", [], ["large.py", "large.json"])]
        
        discovery = MultiFileDiscovery()
        
        # Test Python file size limit (1MB)
        mock_stat.return_value = Mock(st_size=2_000_000)  # 2MB
        result = discovery.discover_files("/repo", file_types=['.py'])
        assert result == []
        
        # Test JSON file size limit (100KB)
        mock_stat.return_value = Mock(st_size=200_000)  # 200KB
        result = discovery.discover_files("/repo", file_types=['.json'])
        assert result == []


================================================
FILE: tests/test_gpu_integration.py
================================================
"""
Integration tests for CrossEncoder GPU acceleration functionality.

Tests CrossEncoder GPU initialization, CPU fallback scenarios, memory cleanup,
and integration with the MCP server following the existing test infrastructure.
"""

import pytest
import os
from unittest.mock import Mock, patch

# Import test fixtures and setup
import sys
from pathlib import Path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

# Import the functions we're testing
from crawl4ai_mcp import rerank_results
from device_manager import get_optimal_device
from utils import health_check_gpu_acceleration


class TestCrossEncoderGPUInitialization:
    """Test CrossEncoder initialization with GPU acceleration."""
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.get_model_kwargs_for_device')
    @patch('crawl4ai_mcp.get_gpu_preference')
    @patch.dict(os.environ, {"USE_RERANKING": "true", "USE_GPU_ACCELERATION": "auto"})
    def test_crossencoder_gpu_initialization_success(self, mock_get_gpu_pref, mock_get_kwargs, mock_get_device, mock_crossencoder):
        """Test successful CrossEncoder initialization with GPU."""
        # Mock device detection
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="cuda:0")
        mock_get_device.return_value = mock_device
        mock_get_gpu_pref.return_value = "auto"
        mock_get_kwargs.return_value = {"torch_dtype": "float16"}
        
        # Mock CrossEncoder
        mock_model = Mock()
        mock_crossencoder.return_value = mock_model
        
        # Import and test initialization (would normally happen in main)
        
        # Verify CrossEncoder was called with correct parameters
        mock_crossencoder.assert_called_with(
            "cross-encoder/ms-marco-MiniLM-L-6-v2",
            device="cuda:0",
            model_kwargs={"torch_dtype": "float16"}
        )
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.get_gpu_preference')
    @patch.dict(os.environ, {"USE_RERANKING": "true", "USE_GPU_ACCELERATION": "cpu"})
    def test_crossencoder_cpu_forced_initialization(self, mock_get_gpu_pref, mock_get_device, mock_crossencoder):
        """Test CrossEncoder initialization when CPU is forced."""
        # Mock CPU device
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="cpu")
        mock_get_device.return_value = mock_device
        mock_get_gpu_pref.return_value = "cpu"
        
        # Mock CrossEncoder
        mock_model = Mock()
        mock_crossencoder.return_value = mock_model
        
        # Verify device selection was called with CPU preference
        mock_get_device.assert_called_with(preference="cpu", gpu_index=0)
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.print')  # Mock print statements
    @patch.dict(os.environ, {"USE_RERANKING": "true"})
    def test_crossencoder_initialization_failure_fallback(self, mock_print, mock_get_device, mock_crossencoder):
        """Test CrossEncoder initialization failure handling."""
        # Mock device detection success but CrossEncoder initialization failure
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="cuda:0")
        mock_get_device.return_value = mock_device
        
        # Mock CrossEncoder initialization failure
        mock_crossencoder.side_effect = RuntimeError("GPU out of memory")
        
        # This would be tested in the actual initialization code
        # Here we simulate the error handling
        try:
            model = mock_crossencoder("cross-encoder/ms-marco-MiniLM-L-6-v2", device="cuda:0")
        except Exception:
            model = None
            
        assert model is None
        mock_print.assert_called()


class TestRerankingWithGPU:
    """Test reranking functionality with GPU acceleration."""
    
    def test_rerank_results_with_gpu_model(self):
        """Test rerank_results function with GPU-enabled model."""
        # Mock CrossEncoder model
        mock_model = Mock()
        mock_model.predict.return_value = [0.9, 0.7, 0.8]  # Relevance scores
        
        # Sample results
        results = [
            {"content": "Document 1", "id": "doc1"},
            {"content": "Document 2", "id": "doc2"}, 
            {"content": "Document 3", "id": "doc3"}
        ]
        
        query = "test query"
        
        with patch('crawl4ai_mcp.cleanup_gpu_memory') as mock_cleanup:
            reranked = rerank_results(mock_model, query, results)
            
            # Verify reranking worked
            assert len(reranked) == 3
            assert reranked[0]["rerank_score"] == 0.9  # Highest score first
            assert reranked[1]["rerank_score"] == 0.8  # Second highest
            assert reranked[2]["rerank_score"] == 0.7  # Lowest
            
            # Verify model was called correctly
            expected_pairs = [["test query", "Document 1"], 
                            ["test query", "Document 2"], 
                            ["test query", "Document 3"]]
            mock_model.predict.assert_called_once_with(expected_pairs)
            
            # Verify GPU memory cleanup was called
            mock_cleanup.assert_called_once()
    
    def test_rerank_results_empty_input(self):
        """Test rerank_results with empty input."""
        mock_model = Mock()
        
        result = rerank_results(mock_model, "query", [])
        
        assert result == []
        mock_model.predict.assert_not_called()
    
    def test_rerank_results_no_model(self):
        """Test rerank_results with None model (fallback case)."""
        results = [{"content": "Document 1", "id": "doc1"}]
        
        result = rerank_results(None, "query", results)
        
        assert result == results  # Should return original results
    
    def test_rerank_results_model_error_handling(self):
        """Test rerank_results handles model prediction errors."""
        mock_model = Mock()
        mock_model.predict.side_effect = RuntimeError("Model prediction failed")
        
        results = [{"content": "Document 1", "id": "doc1"}]
        
        with patch('crawl4ai_mcp.print') as mock_print:
            result = rerank_results(mock_model, "query", results)
            
            assert result == results  # Should return original results on error
            mock_print.assert_called()  # Should log the error


class TestMemoryManagement:
    """Test GPU memory management in reranking operations."""
    
    @patch('crawl4ai_mcp.cleanup_gpu_memory')
    def test_memory_cleanup_called_after_reranking(self, mock_cleanup):
        """Test that GPU memory cleanup is called after reranking."""
        mock_model = Mock()
        mock_model.predict.return_value = [0.8]
        
        results = [{"content": "Test document", "id": "doc1"}]
        
        rerank_results(mock_model, "test query", results)
        
        mock_cleanup.assert_called_once()
    
    @patch('crawl4ai_mcp.cleanup_gpu_memory')
    def test_memory_cleanup_called_even_on_error(self, mock_cleanup):
        """Test that memory cleanup is attempted even when reranking fails."""
        mock_model = Mock()
        mock_model.predict.side_effect = RuntimeError("Prediction error")
        
        results = [{"content": "Test document", "id": "doc1"}]
        
        with patch('crawl4ai_mcp.print'):
            rerank_results(mock_model, "test query", results)
        
        # Memory cleanup should not be called in error path since it returns early
        # This is the current behavior - could be enhanced to always cleanup
        mock_cleanup.assert_not_called()


class TestDeviceHealthCheck:
    """Test GPU acceleration health check functionality."""
    
    @patch('utils.torch')
    def test_health_check_gpu_available_and_working(self, mock_torch):
        """Test health check when GPU is available and working."""
        # Mock CUDA availability and operations
        mock_torch.cuda.is_available.return_value = True
        mock_torch.cuda.get_device_name.return_value = "NVIDIA GeForce RTX 3080"
        
        mock_device_props = Mock()
        mock_device_props.total_memory = 10 * 1024**3  # 10GB
        mock_torch.cuda.get_device_properties.return_value = mock_device_props
        
        # Mock successful tensor operations
        mock_tensor = Mock()
        mock_torch.randn.return_value = mock_tensor
        mock_tensor.__matmul__ = Mock(return_value=mock_tensor)
        mock_torch.device.return_value = Mock()
        
        health_status = health_check_gpu_acceleration()
        
        assert health_status['gpu_available'] is True
        assert health_status['device_name'] == "NVIDIA GeForce RTX 3080"
        assert health_status['memory_available_gb'] == 10.0
        assert health_status['test_passed'] is True
        assert health_status['error_message'] is None
    
    @patch('utils.torch')
    def test_health_check_mps_available_and_working(self, mock_torch):
        """Test health check when MPS (Apple Silicon) is available."""
        # Mock MPS availability, no CUDA
        mock_torch.cuda.is_available.return_value = False
        mock_torch.backends.mps.is_available.return_value = True
        
        # Mock successful MPS operations
        mock_tensor = Mock()
        mock_torch.randn.return_value = mock_tensor
        mock_tensor.sum.return_value = mock_tensor
        mock_torch.device.return_value = Mock()
        
        health_status = health_check_gpu_acceleration()
        
        assert health_status['gpu_available'] is True
        assert health_status['device_name'] == "Apple Silicon GPU (MPS)"
        assert health_status['memory_available_gb'] is None  # MPS doesn't expose memory
        assert health_status['test_passed'] is True
        assert health_status['error_message'] is None
    
    @patch('utils.torch')
    def test_health_check_gpu_operations_fail(self, mock_torch):
        """Test health check when GPU is detected but operations fail."""
        # Mock CUDA available but operations fail
        mock_torch.cuda.is_available.return_value = True
        mock_torch.randn.side_effect = RuntimeError("CUDA out of memory")
        
        health_status = health_check_gpu_acceleration()
        
        assert health_status['gpu_available'] is False
        assert health_status['test_passed'] is False
        assert health_status['error_message'] == "CUDA out of memory"
    
    @patch('utils.torch')
    def test_health_check_no_gpu_available(self, mock_torch):
        """Test health check when no GPU is available."""
        # Mock no GPU availability
        mock_torch.cuda.is_available.return_value = False
        mock_torch.backends.mps.is_available.return_value = False
        
        health_status = health_check_gpu_acceleration()
        
        assert health_status['gpu_available'] is False
        assert health_status['device_name'] == "CPU"
        assert health_status['test_passed'] is False
        assert health_status['error_message'] is None


class TestEnvironmentConfiguration:
    """Test environment variable configuration for GPU acceleration."""
    
    @patch.dict(os.environ, {
        "USE_RERANKING": "true",
        "USE_GPU_ACCELERATION": "true",
        "GPU_PRECISION": "float16",
        "GPU_DEVICE_INDEX": "1"
    })
    def test_environment_configuration_parsing(self):
        """Test that environment variables are correctly parsed."""
        assert os.getenv("USE_RERANKING") == "true"
        assert os.getenv("USE_GPU_ACCELERATION") == "true"
        assert os.getenv("GPU_PRECISION") == "float16"
        assert os.getenv("GPU_DEVICE_INDEX") == "1"
    
    @patch.dict(os.environ, {"USE_RERANKING": "false"})
    def test_reranking_disabled_no_gpu_init(self):
        """Test that GPU initialization is skipped when reranking is disabled."""
        # This test would verify that CrossEncoder is not initialized
        # when USE_RERANKING is false, regardless of GPU settings
        assert os.getenv("USE_RERANKING") == "false"


class TestBackwardCompatibility:
    """Test backward compatibility with CPU-only environments."""
    
    @patch('device_manager.TORCH_AVAILABLE', True)
    @patch('device_manager.torch')
    def test_cpu_only_environment_compatibility(self, mock_torch):
        """Test that CPU-only environments continue to work."""
        # Mock no GPU availability
        mock_torch.cuda.is_available.return_value = False
        hasattr_mock = Mock(return_value=False)
        with patch('builtins.hasattr', hasattr_mock):
            
            # Mock CPU device
            cpu_device = Mock()
            cpu_device.__str__ = Mock(return_value="cpu")
            mock_torch.device.return_value = cpu_device
            
            device = get_optimal_device(preference="auto")
            assert str(device) == "cpu"
    
    @patch.dict(os.environ, {}, clear=True)  # Clear all environment variables
    def test_default_configuration_behavior(self):
        """Test behavior with default configuration (no env vars set)."""
        # Should use default values when environment variables are not set
        assert os.getenv("USE_GPU_ACCELERATION", "auto") == "auto"
        assert os.getenv("GPU_PRECISION", "float32") == "float32"
        assert os.getenv("GPU_DEVICE_INDEX", "0") == "0"


class TestRerankingIntegration:
    """Test full reranking integration with GPU acceleration."""
    
    def test_reranking_preserves_original_fields(self):
        """Test that reranking preserves all original result fields."""
        mock_model = Mock()
        mock_model.predict.return_value = [0.9, 0.7]
        
        results = [
            {
                "content": "Document 1", 
                "id": "doc1", 
                "url": "https://example.com/1",
                "metadata": {"category": "tech"}
            },
            {
                "content": "Document 2", 
                "id": "doc2", 
                "url": "https://example.com/2",
                "metadata": {"category": "science"}
            }
        ]
        
        with patch('crawl4ai_mcp.cleanup_gpu_memory'):
            reranked = rerank_results(mock_model, "query", results)
            
            # Check that all original fields are preserved
            for result in reranked:
                assert "content" in result
                assert "id" in result
                assert "url" in result
                assert "metadata" in result
                assert "rerank_score" in result  # New field added
    
    def test_reranking_custom_content_key(self):
        """Test reranking with custom content key."""
        mock_model = Mock()
        mock_model.predict.return_value = [0.8]
        
        results = [{"text": "Custom content field", "id": "doc1"}]
        
        with patch('crawl4ai_mcp.cleanup_gpu_memory'):
            reranked = rerank_results(mock_model, "query", results, content_key="text")
            
            # Verify correct content was used for reranking
            expected_pairs = [["query", "Custom content field"]]
            mock_model.predict.assert_called_once_with(expected_pairs)


if __name__ == "__main__":
    pytest.main([__file__])


================================================
FILE: tests/test_integration_docker.py
================================================
"""
Integration tests with real Docker services.

Tests the actual integration with Qdrant and Neo4j running in Docker containers.
"""
import pytest
import requests
import time
import os
import sys
import uuid
from pathlib import Path

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))


def is_docker_service_ready(url: str, max_retries: int = 10) -> bool:
    """Check if a Docker service is ready by polling its endpoint."""
    for i in range(max_retries):
        try:
            response = requests.get(url, timeout=2)
            if response.status_code == 200:
                return True
        except requests.exceptions.RequestException:
            pass
        time.sleep(1)
    return False


@pytest.fixture(scope="module")
def docker_services():
    """Ensure Docker services are running and healthy."""
    # Check if Qdrant is ready (use collections endpoint)
    qdrant_ready = is_docker_service_ready("http://localhost:6333/collections")
    
    # Neo4j takes longer to start, so we're more lenient
    neo4j_ready = is_docker_service_ready("http://localhost:7474", max_retries=20)
    
    return {
        "qdrant_ready": qdrant_ready,
        "neo4j_ready": neo4j_ready
    }


class TestQdrantIntegration:
    """Test integration with real Qdrant service."""

    def test_qdrant_health(self, docker_services):
        """Test Qdrant root endpoint."""
        if not docker_services["qdrant_ready"]:
            pytest.skip("Qdrant not ready")
        
        response = requests.get("http://localhost:6333/")
        assert response.status_code == 200
        data = response.json()
        assert "title" in data
        assert "qdrant" in data["title"].lower()

    def test_qdrant_collections_endpoint(self, docker_services):
        """Test Qdrant collections endpoint."""
        if not docker_services["qdrant_ready"]:
            pytest.skip("Qdrant not ready")
        
        response = requests.get("http://localhost:6333/collections")
        assert response.status_code == 200
        data = response.json()
        assert "result" in data

    def test_qdrant_client_wrapper_real_connection(self, docker_services):
        """Test QdrantClientWrapper with real Qdrant service."""
        if not docker_services["qdrant_ready"]:
            pytest.skip("Qdrant not ready")
        
        from qdrant_wrapper import QdrantClientWrapper
        
        # Test connection
        try:
            client = QdrantClientWrapper(host="localhost", port=6333)
            
            # Test health check
            health = client.health_check()
            assert health["status"] == "healthy"
            assert "collections" in health
            
            # Test collections were created
            collections = health["collections"]
            expected_collections = ["crawled_pages", "code_examples"]
            
            for collection_name in expected_collections:
                if collection_name in collections:
                    collection_info = collections[collection_name]
                    assert "status" in collection_info
                    assert "config" in collection_info
                    assert collection_info["config"]["size"] == 1536
            
        except Exception as e:
            pytest.fail(f"QdrantClientWrapper connection failed: {e}")

    def test_qdrant_basic_operations(self, docker_services):
        """Test basic Qdrant operations with real service."""
        if not docker_services["qdrant_ready"]:
            pytest.skip("Qdrant not ready")
        
        from qdrant_wrapper import QdrantClientWrapper
        from qdrant_client.models import PointStruct
        
        try:
            client = QdrantClientWrapper(host="localhost", port=6333)
            
            # Test adding a test point with UUID
            test_id = str(uuid.uuid4())
            test_point = PointStruct(
                id=test_id,
                vector=[0.1] * 1536,  # Mock embedding
                payload={
                    "url": "https://test.com",
                    "content": "Test content for integration",
                    "chunk_number": 1,
                    "source_id": "test.com"
                }
            )
            
            # Insert test point
            client.upsert_points("crawled_pages", [test_point])
            
            # Wait a moment for indexing
            time.sleep(1)
            
            # Test search
            results = client.search_documents(
                query_embedding=[0.1] * 1536,
                match_count=1
            )
            
            # Verify we can search (even if no exact matches)
            assert isinstance(results, list)
            
            # Test keyword search
            keyword_results = client.keyword_search_documents(
                query="Test content",
                match_count=5
            )
            
            assert isinstance(keyword_results, list)
            
            # Clean up test point (optional)
            try:
                client.client.delete(
                    collection_name="crawled_pages",
                    points_selector=[test_id]
                )
            except:
                pass  # Clean up failure is not critical
            
        except Exception as e:
            pytest.fail(f"Qdrant operations failed: {e}")


class TestUtilsWithRealQdrant:
    """Test utils functions with real Qdrant service."""

    def test_get_qdrant_client_real(self, docker_services):
        """Test getting real Qdrant client."""
        if not docker_services["qdrant_ready"]:
            pytest.skip("Qdrant not ready")
        
        from qdrant_wrapper import get_qdrant_client
        
        try:
            client = get_qdrant_client()
            assert client is not None
            
            # Test health check
            health = client.health_check()
            assert health["status"] == "healthy"
            
        except Exception as e:
            pytest.fail(f"get_qdrant_client failed: {e}")

    @pytest.mark.skipif(not os.getenv("EMBEDDINGS_API_KEY"), reason="No embeddings API key")
    def test_embedding_integration_real(self, docker_services):
        """Test embedding creation and storage (requires OpenAI API key)."""
        if not docker_services["qdrant_ready"]:
            pytest.skip("Qdrant not ready")
        
        from utils import create_embedding, get_supabase_client
        from qdrant_client.models import PointStruct
        
        try:
            # Get client
            client = get_supabase_client()  # Returns Qdrant client
            
            # Create test embedding (this will use real OpenAI API if key is available)
            test_text = "This is a test for integration testing"
            embedding = create_embedding(test_text)
            
            # Verify embedding structure
            assert isinstance(embedding, list)
            assert len(embedding) == 1536
            
            # Test storing embedding
            test_point = PointStruct(
                id="integration-test-embedding",
                vector=embedding,
                payload={
                    "url": "https://integration-test.com",
                    "content": test_text,
                    "chunk_number": 1,
                    "source_id": "integration-test.com"
                }
            )
            
            client.upsert_points("crawled_pages", [test_point])
            
            # Wait for indexing
            time.sleep(1)
            
            # Test search with the same embedding
            results = client.search_documents(
                query_embedding=embedding,
                match_count=1
            )
            
            assert isinstance(results, list)
            if results:
                # If we found results, verify structure
                result = results[0]
                assert "id" in result
                assert "similarity" in result
                assert "content" in result
            
            # Clean up
            try:
                client.client.delete(
                    collection_name="crawled_pages",
                    points_selector=["integration-test-embedding"]
                )
            except:
                pass
            
        except Exception as e:
            pytest.fail(f"Embedding integration test failed: {e}")


class TestNeo4jIntegration:
    """Test Neo4j integration (when available)."""

    def test_neo4j_availability(self, docker_services):
        """Test if Neo4j is accessible."""
        if not docker_services["neo4j_ready"]:
            pytest.skip("Neo4j not ready - this is optional for basic RAG functionality")
        
        # Try to connect to Neo4j browser interface
        try:
            response = requests.get("http://localhost:7474", timeout=5)
            # Neo4j returns HTML, so we just check it responds
            assert response.status_code == 200
        except requests.exceptions.RequestException:
            pytest.skip("Neo4j not accessible - this is optional")

    def test_neo4j_bolt_port(self, docker_services):
        """Test if Neo4j Bolt port is accessible."""
        if not docker_services["neo4j_ready"]:
            pytest.skip("Neo4j not ready")
        
        import socket
        
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2)
            result = sock.connect_ex(('localhost', 7687))
            sock.close()
            
            # Port should be open (connect_ex returns 0 on success)
            assert result == 0, "Neo4j Bolt port 7687 should be accessible"
            
        except Exception as e:
            pytest.skip(f"Neo4j port test failed: {e}")


class TestEndToEndWorkflow:
    """Test complete end-to-end workflow with Docker services."""

    @pytest.mark.skipif(not os.getenv("EMBEDDINGS_API_KEY"), reason="No embeddings API key")
    def test_complete_rag_workflow(self, docker_services):
        """Test complete RAG workflow from storage to retrieval."""
        if not docker_services["qdrant_ready"]:
            pytest.skip("Qdrant not ready")
        
        from utils import (
            get_supabase_client, add_documents_to_supabase, search_documents
        )
        
        try:
            # Get client
            client = get_supabase_client()
            
            # Test data
            test_urls = ["https://test-integration.com/page1"]
            test_contents = ["This is integration test content about Python programming and machine learning."]
            test_chunk_numbers = [1]
            test_metadatas = [{"category": "integration_test", "language": "en"}]
            test_url_to_full_doc = {
                "https://test-integration.com/page1": "Full document content for integration testing"
            }
            
            # Store documents
            add_documents_to_supabase(
                client=client,
                urls=test_urls,
                chunk_numbers=test_chunk_numbers,
                contents=test_contents,
                metadatas=test_metadatas,
                url_to_full_document=test_url_to_full_doc
            )
            
            # Wait for indexing
            time.sleep(2)
            
            # Search for documents
            search_results = search_documents(
                client=client,
                query="Python programming",
                match_count=5
            )
            
            # Verify search results
            assert isinstance(search_results, list)
            
            # Clean up test data
            try:
                # Try to remove test documents (cleanup)
                # This is optional as test data won't interfere with normal operation
                pass
            except:
                pass
            
        except Exception as e:
            pytest.fail(f"End-to-end workflow test failed: {e}")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
FILE: tests/test_mcp_basic.py
================================================
"""
Basic tests for MCP server functions.

Simple tests to validate core MCP tool functionality after Qdrant migration.
"""
import pytest
import os
import sys
from pathlib import Path
from unittest.mock import Mock, patch

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

# Setup test environment
# Modern configuration
os.environ.setdefault("CHAT_MODEL", "gpt-3.5-turbo")
os.environ.setdefault("CHAT_API_KEY", "test-chat-api-key")
os.environ.setdefault("CHAT_API_BASE", "https://api.openai.com/v1")
os.environ.setdefault("EMBEDDINGS_MODEL", "text-embedding-3-small")
os.environ.setdefault("EMBEDDINGS_API_KEY", "test-embeddings-api-key")
os.environ.setdefault("EMBEDDINGS_API_BASE", "https://api.openai.com/v1")

# Other configuration
os.environ.setdefault("QDRANT_HOST", "localhost")
os.environ.setdefault("QDRANT_PORT", "6333")


class TestMCPBasicFunctionality:
    """Test basic MCP server functionality."""

    @patch('crawl4ai_mcp.get_supabase_client')
    def test_context_dataclass(self, mock_get_client):
        """Test that context dataclass uses Qdrant client."""
        mock_client = Mock()
        mock_get_client.return_value = mock_client
        
        # Import after mocking
        from crawl4ai_mcp import Crawl4AIContext
        
        # Test
        context = Crawl4AIContext(
            crawler=Mock(),
            qdrant_client=mock_client
        )
        
        # Verify
        assert context.qdrant_client == mock_client
        assert hasattr(context, 'qdrant_client')
        assert hasattr(context, 'crawler')

    @patch('crawl4ai_mcp.search_documents')
    def test_perform_rag_query_basic(self, mock_search):
        """Test basic RAG query functionality."""
        # Setup mock
        mock_search.return_value = [
            {
                "id": "doc1", 
                "similarity": 0.9,
                "content": "Python programming content",
                "url": "https://python.org"
            }
        ]
        
        from crawl4ai_mcp import perform_rag_query
        
        # Create mock context
        mock_ctx = Mock()
        mock_ctx.deps = Mock()
        mock_ctx.deps.qdrant_client = Mock()
        
        # Test (this is an async function)
        # Note: Testing the basic call structure, actual async execution would need more setup
        assert callable(perform_rag_query)

    def test_import_structure(self):
        """Test that all required imports work after migration."""
        # Test importing main modules
        from crawl4ai_mcp import Crawl4AIContext
        from qdrant_wrapper import QdrantClientWrapper, get_qdrant_client
        from utils import search_documents, get_supabase_client
        
        # Verify classes exist
        assert Crawl4AIContext is not None
        assert QdrantClientWrapper is not None
        assert callable(get_qdrant_client)
        assert callable(search_documents)
        assert callable(get_supabase_client)

    @patch('utils.get_qdrant_client')
    def test_legacy_compatibility(self, mock_get_qdrant):
        """Test that legacy Supabase function names still work."""
        mock_client = Mock()
        mock_get_qdrant.return_value = mock_client
        
        from utils import get_supabase_client
        
        # Test legacy function returns Qdrant client
        client = get_supabase_client()
        assert client == mock_client

    def test_qdrant_wrapper_interface(self):
        """Test that Qdrant wrapper has expected interface."""
        from qdrant_wrapper import QdrantClientWrapper
        
        # Check that class has expected methods (without calling them)
        expected_methods = [
            'search_documents',
            'search_code_examples', 
            'keyword_search_documents',
            'keyword_search_code_examples',
            'health_check',
            'get_available_sources',
            'update_source_info',
            'generate_point_id',
            'normalize_search_results'
        ]
        
        for method_name in expected_methods:
            assert hasattr(QdrantClientWrapper, method_name)
            assert callable(getattr(QdrantClientWrapper, method_name))

    @patch('qdrant_wrapper.QdrantClient')
    def test_collections_configuration(self, mock_qdrant_client):
        """Test that collections are properly configured."""
        from qdrant_wrapper import COLLECTIONS
        
        # Verify collection configurations exist
        assert "crawled_pages" in COLLECTIONS
        assert "code_examples" in COLLECTIONS
        
        # Verify collection structure
        for name, config in COLLECTIONS.items():
            assert "vectors_config" in config
            assert "payload_schema" in config
            
            # Check vector configuration
            vector_config = config["vectors_config"]
            assert hasattr(vector_config, 'size')
            assert hasattr(vector_config, 'distance')
            assert config["vectors_config"].size == 1536  # OpenAI embedding size

    def test_environment_configuration(self):
        """Test that environment variables are properly configured."""
        # Test that required environment variables can be accessed
        expected_vars = [
            "QDRANT_HOST",
            "QDRANT_PORT", 
            # Modern configuration variables
            "CHAT_MODEL",
            "CHAT_API_KEY", 
            "EMBEDDINGS_MODEL",
            "EMBEDDINGS_API_KEY"
        ]
        
        for var in expected_vars:
            # Just check that the variables are set in our test environment
            assert os.environ.get(var) is not None


class TestSearchFunctionality:
    """Test search functionality with mocked Qdrant."""

    @patch('utils.create_embedding')
    @patch('utils.QdrantClientWrapper')
    def test_document_search_workflow(self, mock_wrapper_class, mock_create_embedding):
        """Test complete document search workflow."""
        # Setup mocks
        mock_client = Mock()
        mock_client.search_documents.return_value = [
            {"id": "doc1", "similarity": 0.9, "content": "test content"}
        ]
        mock_wrapper_class.return_value = mock_client
        mock_create_embedding.return_value = [0.1] * 1536
        
        from utils import search_documents
        
        # Test
        results = search_documents(mock_client, "test query")
        
        # Verify workflow
        mock_create_embedding.assert_called_once_with("test query")
        mock_client.search_documents.assert_called_once()
        assert len(results) == 1
        assert results[0]["id"] == "doc1"

    @patch('utils.create_embedding')
    @patch('utils.QdrantClientWrapper')
    def test_code_search_workflow(self, mock_wrapper_class, mock_create_embedding):
        """Test complete code search workflow."""
        # Setup mocks
        mock_client = Mock()
        mock_client.search_code_examples.return_value = [
            {"id": "code1", "similarity": 0.85, "content": "def test(): pass"}
        ]
        mock_wrapper_class.return_value = mock_client
        mock_create_embedding.return_value = [0.1] * 1536
        
        from utils import search_code_examples
        
        # Test
        results = search_code_examples(mock_client, "test function")
        
        # Verify workflow
        mock_create_embedding.assert_called_once()
        mock_client.search_code_examples.assert_called_once()
        assert len(results) == 1
        assert results[0]["id"] == "code1"


if __name__ == "__main__":
    pytest.main([__file__])


================================================
FILE: tests/test_mcp_server.py
================================================
"""
Integration tests for the MCP server with Qdrant migration.

Tests the main MCP server functionality after Supabase to Qdrant migration.
"""
import pytest
import os
import sys
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

# Mock environment variables before importing
# Modern configuration
os.environ.setdefault("CHAT_MODEL", "gpt-3.5-turbo")
os.environ.setdefault("CHAT_API_KEY", "test-chat-api-key")
os.environ.setdefault("CHAT_API_BASE", "https://api.openai.com/v1")
os.environ.setdefault("EMBEDDINGS_MODEL", "text-embedding-3-small")
os.environ.setdefault("EMBEDDINGS_API_KEY", "test-embeddings-api-key")
os.environ.setdefault("EMBEDDINGS_API_BASE", "https://api.openai.com/v1")

# Other configuration
os.environ.setdefault("QDRANT_HOST", "localhost")
os.environ.setdefault("QDRANT_PORT", "6333")

# Mock external dependencies
sys.modules['crawl4ai'] = Mock()
sys.modules['sentence_transformers'] = Mock()

# Mock knowledge graph modules
mock_kg_validator = Mock()
mock_neo4j_extractor = Mock()
mock_ai_analyzer = Mock()
mock_hallucination_reporter = Mock()

sys.modules['knowledge_graph_validator'] = mock_kg_validator
sys.modules['parse_repo_into_neo4j'] = mock_neo4j_extractor
sys.modules['ai_script_analyzer'] = mock_ai_analyzer
sys.modules['hallucination_reporter'] = mock_hallucination_reporter

# Add mock classes
mock_kg_validator.KnowledgeGraphValidator = Mock
mock_neo4j_extractor.DirectNeo4jExtractor = Mock
mock_ai_analyzer.AIScriptAnalyzer = Mock
mock_hallucination_reporter.HallucinationReporter = Mock


class TestMCPServerIntegration:
    """Test MCP server integration with Qdrant."""

    @patch('crawl4ai_mcp.get_supabase_client')
    def test_context_initialization(self, mock_get_client):
        """Test that context initializes with Qdrant client."""
        # Setup mock
        mock_client = Mock()
        mock_get_client.return_value = mock_client
        
        # Import after mocking
        from crawl4ai_mcp import MCPContext
        
        # Test
        context = MCPContext(qdrant_client=mock_client)
        
        # Verify
        assert context.qdrant_client == mock_client
        assert hasattr(context, 'qdrant_client')

    @patch('crawl4ai_mcp.search_documents')
    @patch('crawl4ai_mcp.get_supabase_client')
    def test_perform_rag_query_integration(self, mock_get_client, mock_search):
        """Test RAG query with Qdrant integration."""
        # Setup mocks
        mock_client = Mock()
        mock_get_client.return_value = mock_client
        
        mock_search.return_value = [
            {
                "id": "doc1",
                "similarity": 0.9,
                "content": "Python is a programming language",
                "url": "https://python.org",
                "chunk_number": 1
            }
        ]
        
        # Import the function
        from crawl4ai_mcp import perform_rag_query
        
        context = Mock()
        context.qdrant_client = mock_client
        
        # Test
        result = perform_rag_query(
            query="What is Python?",
            context=context,
            match_count=5,
            filter_metadata=None,
            source_filter=None
        )
        
        # Verify
        assert "results" in result
        assert len(result["results"]) == 1
        assert result["results"][0]["similarity"] == 0.9
        mock_search.assert_called_once_with(
            mock_client, "What is Python?", 5, None
        )

    @patch('crawl4ai_mcp.search_code_examples')
    @patch('crawl4ai_mcp.get_supabase_client')
    def test_search_code_examples_integration(self, mock_get_client, mock_search):
        """Test code examples search with Qdrant integration."""
        # Setup mocks
        mock_client = Mock()
        mock_get_client.return_value = mock_client
        
        mock_search.return_value = [
            {
                "id": "code1",
                "similarity": 0.85,
                "content": "def hello_world():\n    print('Hello, World!')",
                "summary": "Basic Python function example",
                "url": "https://example.com/python-basics"
            }
        ]
        
        # Import the function
        from crawl4ai_mcp import search_code_examples as mcp_search_code
        
        context = Mock()
        context.qdrant_client = mock_client
        
        # Test
        result = mcp_search_code(
            query="hello world function",
            context=context,
            match_count=3,
            filter_metadata=None,
            source_id=None
        )
        
        # Verify
        assert "results" in result
        assert len(result["results"]) == 1
        assert "def hello_world" in result["results"][0]["content"]
        mock_search.assert_called_once_with(
            mock_client, "hello world function", 3, None, None
        )

    @patch('crawl4ai_mcp.get_supabase_client')
    def test_get_available_sources_integration(self, mock_get_client):
        """Test getting available sources from Qdrant."""
        # Setup mock
        mock_client = Mock()
        mock_client.get_available_sources.return_value = [
            {
                "source_id": "python.org",
                "summary": "Official Python documentation",
                "total_word_count": 50000,
                "updated_at": "2024-01-01T00:00:00"
            }
        ]
        mock_get_client.return_value = mock_client
        
        # Import the function
        from crawl4ai_mcp import get_available_sources
        
        context = Mock()
        context.qdrant_client = mock_client
        
        # Test
        result = get_available_sources(context=context)
        
        # Verify
        assert "sources" in result
        assert len(result["sources"]) == 1
        assert result["sources"][0]["source_id"] == "python.org"
        mock_client.get_available_sources.assert_called_once()

    @patch('crawl4ai_mcp.AsyncWebCrawler')
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    @patch('crawl4ai_mcp.get_supabase_client')
    async def test_crawl_integration_mock(self, mock_get_client, mock_add_docs, mock_crawler_class):
        """Test crawling integration with Qdrant (mocked)."""
        # Setup mocks
        mock_client = Mock()
        mock_get_client.return_value = mock_client
        
        mock_add_docs.return_value = None
        
        # Mock crawler
        mock_crawler = AsyncMock()
        mock_result = Mock()
        mock_result.success = True
        mock_result.markdown = "# Test Page\n\nThis is test content."
        mock_result.extracted_content = "This is test content."
        mock_crawler.arun.return_value = mock_result
        mock_crawler.__aenter__.return_value = mock_crawler
        mock_crawler.__aexit__.return_value = None
        mock_crawler_class.return_value = mock_crawler
        
        # Import the function
        from crawl4ai_mcp import crawl_single_page
        
        context = Mock()
        context.qdrant_client = mock_client
        
        # Test
        result = await crawl_single_page(
            url="https://example.com",
            context=context,
            chunk_size=2000,
            overlap_size=100
        )
        
        # Verify
        assert result["success"] is True
        assert "content_preview" in result
        mock_add_docs.assert_called_once()


class TestHybridSearch:
    """Test hybrid search functionality with Qdrant."""

    @patch('crawl4ai_mcp.search_documents')
    @patch('crawl4ai_mcp.get_supabase_client')
    def test_hybrid_search_vector_only(self, mock_get_client, mock_search_docs):
        """Test hybrid search with only vector search."""
        # Setup mocks
        mock_client = Mock()
        mock_client.keyword_search_documents.return_value = []  # No keyword results
        mock_get_client.return_value = mock_client
        
        mock_search_docs.return_value = [
            {"id": "doc1", "similarity": 0.9, "content": "vector result"}
        ]
        
        # Import the function
        from crawl4ai_mcp import perform_rag_query
        
        context = Mock()
        context.qdrant_client = mock_client
        
        # Test
        result = perform_rag_query(
            query="test query",
            context=context,
            match_count=5,
            filter_metadata=None,
            source_filter=None
        )
        
        # Verify vector search was called
        mock_search_docs.assert_called_once()
        assert len(result["results"]) == 1

    @patch('crawl4ai_mcp.search_documents')  
    @patch('crawl4ai_mcp.get_supabase_client')
    def test_hybrid_search_with_keyword_results(self, mock_get_client, mock_search_docs):
        """Test hybrid search combining vector and keyword results."""
        # Setup mocks
        mock_client = Mock()
        mock_client.keyword_search_documents.return_value = [
            {"id": "doc2", "similarity": 0.5, "content": "keyword result"}
        ]
        mock_get_client.return_value = mock_client
        
        mock_search_docs.return_value = [
            {"id": "doc1", "similarity": 0.9, "content": "vector result"}
        ]
        
        # Import the function
        from crawl4ai_mcp import perform_rag_query
        
        context = Mock()
        context.qdrant_client = mock_client
        
        # Test
        result = perform_rag_query(
            query="test query",
            context=context,
            match_count=5,
            filter_metadata=None,
            source_filter=None
        )
        
        # Verify both searches were called
        mock_search_docs.assert_called_once()
        mock_client.keyword_search_documents.assert_called_once()
        
        # Should have results from both searches
        assert len(result["results"]) == 2


class TestErrorHandling:
    """Test error handling in MCP server."""

    @patch('crawl4ai_mcp.get_supabase_client')
    def test_qdrant_connection_error(self, mock_get_client):
        """Test handling of Qdrant connection errors."""
        # Setup mock to raise connection error
        mock_get_client.side_effect = Exception("Cannot connect to Qdrant")
        
        # Import after setting up mock
        from crawl4ai_mcp import get_available_sources
        
        context = Mock()
        
        # Test - should handle error gracefully
        try:
            result = get_available_sources(context=context)
            # If no exception, should return error result
            assert "error" in result or "sources" in result
        except Exception as e:
            # Connection errors are expected and should be handled
            assert "Qdrant" in str(e)

    @patch('crawl4ai_mcp.search_documents')
    @patch('crawl4ai_mcp.get_supabase_client')
    def test_search_error_handling(self, mock_get_client, mock_search):
        """Test error handling in search operations."""
        # Setup mocks
        mock_client = Mock()
        mock_get_client.return_value = mock_client
        
        # Mock search to raise error
        mock_search.side_effect = Exception("Search failed")
        
        from crawl4ai_mcp import perform_rag_query
        
        context = Mock()
        context.qdrant_client = mock_client
        
        # Test - should handle error gracefully
        result = perform_rag_query(
            query="test",
            context=context,
            match_count=5
        )
        
        # Should return some result structure even on error
        assert isinstance(result, dict)


if __name__ == "__main__":
    pytest.main([__file__])


================================================
FILE: tests/test_performance_validation.py
================================================
"""
Performance validation tests for multi-file processing.

Tests processing speed, memory usage, and error handling performance.
"""
import pytest
import time
import tempfile
import os
from pathlib import Path
import sys

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from utils.github_processor import (
    PythonProcessor, TypeScriptProcessor, ConfigProcessor, 
    MarkdownProcessor, MultiFileDiscovery
)


class TestPerformanceValidation:
    """Performance validation tests."""
    
    def test_python_processor_performance(self):
        """Test Python processor performance with realistic file."""
        processor = PythonProcessor()
        
        # Create a moderately sized Python file with multiple docstrings
        base_content = '''"""
Large Python module for performance testing.

This module contains multiple classes and functions to test
the performance of AST-based docstring extraction.
"""
import os
import sys
from typing import List, Dict, Any, Optional, Union

'''
        
        # Generate multiple classes and functions
        class_and_function_template = '''
class TestClass{i}:
    """
    Test class number {i}.
    
    This class demonstrates performance with multiple methods
    and comprehensive docstrings for testing purposes.
    """
    
    def __init__(self, name: str, value: int = 0):
        """Initialize the test class with name and value."""
        self.name = name
        self.value = value
    
    def process_data(self, data: List[str]) -> Dict[str, Any]:
        """
        Process a list of strings and return statistics.
        
        Args:
            data: List of strings to process
            
        Returns:
            Dictionary containing processing statistics
            
        Raises:
            ValueError: When data is empty or invalid
        """
        if not data:
            raise ValueError("Data cannot be empty")
        
        return {{
            "count": len(data),
            "total_length": sum(len(s) for s in data),
            "average_length": sum(len(s) for s in data) / len(data)
        }}
    
    async def async_method(self, items: List[Any]) -> Optional[Dict[str, Union[str, int]]]:
        """
        Asynchronously process items with complex type annotations.
        
        This method demonstrates complex type annotations and async processing
        for performance testing of the AST parser.
        
        Args:
            items: List of items to process asynchronously
            
        Returns:
            Optional dictionary with processed results, or None if no items
        """
        if not items:
            return None
        
        # Simulate async processing
        processed = []
        for item in items:
            processed.append(str(item))
        
        return {{
            "processed_count": len(processed),
            "first_item": processed[0] if processed else None,
            "status": "completed"
        }}

def utility_function_{i}(param1: str, param2: int = 42) -> bool:
    """
    Utility function number {i} for testing performance.
    
    Args:
        param1: String parameter for processing
        param2: Integer parameter with default value
        
    Returns:
        Boolean result of processing
    """
    return len(param1) > param2

'''
        
        # Create content with 10 classes and functions
        python_content = base_content
        for i in range(10):
            python_content += class_and_function_template.format(i=i)
        
        # Write to temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(python_content)
            temp_path = f.name
        
        try:
            # Measure processing time
            start_time = time.time()
            result = processor.process_file(temp_path, "test_performance.py")
            end_time = time.time()
            
            processing_time = end_time - start_time
            
            # Validate results
            assert len(result) > 0
            assert processing_time < 2.0  # Should process within 2 seconds
            
            # Check that we extracted docstrings from multiple sources
            doc_types = {item['type'] for item in result}
            assert 'module' in doc_types
            assert 'class' in doc_types
            assert 'function' in doc_types
            
            print(f"Python processing: {len(result)} items in {processing_time:.3f}s")
            
        finally:
            os.unlink(temp_path)
    
    def test_typescript_processor_performance(self):
        """Test TypeScript processor performance with realistic file."""
        processor = TypeScriptProcessor()
        
        # Create a TypeScript file with multiple JSDoc comments
        base_typescript_content = '''
/**
 * Performance test TypeScript module.
 * @module PerformanceTest
 */

'''
        
        typescript_template = '''
/**
 * Interface definition for test data {i}.
 * @interface TestData{i}
 */
export interface TestData{i} {{
    id: number;
    name: string;
    value: string;
    metadata: Record<string, any>;
}}

/**
 * Service class for handling test operations {i}.
 * @class TestService{i}
 */
export class TestService{i} {{
    private data: TestData{i}[] = [];
    
    /**
     * Retrieves data by ID for service {i}.
     * @param id - The unique identifier
     * @returns The data object if found
     * @example
     * ```typescript
     * const service = new TestService{i}();
     * const data = service.getData(123);
     * ```
     */
    getData(id: number): TestData{i} | undefined {{
        return this.data.find(item => item.id === id);
    }}
    
    /**
     * Processes multiple data items asynchronously.
     * @param items - Array of data items to process
     * @returns Promise resolving to processed results
     * @throws {{ValidationError}} When items are invalid
     */
    async processItems(items: TestData{i}[]): Promise<ProcessedResult[]> {{
        const results: ProcessedResult[] = [];
        
        for (const item of items) {{
            const processed = await this.processItem(item);
            results.push(processed);
        }}
        
        return results;
    }}
}}

/**
 * Utility function for data validation {i}.
 * @param data - Data to validate
 * @returns True if valid, false otherwise
 */
export function validateData{i}(data: TestData{i}): boolean {{
    return data && data.id > 0 && data.name.length > 0;
}}

'''
        
        # Create content with 5 sets of interfaces, classes, and functions
        typescript_content = base_typescript_content
        for i in range(5):
            typescript_content += typescript_template.format(i=i)
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.ts', delete=False) as f:
            f.write(typescript_content)
            temp_path = f.name
        
        try:
            start_time = time.time()
            result = processor.process_file(temp_path, "test_performance.ts")
            end_time = time.time()
            
            processing_time = end_time - start_time
            
            assert processing_time < 3.0  # Should process within 3 seconds
            print(f"TypeScript processing: {len(result)} items in {processing_time:.3f}s")
            
        finally:
            os.unlink(temp_path)
    
    def test_multifile_discovery_performance(self):
        """Test MultiFileDiscovery performance with many files."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create multiple directories and files
            file_count = 0
            
            # Create directory structure
            dirs = ['src', 'docs', 'config', 'tests', 'examples']
            for dir_name in dirs:
                os.makedirs(os.path.join(temp_dir, dir_name))
            
            # Create files of different types
            file_types = {
                '.md': 'markdown content with sufficient length for testing performance of file discovery and processing',
                '.py': '"""Python file for testing."""\ndef test(): pass',
                '.ts': '// TypeScript file\ninterface Test { id: number; }',
                '.json': '{"name": "test", "version": "1.0.0"}',
                '.yaml': 'name: test\nversion: 1.0.0\ndescription: test file'
            }
            
            # Create 20 files of each type across directories
            for dir_name in dirs:
                for ext, content in file_types.items():
                    for i in range(4):  # 4 files per type per directory
                        filename = f"test_{i}{ext}"
                        filepath = os.path.join(temp_dir, dir_name, filename)
                        with open(filepath, 'w') as f:
                            f.write(content)
                        file_count += 1
            
            # Test discovery performance
            discovery = MultiFileDiscovery()
            
            start_time = time.time()
            result = discovery.discover_files(
                temp_dir, 
                file_types=['.md', '.py', '.ts', '.json', '.yaml'],
                max_files=100
            )
            end_time = time.time()
            
            processing_time = end_time - start_time
            
            assert len(result) > 0
            assert processing_time < 5.0  # Should discover files within 5 seconds
            
            print(f"File discovery: {len(result)} files from {file_count} total in {processing_time:.3f}s")
    
    def test_error_handling_performance(self):
        """Test performance when handling files with errors."""
        processors = [
            PythonProcessor(),
            TypeScriptProcessor(),
            ConfigProcessor(),
            MarkdownProcessor()
        ]
        
        # Create problematic files
        problematic_files = {
            'syntax_error.py': 'def broken(\n  # Missing closing parenthesis',
            'large_file.py': 'x = 1\n' * 100000,  # Very large file
            'binary.json': '\x00\x01\x02\x03\x04',  # Binary content
            'empty.md': '',  # Empty file
            'minified.ts': 'function test(){return true;}' + 'x' * 1000  # Minified-like
        }
        
        temp_files = []
        
        try:
            # Create temporary files
            for filename, content in problematic_files.items():
                with tempfile.NamedTemporaryFile(mode='w', suffix=os.path.splitext(filename)[1], delete=False) as f:
                    if filename == 'binary.json':
                        f.close()  # Close first for binary write
                        with open(f.name, 'wb') as bf:
                            bf.write(content.encode('latin1'))
                    else:
                        f.write(content)
                    temp_files.append(f.name)
            
            # Test error handling performance
            start_time = time.time()
            total_results = 0
            
            for temp_file in temp_files:
                for processor in processors:
                    try:
                        result = processor.process_file(temp_file, os.path.basename(temp_file))
                        total_results += len(result)
                    except Exception:
                        # Errors should be handled gracefully
                        pass
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Should handle errors quickly without hanging
            assert processing_time < 2.0
            
            print(f"Error handling: {len(temp_files)} problematic files processed in {processing_time:.3f}s")
            
        finally:
            # Cleanup
            for temp_file in temp_files:
                try:
                    os.unlink(temp_file)
                except OSError:
                    pass
    
    def test_memory_efficiency_estimate(self):
        """Test memory efficiency with moderately sized content."""
        # This is a basic test since we can't easily measure memory in unit tests
        processor = PythonProcessor()
        
        # Create content that would use significant memory if not handled efficiently
        large_docstring = '"""\\n' + 'This is a test line for memory efficiency.\\n' * 1000 + '"""'
        
        python_content = f'''
{large_docstring}

def test_function():
    {large_docstring}
    pass

class TestClass:
    {large_docstring}
    
    def method(self):
        {large_docstring}
        pass
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(python_content)
            temp_path = f.name
        
        try:
            # Process large content
            result = processor.process_file(temp_path, "large_test.py")
            
            # Should handle large content without errors
            assert len(result) > 0
            
            # Verify content was processed (not just truncated)
            total_content_length = sum(len(item['content']) for item in result)
            assert total_content_length > 1000  # Should have substantial content
            
            print(f"Memory efficiency: Processed {total_content_length} characters in {len(result)} items")
            
        finally:
            os.unlink(temp_path)


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])  # -s to show print statements


================================================
FILE: tests/test_qdrant_optimization.py
================================================
"""
Tests for Qdrant client optimizations and connection management.

Tests the singleton pattern implementation and collection verification caching
to ensure unnecessary reconnections and schema checks are avoided.
"""
import pytest
import os
import time
from unittest.mock import Mock, patch
from pathlib import Path
import sys

# Add src to path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))


class TestQdrantOptimization:
    """Test Qdrant client optimization features."""

    def setup_method(self):
        """Reset singleton state before each test."""
        # Import and reset singleton state
        from src.qdrant_wrapper import QdrantClientWrapper
        # Reset global state
        import src.qdrant_wrapper as qw
        qw._qdrant_client_instance = None
        QdrantClientWrapper._collections_verified = False

    def teardown_method(self):
        """Clean up after each test."""
        # Reset global state
        import src.qdrant_wrapper as qw
        qw._qdrant_client_instance = None
        from src.qdrant_wrapper import QdrantClientWrapper
        QdrantClientWrapper._collections_verified = False

    @patch('src.qdrant_wrapper.QdrantClient')
    def test_singleton_pattern_reuse(self, mock_qdrant_client):
        """Test that the singleton pattern reuses existing client instances."""
        # Mock successful Qdrant client
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        from src.qdrant_wrapper import get_qdrant_client
        
        # First call should create new instance
        client1 = get_qdrant_client()
        assert mock_qdrant_client.call_count == 1
        
        # Second call should reuse existing instance
        client2 = get_qdrant_client()
        assert mock_qdrant_client.call_count == 1  # No additional calls
        assert client1 is client2  # Same instance

    @patch('src.qdrant_wrapper.QdrantClient')
    def test_collection_verification_caching(self, mock_qdrant_client):
        """Test that collection verification is cached across instances."""
        # Mock successful Qdrant client
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_client_instance.get_collection.side_effect = Exception("Collection not found")
        mock_qdrant_client.return_value = mock_client_instance
        
        from src.qdrant_wrapper import QdrantClientWrapper
        
        # First instance should verify collections
        client1 = QdrantClientWrapper()
        assert QdrantClientWrapper._collections_verified is True
        
        # Reset singleton to force new instance creation
        import src.qdrant_wrapper as qw
        qw._qdrant_client_instance = None
        
        # Second instance should skip verification
        with patch('src.qdrant_wrapper.QdrantClientWrapper._ensure_collections_exist') as mock_ensure:
            client2 = QdrantClientWrapper()
            mock_ensure.assert_not_called()  # Should not be called due to caching

    @patch('src.qdrant_wrapper.QdrantClient')
    def test_unhealthy_client_recreation(self, mock_qdrant_client):
        """Test that unhealthy clients are recreated."""
        # Mock client that becomes unhealthy
        mock_client_instance = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        from src.qdrant_wrapper import get_qdrant_client
        
        # First call creates healthy client
        mock_client_instance.get_collections.return_value = Mock()
        client1 = get_qdrant_client()
        
        # Make client unhealthy
        mock_client_instance.get_collections.side_effect = Exception("Connection lost")
        
        # Next call should create new instance
        mock_new_client = Mock()
        mock_new_client.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_new_client
        
        client2 = get_qdrant_client()
        assert mock_qdrant_client.call_count == 2  # Two instances created

    def test_reset_verification_cache(self):
        """Test that verification cache can be reset."""
        from src.qdrant_wrapper import QdrantClientWrapper
        
        # Set verification flag
        QdrantClientWrapper._collections_verified = True
        
        # Reset cache
        QdrantClientWrapper.reset_verification_cache()
        
        # Should be reset
        assert QdrantClientWrapper._collections_verified is False

    @patch('src.qdrant_wrapper.QdrantClient')
    def test_health_check_includes_cache_status(self, mock_qdrant_client):
        """Test that health check includes verification cache status."""
        # Mock successful Qdrant client
        mock_client_instance = Mock()
        mock_collections = Mock()
        mock_collections.collections = []
        mock_client_instance.get_collections.return_value = mock_collections
        mock_qdrant_client.return_value = mock_client_instance
        
        from src.qdrant_wrapper import QdrantClientWrapper
        
        client = QdrantClientWrapper()
        health = client.health_check()
        
        assert "collections_verified" in health
        assert health["collections_verified"] is True

    @patch('src.qdrant_wrapper.QdrantClient')
    def test_performance_optimization(self, mock_qdrant_client):
        """Test that optimizations improve performance."""
        # Mock Qdrant client
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        from src.qdrant_wrapper import get_qdrant_client
        
        # Measure time for multiple client retrievals
        start_time = time.time()
        
        # First call (creates client)
        client1 = get_qdrant_client()
        
        # Multiple subsequent calls (should reuse)
        for _ in range(50):
            client = get_qdrant_client()
            assert client is client1  # Same instance
        
        end_time = time.time()
        elapsed = end_time - start_time
        
        # Should be very fast due to singleton pattern
        assert elapsed < 0.05, f"Client retrieval too slow: {elapsed:.3f}s"
        
        # Should only create one Qdrant client instance
        assert mock_qdrant_client.call_count == 1

    @patch('src.qdrant_wrapper.QdrantClient')
    def test_concurrent_access_safety(self, mock_qdrant_client):
        """Test that singleton pattern is safe for concurrent access."""
        import concurrent.futures
        
        # Mock Qdrant client
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        from src.qdrant_wrapper import get_qdrant_client
        
        clients = []
        
        def get_client():
            return get_qdrant_client()
        
        # Create multiple clients concurrently
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(get_client) for _ in range(20)]
            for future in concurrent.futures.as_completed(futures):
                clients.append(future.result())
        
        # All clients should be the same instance
        first_client = clients[0]
        for client in clients:
            assert client is first_client
        
        # Should only create one Qdrant client instance
        assert mock_qdrant_client.call_count == 1


class TestQdrantConnectionManagement:
    """Test Qdrant connection management and lifecycle."""

    def setup_method(self):
        """Reset state before each test."""
        import src.qdrant_wrapper as qw
        qw._qdrant_client_instance = None
        from src.qdrant_wrapper import QdrantClientWrapper
        QdrantClientWrapper._collections_verified = False

    def teardown_method(self):
        """Clean up after each test."""
        import src.qdrant_wrapper as qw
        qw._qdrant_client_instance = None
        from src.qdrant_wrapper import QdrantClientWrapper
        QdrantClientWrapper._collections_verified = False

    @patch('src.qdrant_wrapper.QdrantClient')
    def test_connection_error_handling(self, mock_qdrant_client):
        """Test proper handling of connection errors."""
        # Mock connection failure
        mock_qdrant_client.side_effect = ConnectionError("Cannot connect to Qdrant")
        
        from src.qdrant_wrapper import get_qdrant_client
        
        with pytest.raises(ConnectionError):
            get_qdrant_client()

    @patch('src.qdrant_wrapper.QdrantClient')
    def test_connection_recovery(self, mock_qdrant_client):
        """Test that connections can be recovered after failures."""
        from src.qdrant_wrapper import get_qdrant_client
        
        # First call fails
        mock_qdrant_client.side_effect = ConnectionError("Connection failed")
        
        with pytest.raises(ConnectionError):
            get_qdrant_client()
        
        # Recovery: successful connection
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.side_effect = None
        mock_qdrant_client.return_value = mock_client_instance
        
        # Should succeed on retry
        client = get_qdrant_client()
        assert client is not None

    @patch.dict(os.environ, {"QDRANT_HOST": "test-host", "QDRANT_PORT": "9999"})
    @patch('src.qdrant_wrapper.QdrantClient')
    def test_environment_configuration(self, mock_qdrant_client):
        """Test that environment variables are properly used."""
        # Mock successful client
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        from src.qdrant_wrapper import QdrantClientWrapper
        
        client = QdrantClientWrapper()
        
        # Check that custom host and port are used
        assert client.host == "test-host"
        assert client.port == 9999
        
        # Verify Qdrant client was created with correct parameters
        mock_qdrant_client.assert_called_with(
            host="test-host",
            port=9999,
            prefer_grpc=True,
            timeout=30
        )


================================================
FILE: tests/test_qdrant_wrapper.py
================================================
"""
Unit tests for QdrantClientWrapper.

Tests the core functionality of the Qdrant client wrapper that replaced Supabase.
"""
import pytest
import sys
from pathlib import Path
from unittest.mock import Mock, patch

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from qdrant_wrapper import QdrantClientWrapper, get_qdrant_client
from embedding_config import get_embedding_dimensions


class TestQdrantClientWrapper:
    """Test cases for QdrantClientWrapper class."""

    @patch('qdrant_wrapper.QdrantClient')
    def test_init_default_config(self, mock_qdrant_client):
        """Test initialization with default configuration."""
        # Setup mock
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        # Test
        wrapper = QdrantClientWrapper()
        
        # Verify
        assert wrapper.host == "localhost"
        assert wrapper.port == 6333
        mock_qdrant_client.assert_called_once_with(
            host="localhost",
            port=6333,
            prefer_grpc=True,
            timeout=30
        )

    @patch('qdrant_wrapper.QdrantClient')
    def test_init_custom_config(self, mock_qdrant_client):
        """Test initialization with custom configuration."""
        # Setup mock
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        # Test
        wrapper = QdrantClientWrapper(host="custom-host", port=9999)
        
        # Verify
        assert wrapper.host == "custom-host"
        assert wrapper.port == 9999

    @patch('qdrant_wrapper.QdrantClient')
    def test_generate_point_id(self, mock_qdrant_client):
        """Test point ID generation consistency."""
        # Setup mock
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        wrapper = QdrantClientWrapper()
        
        # Test
        url = "https://example.com/page"
        chunk_number = 1
        
        id1 = wrapper.generate_point_id(url, chunk_number)
        id2 = wrapper.generate_point_id(url, chunk_number)
        
        # Verify consistency
        assert id1 == id2
        assert isinstance(id1, str)
        assert f"_{chunk_number}" in id1

    @patch('qdrant_wrapper.QdrantClient')
    def test_normalize_search_results(self, mock_qdrant_client):
        """Test search result normalization."""
        # Setup mock
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        wrapper = QdrantClientWrapper()
        
        # Test data
        mock_hit = Mock()
        mock_hit.id = "test_id"
        mock_hit.score = 0.95
        mock_hit.payload = {
            "url": "https://example.com",
            "content": "test content",
            "chunk_number": 1
        }
        
        qdrant_results = [mock_hit]
        
        # Test
        normalized = wrapper.normalize_search_results(qdrant_results)
        
        # Verify
        assert len(normalized) == 1
        result = normalized[0]
        assert result["id"] == "test_id"
        assert result["similarity"] == 0.95
        assert result["url"] == "https://example.com"
        assert result["content"] == "test content"
        assert result["chunk_number"] == 1

    @patch('qdrant_wrapper.QdrantClient')
    def test_health_check_healthy(self, mock_qdrant_client):
        """Test health check when system is healthy."""
        # Setup mocks
        mock_client_instance = Mock()
        mock_collections = Mock()
        mock_collections.collections = [Mock(name="crawled_pages"), Mock(name="code_examples")]
        mock_client_instance.get_collections.return_value = mock_collections
        
        # Mock collection info
        mock_collection_info = Mock()
        mock_collection_info.status = "green"
        mock_collection_info.points_count = 100
        mock_collection_info.config.params.vectors.distance.value = "Cosine"
        mock_collection_info.config.params.vectors.size = get_embedding_dimensions()
        mock_client_instance.get_collection.return_value = mock_collection_info
        
        mock_qdrant_client.return_value = mock_client_instance
        
        wrapper = QdrantClientWrapper()
        
        # Test
        health = wrapper.health_check()
        
        # Verify
        assert health["status"] == "healthy"
        assert "collections" in health
        assert health["sources_count"] == 0  # Empty sources_storage initially

    @patch('qdrant_wrapper.QdrantClient')
    def test_health_check_unhealthy(self, mock_qdrant_client):
        """Test health check when system is unhealthy."""
        # Setup mock client that works for initialization but fails for health check
        mock_client_instance = Mock()
        mock_client_instance.get_collections.side_effect = [Mock(), Exception("Connection failed")]
        mock_qdrant_client.return_value = mock_client_instance
        
        wrapper = QdrantClientWrapper()
        
        # Test
        health = wrapper.health_check()
        
        # Verify
        assert health["status"] == "unhealthy"
        assert "error" in health
        assert "Connection failed" in health["error"]

    @patch('qdrant_wrapper.QdrantClient')
    def test_update_source_info(self, mock_qdrant_client):
        """Test source information update."""
        # Setup mock
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_qdrant_client.return_value = mock_client_instance
        
        wrapper = QdrantClientWrapper()
        
        # Test
        source_id = "example.com"
        summary = "Test website"
        word_count = 1000
        
        wrapper.update_source_info(source_id, summary, word_count)
        
        # Verify
        sources = wrapper.get_available_sources()
        assert len(sources) == 1
        source = sources[0]
        assert source["source_id"] == source_id
        assert source["summary"] == summary
        assert source["total_word_count"] == word_count

    @patch('qdrant_wrapper.QdrantClient')
    def test_search_documents_no_filter(self, mock_qdrant_client):
        """Test document search without filters."""
        # Setup mock
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        
        # Mock search results
        mock_hit = Mock()
        mock_hit.id = "doc1"
        mock_hit.score = 0.9
        mock_hit.payload = {"content": "test document"}
        mock_client_instance.search.return_value = [mock_hit]
        
        mock_qdrant_client.return_value = mock_client_instance
        
        wrapper = QdrantClientWrapper()
        
        # Test
        query_embedding = [0.1] * get_embedding_dimensions()
        results = wrapper.search_documents(query_embedding, match_count=5)
        
        # Verify
        assert len(results) == 1
        assert results[0]["id"] == "doc1"
        assert results[0]["similarity"] == 0.9
        
        # Verify search was called correctly
        mock_client_instance.search.assert_called_once_with(
            collection_name="crawled_pages",
            query_vector=query_embedding,
            query_filter=None,
            limit=5,
            with_payload=True,
            score_threshold=0.0
        )

    @patch('qdrant_wrapper.QdrantClient')
    def test_search_documents_with_filters(self, mock_qdrant_client):
        """Test document search with metadata and source filters."""
        # Setup mock
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        mock_client_instance.search.return_value = []
        mock_qdrant_client.return_value = mock_client_instance
        
        wrapper = QdrantClientWrapper()
        
        # Test
        query_embedding = [0.1] * get_embedding_dimensions()
        filter_metadata = {"category": "docs"}
        source_filter = "example.com"
        
        wrapper.search_documents(
            query_embedding, 
            match_count=10, 
            filter_metadata=filter_metadata,
            source_filter=source_filter
        )
        
        # Verify search was called with filters
        call_args = mock_client_instance.search.call_args
        assert call_args[1]["query_filter"] is not None

    @patch('qdrant_wrapper.QdrantClient')
    def test_keyword_search_documents(self, mock_qdrant_client):
        """Test keyword search functionality."""
        # Setup mock
        mock_client_instance = Mock()
        mock_client_instance.get_collections.return_value = Mock()
        
        # Mock scroll results
        mock_point = Mock()
        mock_point.id = "doc1"
        mock_point.payload = {"content": "This is a Python tutorial"}
        mock_client_instance.scroll.return_value = ([mock_point], None)
        
        mock_qdrant_client.return_value = mock_client_instance
        
        wrapper = QdrantClientWrapper()
        
        # Test
        results = wrapper.keyword_search_documents("python", match_count=5)
        
        # Verify
        assert len(results) == 1
        assert results[0]["id"] == "doc1"
        assert results[0]["similarity"] == 0.5  # Default similarity for keyword matches
        
        # Verify scroll was called
        mock_client_instance.scroll.assert_called_once()


class TestUtilityFunctions:
    """Test utility functions."""

    @patch('qdrant_wrapper.QdrantClientWrapper')
    def test_get_qdrant_client(self, mock_wrapper):
        """Test Qdrant client factory function."""
        # Setup mock
        mock_instance = Mock()
        mock_wrapper.return_value = mock_instance
        
        # Test
        client = get_qdrant_client()
        
        # Verify
        assert client == mock_instance
        mock_wrapper.assert_called_once()

    @patch('qdrant_wrapper.QdrantClientWrapper')
    def test_get_qdrant_client_failure(self, mock_wrapper):
        """Test Qdrant client factory function with failure."""
        # Setup mock to raise exception
        mock_wrapper.side_effect = Exception("Connection failed")
        
        # Test
        with pytest.raises(Exception) as exc_info:
            get_qdrant_client()
        
        # Verify
        assert "Connection failed" in str(exc_info.value)


if __name__ == "__main__":
    pytest.main([__file__])


================================================
FILE: tests/test_redis_integration.py
================================================
Error reading file with 'cp1252': 'charmap' codec can't decode byte 0x9d in position 15772: character maps to <undefined>


================================================
FILE: tests/test_reranking_enhanced.py
================================================
"""
Tests for enhanced reranking functionality.

Tests the new reranking enhancements including configurable model selection,
model warming, and health check functionality as implemented in the PRP.
"""

import pytest
import os
import json
import asyncio
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from pathlib import Path

# Import the device manager functions
import sys
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

# Import directly from utils.py to avoid stub functions
import importlib.util
utils_spec = importlib.util.spec_from_file_location("utils_module", src_path / "utils.py")
utils_module = importlib.util.module_from_spec(utils_spec)
utils_spec.loader.exec_module(utils_module)
health_check_reranking_model = utils_module.health_check_reranking_model
cleanup_gpu_memory = utils_module.cleanup_gpu_memory


class TestConfigurableModelSelection:
    """Test configurable model selection enhancement."""
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.get_model_kwargs_for_device')
    @patch.dict(os.environ, {
        'USE_RERANKING': 'true',
        'RERANKING_MODEL_NAME': 'cross-encoder/ms-marco-MiniLM-L-12-v2'
    })
    def test_custom_model_name_used(self, mock_get_kwargs, mock_get_device, mock_cross_encoder):
        """Test that custom model name from environment is used."""
        # Import here to get the patched environment
        from crawl4ai_mcp import crawl4ai_lifespan
        
        # Mock device and model kwargs
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="cuda:0")
        mock_get_device.return_value = mock_device
        mock_get_kwargs.return_value = {}
        
        # Mock CrossEncoder
        mock_model = Mock()
        mock_cross_encoder.return_value = mock_model
        
        # Create async context manager manually for testing
        async def test_lifespan():
            async with crawl4ai_lifespan(None) as context:
                # Verify CrossEncoder was called with custom model name
                mock_cross_encoder.assert_called_with(
                    'cross-encoder/ms-marco-MiniLM-L-12-v2',  # Custom model name
                    device=str(mock_device),
                    model_kwargs={}
                )
                
                # Verify model is stored in context
                assert context.reranking_model == mock_model
        
        # Run the async test
        asyncio.run(test_lifespan())
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.get_model_kwargs_for_device')
    @patch.dict(os.environ, {
        'USE_RERANKING': 'true'
    }, clear=False)
    def test_default_model_name_used(self, mock_get_kwargs, mock_get_device, mock_cross_encoder):
        """Test that default model name is used when no custom name is set."""
        # Ensure RERANKING_MODEL_NAME is not set
        os.environ.pop('RERANKING_MODEL_NAME', None)
        
        from crawl4ai_mcp import crawl4ai_lifespan
        
        # Mock device and model kwargs
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="cpu")
        mock_get_device.return_value = mock_device
        mock_get_kwargs.return_value = {}
        
        # Mock CrossEncoder
        mock_model = Mock()
        mock_cross_encoder.return_value = mock_model
        
        async def test_lifespan():
            async with crawl4ai_lifespan(None) as context:
                # Verify CrossEncoder was called with default model name
                mock_cross_encoder.assert_called_with(
                    'cross-encoder/ms-marco-MiniLM-L-6-v2',  # Default model name
                    device=str(mock_device),
                    model_kwargs={}
                )
        
        asyncio.run(test_lifespan())


class TestModelWarming:
    """Test model warming enhancement."""
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.get_model_kwargs_for_device')
    @patch('crawl4ai_mcp.cleanup_gpu_memory')
    @patch.dict(os.environ, {
        'USE_RERANKING': 'true',
        'RERANKING_WARMUP_SAMPLES': '3'
    })
    def test_model_warming_enabled(self, mock_cleanup, mock_get_kwargs, mock_get_device, mock_cross_encoder):
        """Test that model warming works with custom sample count."""
        from crawl4ai_mcp import crawl4ai_lifespan
        
        # Mock device
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="cuda:0")
        mock_get_device.return_value = mock_device
        mock_get_kwargs.return_value = {}
        
        # Mock CrossEncoder with predict method
        mock_model = Mock()
        mock_model.predict = Mock(return_value=[0.5, 0.6, 0.7])
        mock_cross_encoder.return_value = mock_model
        
        async def test_lifespan():
            async with crawl4ai_lifespan(None) as context:
                # Verify predict was called for warming with 3 samples
                mock_model.predict.assert_called_once()
                call_args = mock_model.predict.call_args[0][0]  # Get the pairs argument
                assert len(call_args) == 3  # 3 warmup samples
                
                # Verify cleanup was called after warming
                mock_cleanup.assert_called()
        
        asyncio.run(test_lifespan())
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.get_model_kwargs_for_device')
    @patch.dict(os.environ, {
        'USE_RERANKING': 'true',
        'RERANKING_WARMUP_SAMPLES': '0'
    })
    def test_model_warming_disabled(self, mock_get_kwargs, mock_get_device, mock_cross_encoder):
        """Test that model warming is skipped when set to 0."""
        from crawl4ai_mcp import crawl4ai_lifespan
        
        # Mock device
        mock_device = Mock()
        mock_get_device.return_value = mock_device
        mock_get_kwargs.return_value = {}
        
        # Mock CrossEncoder
        mock_model = Mock()
        mock_model.predict = Mock()
        mock_cross_encoder.return_value = mock_model
        
        async def test_lifespan():
            async with crawl4ai_lifespan(None) as context:
                # Verify predict was NOT called for warming
                mock_model.predict.assert_not_called()
        
        asyncio.run(test_lifespan())
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.get_model_kwargs_for_device')
    @patch('crawl4ai_mcp.cleanup_gpu_memory')
    @patch.dict(os.environ, {
        'USE_RERANKING': 'true',
        'RERANKING_WARMUP_SAMPLES': '5'
    })
    def test_model_warming_error_handling(self, mock_cleanup, mock_get_kwargs, mock_get_device, mock_cross_encoder):
        """Test that model warming errors are handled gracefully."""
        from crawl4ai_mcp import crawl4ai_lifespan
        
        # Mock device
        mock_device = Mock()
        mock_get_device.return_value = mock_device
        mock_get_kwargs.return_value = {}
        
        # Mock CrossEncoder with predict that raises an error
        mock_model = Mock()
        mock_model.predict = Mock(side_effect=RuntimeError("Warmup failed"))
        mock_cross_encoder.return_value = mock_model
        
        async def test_lifespan():
            # Should not raise an exception despite warmup failure
            async with crawl4ai_lifespan(None) as context:
                # Model should still be available despite warmup failure
                assert context.reranking_model == mock_model
        
        asyncio.run(test_lifespan())


class TestHealthCheckReranking:
    """Test health check functionality for reranking."""
    
    def test_health_check_with_valid_model(self):
        """Test health check with a working CrossEncoder model."""
        # Mock CrossEncoder model
        mock_model = Mock()
        mock_model.predict = Mock(return_value=[0.7, 0.8])
        mock_model.device = "cuda:0"
        mock_model.model = Mock()
        mock_model.model.name_or_path = "cross-encoder/test-model"
        
        with patch.object(utils_module, 'cleanup_gpu_memory'):
            with patch('builtins.isinstance', return_value=True):  # Mock isinstance check
                result = health_check_reranking_model(mock_model)
        
        print(f"Health check result: {result}")  # Debug output
        assert result['model_available'] is True
        assert result['model_name'] == "cross-encoder/test-model"
        assert result['device'] == "cuda:0"
        assert result['inference_test_passed'] is True
        assert result['inference_latency_ms'] is not None
        assert result['inference_latency_ms'] > 0
        assert result['error_message'] is None
    
    def test_health_check_with_no_model(self):
        """Test health check when no model is provided and reranking is disabled."""
        with patch.dict(os.environ, {'USE_RERANKING': 'false'}):
            result = health_check_reranking_model(None)
        
        assert result['model_available'] is False
        assert result['inference_test_passed'] is False
        assert result['error_message'] == "Reranking not enabled (USE_RERANKING=false)"
    
    def test_health_check_with_invalid_model_type(self):
        """Test health check with invalid model type."""
        invalid_model = "not a crossencoder"
        
        result = health_check_reranking_model(invalid_model)
        
        assert result['model_available'] is False
        assert result['inference_test_passed'] is False
        assert result['error_message'] == "Invalid model type - expected CrossEncoder"
    
    def test_health_check_inference_failure(self):
        """Test health check when model inference fails."""
        mock_model = Mock()
        mock_model.predict = Mock(side_effect=RuntimeError("Inference failed"))
        mock_model.device = "cpu"
        mock_model.model = Mock()
        mock_model.model.name_or_path = "test-model"
        
        result = health_check_reranking_model(mock_model)
        
        assert result['model_available'] is True
        assert result['inference_test_passed'] is False
        assert "Health check failed: Inference failed" in result['error_message']
    
    def test_health_check_invalid_inference_output(self):
        """Test health check when model returns invalid output."""
        mock_model = Mock()
        mock_model.predict = Mock(return_value="invalid output")  # Should be a list of numbers
        mock_model.device = "cpu"
        mock_model.model = Mock()
        mock_model.model.name_or_path = "test-model"
        
        result = health_check_reranking_model(mock_model)
        
        assert result['model_available'] is True
        assert result['inference_test_passed'] is False
        assert "Invalid inference output" in result['error_message']


class TestMCPHealthCheckTool:
    """Test the MCP health check tool integration."""
    
    @pytest.mark.asyncio
    async def test_health_check_reranking_tool_success(self):
        """Test the MCP health check tool with successful reranking model."""
        from crawl4ai_mcp import health_check_reranking
        
        # Mock context with reranking model
        mock_context = Mock()
        mock_request_context = Mock()
        mock_lifespan_context = Mock()
        
        # Mock reranking model
        mock_model = Mock()
        mock_model.predict = Mock(return_value=[0.7, 0.8])
        mock_model.device = "cuda:0"
        mock_model.model = Mock()
        mock_model.model.name_or_path = "cross-encoder/test-model"
        
        mock_lifespan_context.reranking_model = mock_model
        mock_request_context.lifespan_context = mock_lifespan_context
        mock_context.request_context = mock_request_context
        
        with patch.object(utils_module, 'cleanup_gpu_memory'):
            with patch.dict(os.environ, {
                'USE_RERANKING': 'true',
                'RERANKING_MODEL_NAME': 'test-model',
                'RERANKING_WARMUP_SAMPLES': '5'
            }):
                result_json = await health_check_reranking(mock_context)
        
        result = json.loads(result_json)
        
        assert result['overall_status'] == 'healthy'
        assert result['model_available'] is True
        assert result['inference_test_passed'] is True
        assert result['configuration']['use_reranking_enabled'] == 'true'
        assert result['configuration']['model_name_config'] == 'test-model'
        assert result['configuration']['warmup_samples_config'] == '5'
    
    @pytest.mark.asyncio
    async def test_health_check_reranking_tool_no_model(self):
        """Test the MCP health check tool when no reranking model is available."""
        from crawl4ai_mcp import health_check_reranking
        
        # Mock context without reranking model
        mock_context = Mock()
        mock_context.request_context = None
        
        with patch.dict(os.environ, {'USE_RERANKING': 'false'}):
            result_json = await health_check_reranking(mock_context)
        
        result = json.loads(result_json)
        
        assert result['overall_status'] == 'unhealthy'
        assert result['model_available'] is False
        assert result['configuration']['use_reranking_enabled'] == 'false'
    
    @pytest.mark.asyncio
    async def test_health_check_reranking_tool_exception(self):
        """Test the MCP health check tool handles exceptions gracefully."""
        from crawl4ai_mcp import health_check_reranking
        
        # Mock context that will cause an exception
        mock_context = Mock()
        mock_context.request_context = Mock()
        mock_context.request_context.lifespan_context = Mock()
        mock_context.request_context.lifespan_context.reranking_model = "invalid"
        
        with patch('crawl4ai_mcp.health_check_reranking_model', side_effect=Exception("Test error")):
            result_json = await health_check_reranking(mock_context)
        
        result = json.loads(result_json)
        
        assert result['overall_status'] == 'error'
        assert result['model_available'] is False
        assert "Health check failed with exception: Test error" in result['error_message']


class TestIntegrationWithExistingSystem:
    """Test integration of enhancements with existing reranking system."""
    
    @patch('crawl4ai_mcp.CrossEncoder')
    @patch('crawl4ai_mcp.get_optimal_device')
    @patch('crawl4ai_mcp.get_model_kwargs_for_device')
    @patch('crawl4ai_mcp.cleanup_gpu_memory')
    @patch.dict(os.environ, {
        'USE_RERANKING': 'true',
        'RERANKING_MODEL_NAME': 'custom-model',
        'RERANKING_WARMUP_SAMPLES': '2'
    })
    def test_full_integration_workflow(self, mock_cleanup, mock_get_kwargs, mock_get_device, mock_cross_encoder):
        """Test the full workflow with all enhancements enabled."""
        from crawl4ai_mcp import crawl4ai_lifespan, rerank_results
        
        # Mock device
        mock_device = Mock()
        mock_device.__str__ = Mock(return_value="cuda:0")
        mock_get_device.return_value = mock_device
        mock_get_kwargs.return_value = {}
        
        # Mock CrossEncoder
        mock_model = Mock()
        mock_model.predict = Mock(side_effect=[
            [0.1, 0.2],  # Warmup call
            [0.8, 0.9, 0.7]  # Actual reranking call
        ])
        mock_cross_encoder.return_value = mock_model
        
        async def test_workflow():
            async with crawl4ai_lifespan(None) as context:
                # Verify model was loaded with custom name
                mock_cross_encoder.assert_called_with(
                    'custom-model',
                    device=str(mock_device),
                    model_kwargs={}
                )
                
                # Verify warmup was performed
                assert mock_model.predict.call_count >= 1
                
                # Test actual reranking functionality
                test_results = [
                    {"content": "result 1", "id": 1},
                    {"content": "result 2", "id": 2},
                    {"content": "result 3", "id": 3}
                ]
                
                reranked = rerank_results(context.reranking_model, "test query", test_results)
                
                # Verify reranking worked and results are sorted by score
                assert len(reranked) == 3
                assert reranked[0]["rerank_score"] == 0.9  # Highest
                assert reranked[1]["rerank_score"] == 0.8  # Middle  
                assert reranked[2]["rerank_score"] == 0.7  # Lowest
                
                # Verify cleanup was called
                assert mock_cleanup.call_count >= 2  # Once for warmup, once for reranking
        
        asyncio.run(test_workflow())


class TestEnvironmentConfiguration:
    """Test various environment configuration scenarios."""
    
    @patch.dict(os.environ, {
        'USE_RERANKING': 'false'
    })
    def test_reranking_disabled_no_enhancements(self):
        """Test that when reranking is disabled, no enhancements are loaded."""
        from crawl4ai_mcp import crawl4ai_lifespan
        
        async def test_disabled():
            async with crawl4ai_lifespan(None) as context:
                # No reranking model should be loaded
                assert context.reranking_model is None
        
        asyncio.run(test_disabled())
    
    def test_default_environment_values(self):
        """Test that default values are used when environment variables are not set."""
        # Clear relevant environment variables
        env_vars_to_clear = ['RERANKING_MODEL_NAME', 'RERANKING_WARMUP_SAMPLES']
        original_values = {}
        
        for var in env_vars_to_clear:
            original_values[var] = os.environ.get(var)
            os.environ.pop(var, None)
        
        try:
            # Test default model name
            default_model = os.getenv("RERANKING_MODEL_NAME", "cross-encoder/ms-marco-MiniLM-L-6-v2")
            assert default_model == "cross-encoder/ms-marco-MiniLM-L-6-v2"
            
            # Test default warmup samples
            default_samples = int(os.getenv("RERANKING_WARMUP_SAMPLES", "5"))
            assert default_samples == 5
            
        finally:
            # Restore original environment
            for var, value in original_values.items():
                if value is not None:
                    os.environ[var] = value


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
FILE: tests/test_script.py
================================================
from __future__ import annotations
from typing import List, Optional
from dataclasses import dataclass
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from rich.markdown import Markdown
from rich.console import Console
from rich.live import Live
import asyncio
import os

from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai import Agent, RunContext
from graphiti_core import Graphiti

load_dotenv()

# ========== Define dependencies ==========
@dataclass
class GraphitiDependencies:
    """Dependencies for the Graphiti agent."""
    graphiti_client: Graphiti

# ========== Helper function to get model configuration ==========
def get_model():
    """Configure and return the LLM model to use."""
    # Get chat model with modern fallback configuration
    model_choice = os.getenv('CHAT_MODEL') or os.getenv('CHAT_FALLBACK_MODEL') or 'gpt-4o-mini'
    
    # Get API key
    api_key = os.getenv('CHAT_API_KEY') or 'no-api-key-provided'
    
    # Get base URL for flexible API configuration
    base_url = os.getenv('CHAT_API_BASE')
    
    # Configure provider with optional base URL
    if base_url:
        provider = OpenAIProvider(api_key=api_key, base_url=base_url)
    else:
        provider = OpenAIProvider(api_key=api_key)

    return OpenAIModel(model_choice, provider=provider)

# ========== Create the Graphiti agent ==========
graphiti_agent = Agent(
    get_model(),
    system_prompt="""You are a helpful assistant with access to a knowledge graph filled with temporal data about LLMs.
    When the user asks you a question, use your search tool to query the knowledge graph and then answer honestly.
    Be willing to admit when you didn't find the information necessary to answer the question.""",
    deps_type=GraphitiDependencies
)

# ========== Define a result model for Graphiti search ==========
class GraphitiSearchResult(BaseModel):
    """Model representing a search result from Graphiti."""
    uuid: str = Field(description="The unique identifier for this fact")
    fact: str = Field(description="The factual statement retrieved from the knowledge graph")
    valid_at: Optional[str] = Field(None, description="When this fact became valid (if known)")
    invalid_at: Optional[str] = Field(None, description="When this fact became invalid (if known)")
    source_node_uuid: Optional[str] = Field(None, description="UUID of the source node")

# ========== Graphiti search tool ==========
@graphiti_agent.tool
async def search_graphiti(ctx: RunContext[GraphitiDependencies], query: str) -> List[GraphitiSearchResult]:
    """Search the Graphiti knowledge graph with the given query.
    
    Args:
        ctx: The run context containing dependencies
        query: The search query to find information in the knowledge graph
        
    Returns:
        A list of search results containing facts that match the query
    """
    # Access the Graphiti client from dependencies
    graphiti = ctx.deps.graphiti_client
    
    try:
        # Perform the search
        results = await graphiti.search(query)
        
        # Format the results
        formatted_results = []
        for result in results:
            formatted_result = GraphitiSearchResult(
                uuid=result.uuid,
                fact=result.fact,
                source_node_uuid=result.source_node_uuid if hasattr(result, 'source_node_uuid') else None
            )
            
            # Add temporal information if available
            if hasattr(result, 'valid_at') and result.valid_at:
                formatted_result.valid_at = str(result.valid_at)
            if hasattr(result, 'invalid_at') and result.invalid_at:
                formatted_result.invalid_at = str(result.invalid_at)
            
            formatted_results.append(formatted_result)
        
        return formatted_results
    except Exception as e:
        # Log the error but don't close the connection since it's managed by the dependency
        print(f"Error searching Graphiti: {str(e)}")
        raise

# ========== Main execution function ==========
async def main():
    """Run the Graphiti agent with user queries."""
    print("Graphiti Agent - Powered by Pydantic AI, Graphiti, and Neo4j")
    print("Enter 'exit' to quit the program.")

    # Neo4j connection parameters
    neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')
    neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')
    neo4j_password = os.environ.get('NEO4J_PASSWORD', 'password')
    
    # Initialize Graphiti with Neo4j connection
    graphiti_client = Graphiti(neo4j_uri, neo4j_user, neo4j_password)
    
    # Initialize the graph database with graphiti's indices if needed
    try:
        await graphiti_client.build_indices_and_constraints()
        print("Graphiti indices built successfully.")
    except Exception as e:
        print(f"Note: {str(e)}")
        print("Continuing with existing indices...")

    console = Console()
    messages = []
    
    try:
        while True:
            # Get user input
            user_input = input("\n[You] ")
            
            # Check if user wants to exit
            if user_input.lower() in ['exit', 'quit', 'bye', 'goodbye']:
                print("Goodbye!")
                break
            
            try:
                # Process the user input and output the response
                print("\n[Assistant]")
                with Live('', console=console, vertical_overflow='visible') as live:
                    # Pass the Graphiti client as a dependency
                    deps = GraphitiDependencies(graphiti_client=graphiti_client)
                    
                    async with graphiti_agent.run_a_stream(
                        user_input, message_history=messages, deps=deps
                    ) as result:
                        curr_message = ""
                        async for message in result.stream_text(delta=True):
                            curr_message += message
                            live.update(Markdown(curr_message))
                    
                    # Add the new messages to the chat history
                    messages.extend(result.all_messages())
                
            except Exception as e:
                print(f"\n[Error] An error occurred: {str(e)}")
    finally:
        # Close the Graphiti connection when done
        await graphiti_client.close()
        print("\nGraphiti connection closed.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nProgram terminated by user.")
    except Exception as e:
        print(f"\nUnexpected error: {str(e)}")
        raise



================================================
FILE: tests/test_smart_crawl_github.py
================================================
"""
Unit tests for smart_crawl_github MCP tool.

Tests the complete GitHub repository crawling functionality.
"""
import pytest
import sys
import json
from pathlib import Path
from unittest.mock import Mock, patch

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from crawl4ai_mcp import smart_crawl_github


class TestSmartCrawlGitHub:
    """Test cases for smart_crawl_github MCP tool."""
    
    def setup_method(self):
        """Setup test method with common fixtures."""
        self.mock_context = Mock()
        self.mock_qdrant_client = Mock()
        self.mock_context.request_context.lifespan_context.qdrant_client = self.mock_qdrant_client
    
    @pytest.mark.asyncio
    async def test_invalid_github_url(self):
        """Test with invalid GitHub URL."""
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url="https://gitlab.com/user/repo"
        )
        
        response = json.loads(result)
        assert response["success"] is False
        assert "Invalid GitHub repository URL" in response["error"]
        assert response["repo_url"] == "https://gitlab.com/user/repo"
    
    @pytest.mark.asyncio
    async def test_empty_url(self):
        """Test with empty URL."""
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url=""
        )
        
        response = json.loads(result)
        assert response["success"] is False
        assert "Invalid GitHub repository URL" in response["error"]
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.MarkdownDiscovery')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_no_markdown_files_found(self, mock_extractor_cls, mock_discovery_cls, mock_manager_cls):
        """Test when no markdown files are found in repository."""
        # Setup mocks
        mock_manager = Mock()
        mock_discovery = Mock()
        mock_extractor = Mock()
        
        mock_manager_cls.return_value = mock_manager
        mock_discovery_cls.return_value = mock_discovery
        mock_extractor_cls.return_value = mock_extractor
        
        mock_manager.clone_repository.return_value = "/tmp/test_repo"
        mock_extractor.extract_repo_metadata.return_value = {
            "owner": "user",
            "repo_name": "repo"
        }
        mock_discovery.discover_markdown_files.return_value = []
        
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url="https://github.com/user/repo"
        )
        
        response = json.loads(result)
        assert response["success"] is False
        assert "No markdown files found" in response["error"]
        mock_manager.cleanup.assert_called_once()
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.MarkdownDiscovery')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    @patch('crawl4ai_mcp.smart_chunk_markdown')
    @patch('crawl4ai_mcp.extract_source_summary')
    @patch('crawl4ai_mcp.update_source_info')
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    async def test_successful_crawl(self, mock_add_docs, mock_update_source, mock_extract_summary,
                                  mock_chunk, mock_extractor_cls, mock_discovery_cls, mock_manager_cls):
        """Test successful GitHub repository crawling."""
        # Setup mocks
        mock_manager = Mock()
        mock_discovery = Mock()
        mock_extractor = Mock()
        
        mock_manager_cls.return_value = mock_manager
        mock_discovery_cls.return_value = mock_discovery
        mock_extractor_cls.return_value = mock_extractor
        
        # Mock repository data
        mock_manager.clone_repository.return_value = "/tmp/test_repo"
        mock_manager._get_directory_size_mb.return_value = 25.5
        
        mock_repo_metadata = {
            "owner": "user",
            "repo_name": "test-repo",
            "full_name": "user/test-repo",
            "language": "python",
            "description": "A test repository"
        }
        mock_extractor.extract_repo_metadata.return_value = mock_repo_metadata
        
        # Mock markdown files
        mock_markdown_files = [
            {
                "filename": "README.md",
                "relative_path": "README.md",
                "content": "# Test Repository\n\nThis is a test repository for testing purposes.",
                "size_bytes": 1024,
                "word_count": 50,
                "is_readme": True
            },
            {
                "filename": "docs.md",
                "relative_path": "docs/guide.md",
                "content": "# User Guide\n\nThis is the user guide.",
                "size_bytes": 512,
                "word_count": 25,
                "is_readme": False
            }
        ]
        mock_discovery.discover_markdown_files.return_value = mock_markdown_files
        
        # Mock chunking
        mock_chunk.side_effect = lambda content, chunk_size: [content[:chunk_size]]
        
        # Mock source summary
        mock_extract_summary.return_value = "Test repository for demonstration"
        
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url="https://github.com/user/test-repo"
        )
        
        response = json.loads(result)
        
        # Verify success response
        assert response["success"] is True
        assert response["repo_url"] == "https://github.com/user/test-repo"
        assert response["owner"] == "user"
        assert response["repo_name"] == "test-repo"
        assert response["markdown_files_processed"] == 2
        assert response["chunks_stored"] == 2
        assert response["total_word_count"] == 75
        assert response["repository_size_mb"] == 25.5
        assert "github.com/user/test-repo" in response["source_id"]
        assert len(response["files_processed"]) == 2
        
        # Verify method calls
        mock_manager.clone_repository.assert_called_once_with("https://github.com/user/test-repo", 500)
        mock_extractor.extract_repo_metadata.assert_called_once()
        mock_discovery.discover_markdown_files.assert_called_once()
        mock_update_source.assert_called_once()
        mock_add_docs.assert_called_once()
        mock_manager.cleanup.assert_called_once()
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.MarkdownDiscovery')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    @patch('crawl4ai_mcp.smart_chunk_markdown')
    @patch('crawl4ai_mcp.extract_source_summary')
    @patch('crawl4ai_mcp.update_source_info')
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    @patch('crawl4ai_mcp.extract_code_blocks')
    @patch('crawl4ai_mcp.generate_code_example_summary')
    @patch('crawl4ai_mcp.add_code_examples_to_supabase')
    @patch('crawl4ai_mcp.os.getenv')
    async def test_with_code_examples_enabled(self, mock_getenv, mock_add_code, mock_gen_summary,
                                            mock_extract_code, mock_add_docs, mock_update_source,
                                            mock_extract_summary, mock_chunk, mock_extractor_cls,
                                            mock_discovery_cls, mock_manager_cls):
        """Test crawling with code examples extraction enabled."""
        # Setup environment variable
        mock_getenv.side_effect = lambda key, default=None: "true" if key == "USE_AGENTIC_RAG" else default
        
        # Setup mocks
        mock_manager = Mock()
        mock_discovery = Mock()
        mock_extractor = Mock()
        
        mock_manager_cls.return_value = mock_manager
        mock_discovery_cls.return_value = mock_discovery
        mock_extractor_cls.return_value = mock_extractor
        
        mock_manager.clone_repository.return_value = "/tmp/test_repo"
        mock_manager._get_directory_size_mb.return_value = 10.0
        
        mock_repo_metadata = {
            "owner": "user",
            "repo_name": "test-repo"
        }
        mock_extractor.extract_repo_metadata.return_value = mock_repo_metadata
        
        # Mock markdown files with code
        mock_markdown_files = [
            {
                "filename": "README.md",
                "relative_path": "README.md",
                "content": "# Test\n```python\ndef hello():\n    print('Hello')\n```",
                "size_bytes": 1024,
                "word_count": 20,
                "is_readme": True
            }
        ]
        mock_discovery.discover_markdown_files.return_value = mock_markdown_files
        
        # Mock code extraction
        mock_code_blocks = [
            {
                "code": "def hello():\n    print('Hello')",
                "language": "python",
                "context_before": "# Test",
                "context_after": ""
            }
        ]
        mock_extract_code.return_value = mock_code_blocks
        mock_gen_summary.return_value = "Python function that prints Hello"
        
        mock_chunk.side_effect = lambda content, chunk_size: [content]
        mock_extract_summary.return_value = "Test repository"
        
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url="https://github.com/user/test-repo"
        )
        
        response = json.loads(result)
        
        # Verify code examples were processed
        assert response["success"] is True
        assert response["code_examples_stored"] == 1
        
        # Verify code processing methods were called
        mock_extract_code.assert_called_once()
        mock_gen_summary.assert_called_once()
        mock_add_code.assert_called_once()
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.GitHubRepoManager')
    async def test_clone_failure(self, mock_manager_cls):
        """Test when repository cloning fails."""
        mock_manager = Mock()
        mock_manager_cls.return_value = mock_manager
        mock_manager.clone_repository.side_effect = RuntimeError("Clone failed")
        
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url="https://github.com/user/repo"
        )
        
        response = json.loads(result)
        assert response["success"] is False
        assert "Clone failed" in response["error"]
        mock_manager.cleanup.assert_called_once()
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.MarkdownDiscovery')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_metadata_extraction_failure(self, mock_extractor_cls, mock_discovery_cls, mock_manager_cls):
        """Test when metadata extraction fails."""
        mock_manager = Mock()
        mock_discovery = Mock()
        mock_extractor = Mock()
        
        mock_manager_cls.return_value = mock_manager
        mock_discovery_cls.return_value = mock_discovery
        mock_extractor_cls.return_value = mock_extractor
        
        mock_manager.clone_repository.return_value = "/tmp/test_repo"
        mock_extractor.extract_repo_metadata.side_effect = Exception("Metadata extraction failed")
        
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url="https://github.com/user/repo"
        )
        
        response = json.loads(result)
        assert response["success"] is False
        assert "Metadata extraction failed" in response["error"]
        mock_manager.cleanup.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_custom_parameters(self):
        """Test with custom parameters."""
        with patch('crawl4ai_mcp.GitHubRepoManager') as mock_manager_cls, \
             patch('crawl4ai_mcp.MarkdownDiscovery') as mock_discovery_cls, \
             patch('crawl4ai_mcp.GitHubMetadataExtractor') as mock_extractor_cls, \
             patch('crawl4ai_mcp.smart_chunk_markdown') as mock_chunk, \
             patch('crawl4ai_mcp.extract_source_summary') as mock_extract_summary, \
             patch('crawl4ai_mcp.update_source_info') as mock_update_source, \
             patch('crawl4ai_mcp.add_documents_to_supabase') as mock_add_docs:
            
            # Setup mocks
            mock_manager = Mock()
            mock_discovery = Mock()
            mock_extractor = Mock()
            
            mock_manager_cls.return_value = mock_manager
            mock_discovery_cls.return_value = mock_discovery
            mock_extractor_cls.return_value = mock_extractor
            
            mock_manager.clone_repository.return_value = "/tmp/test_repo"
            mock_manager._get_directory_size_mb.return_value = 100.0
            
            mock_extractor.extract_repo_metadata.return_value = {
                "owner": "user",
                "repo_name": "repo"
            }
            
            mock_markdown_files = [
                {
                    "filename": "README.md",
                    "relative_path": "README.md",
                    "content": "Test content",
                    "size_bytes": 500,
                    "word_count": 10,
                    "is_readme": True
                }
            ]
            mock_discovery.discover_markdown_files.return_value = mock_markdown_files
            mock_chunk.return_value = ["Test content"]
            mock_extract_summary.return_value = "Test"
            
            # Test with custom parameters
            result = await smart_crawl_github(
                ctx=self.mock_context,
                repo_url="https://github.com/user/repo",
                max_files=25,
                chunk_size=3000,
                max_size_mb=200
            )
            
            response = json.loads(result)
            assert response["success"] is True
            
            # Verify custom parameters were used
            mock_manager.clone_repository.assert_called_once_with("https://github.com/user/repo", 200)
            mock_discovery.discover_markdown_files.assert_called_once()
            
            # Check that max_files parameter was passed
            call_args = mock_discovery.discover_markdown_files.call_args
            assert call_args[1]["max_files"] == 25
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.MarkdownDiscovery')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    @patch('crawl4ai_mcp.smart_chunk_markdown')
    @patch('crawl4ai_mcp.extract_source_summary')
    @patch('crawl4ai_mcp.update_source_info')
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    async def test_many_files_truncation(self, mock_add_docs, mock_update_source, mock_extract_summary,
                                       mock_chunk, mock_extractor_cls, mock_discovery_cls, mock_manager_cls):
        """Test that file list is truncated in response when there are many files."""
        # Setup mocks
        mock_manager = Mock()
        mock_discovery = Mock()
        mock_extractor = Mock()
        
        mock_manager_cls.return_value = mock_manager
        mock_discovery_cls.return_value = mock_discovery
        mock_extractor_cls.return_value = mock_extractor
        
        mock_manager.clone_repository.return_value = "/tmp/test_repo"
        mock_manager._get_directory_size_mb.return_value = 50.0
        
        mock_extractor.extract_repo_metadata.return_value = {
            "owner": "user",
            "repo_name": "repo"
        }
        
        # Create 15 mock files (more than the 10 limit)
        mock_markdown_files = []
        for i in range(15):
            mock_markdown_files.append({
                "filename": f"file{i}.md",
                "relative_path": f"docs/file{i}.md",
                "content": f"Content of file {i}",
                "size_bytes": 100,
                "word_count": 5,
                "is_readme": False
            })
        
        mock_discovery.discover_markdown_files.return_value = mock_markdown_files
        mock_chunk.side_effect = lambda content, chunk_size: [content]
        mock_extract_summary.return_value = "Test repository"
        
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url="https://github.com/user/repo"
        )
        
        response = json.loads(result)
        assert response["success"] is True
        assert response["markdown_files_processed"] == 15
        
        # Check that files_processed list is truncated and has "..." indicator
        assert len(response["files_processed"]) == 11  # 10 files + "..."
        assert response["files_processed"][-1] == "..."
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.GitHubRepoManager')
    async def test_cleanup_on_exception(self, mock_manager_cls):
        """Test that cleanup is called even when an exception occurs."""
        mock_manager = Mock()
        mock_manager_cls.return_value = mock_manager
        mock_manager.clone_repository.side_effect = Exception("Unexpected error")
        
        result = await smart_crawl_github(
            ctx=self.mock_context,
            repo_url="https://github.com/user/repo"
        )
        
        response = json.loads(result)
        assert response["success"] is False
        assert "Unexpected error" in response["error"]
        
        # Verify cleanup was called despite the exception
        mock_manager.cleanup.assert_called_once()


@pytest.mark.integration
class TestSmartCrawlGitHubIntegration:
    """Integration tests for smart_crawl_github with real components."""
    
    def setup_method(self):
        """Setup test method with real components."""
        self.mock_context = Mock()
        self.mock_qdrant_client = Mock()
        self.mock_context.request_context.lifespan_context.qdrant_client = self.mock_qdrant_client
        
        # Setup realistic Qdrant client behavior
        self.mock_qdrant_client.update_source_info.return_value = None
        self.mock_qdrant_client.upsert_points.return_value = None
    
    @pytest.mark.asyncio
    @patch('utils.github_processor.tempfile.mkdtemp')
    @patch('utils.github_processor.subprocess.run')
    @patch('utils.github_processor.os.walk')
    @patch('utils.github_processor.os.path.getsize')
    @patch('utils.github_processor.os.path.exists')
    @patch('builtins.open')
    async def test_end_to_end_workflow(self, mock_open, mock_exists, mock_getsize, 
                                     mock_walk, mock_subprocess, mock_mkdtemp):
        """Test end-to-end workflow with realistic file system simulation."""
        # Setup file system simulation
        mock_mkdtemp.return_value = "/tmp/github_clone_test"
        mock_subprocess.return_value = Mock(returncode=0, stderr="")
        
        # Simulate repository structure
        mock_walk.return_value = [
            ("/tmp/github_clone_test", ["docs"], ["README.md", "setup.py"]),
            ("/tmp/github_clone_test/docs", [], ["guide.md", "api.md"])
        ]
        
        mock_getsize.return_value = 2048  # 2KB files
        mock_exists.return_value = False  # No package files for simplicity
        
        # Setup file contents
        readme_content = """# Test Repository

This is a test repository for demonstration purposes.

## Features
- Feature 1
- Feature 2

## Installation
```bash
pip install test-package
```
"""
        
        guide_content = """# User Guide

This guide explains how to use the test package.

## Quick Start
```python
import test_package
test_package.hello()
```
"""
        
        mock_file_contents = {
            "/tmp/github_clone_test/README.md": readme_content,
            "/tmp/github_clone_test/docs/guide.md": guide_content
        }
        
        def mock_open_func(filename, mode='r', encoding=None, errors=None):
            mock_file = Mock()
            mock_file.read.return_value = mock_file_contents.get(filename, "")
            mock_file.__enter__.return_value = mock_file
            mock_file.__exit__.return_value = None
            return mock_file
        
        mock_open.side_effect = mock_open_func
        
        # Mock additional utilities
        with patch('crawl4ai_mcp.smart_chunk_markdown') as mock_chunk, \
             patch('crawl4ai_mcp.extract_source_summary') as mock_extract_summary, \
             patch('crawl4ai_mcp.update_source_info') as mock_update_source, \
             patch('crawl4ai_mcp.add_documents_to_supabase') as mock_add_docs, \
             patch('utils.github_processor.shutil.rmtree') as mock_rmtree:
            
            # Setup utility mocks
            mock_chunk.side_effect = lambda content, chunk_size: [content[:chunk_size], content[chunk_size:]] if len(content) > chunk_size else [content]
            mock_extract_summary.return_value = "Test repository for demonstration purposes"
            
            result = await smart_crawl_github(
                ctx=self.mock_context,
                repo_url="https://github.com/testuser/test-repo"
            )
            
            response = json.loads(result)
            
            # Verify successful execution
            assert response["success"] is True
            assert response["owner"] == "testuser"
            assert response["repo_name"] == "test-repo"
            assert response["markdown_files_processed"] >= 1
            assert response["chunks_stored"] >= 1
            
            # Verify all components were used
            mock_subprocess.assert_called_once()  # Git clone
            mock_update_source.assert_called_once()  # Source info updated
            mock_add_docs.assert_called_once()  # Documents added
            mock_rmtree.assert_called_once()  # Cleanup performed


================================================
FILE: tests/test_smart_crawl_github_integration.py
================================================
"""
Integration tests for smart_crawl_github multi-file type support.

Tests the complete workflow from repository cloning through multi-file processing
to storage in the vector database.
"""
import pytest
import json
import os
import sys
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, patch

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from crawl4ai_mcp import smart_crawl_github, Context


class TestSmartCrawlGitHubIntegration:
    """Integration tests for smart_crawl_github multi-file functionality."""
    
    @pytest.fixture
    def mock_context(self):
        """Create a mock MCP context."""
        context = Mock(spec=Context)
        return context
    
    @pytest.fixture
    def sample_repo_structure(self):
        """Create a temporary repository with multiple file types."""
        temp_dir = tempfile.mkdtemp(prefix="test_repo_")
        
        # Create directory structure
        os.makedirs(os.path.join(temp_dir, "src"))
        os.makedirs(os.path.join(temp_dir, "docs"))
        os.makedirs(os.path.join(temp_dir, "config"))
        
        # Create README.md
        readme_content = """# Test Repository

This is a test repository for integration testing.

## Features

- Multi-language support
- Configuration management
- Comprehensive documentation

## Getting Started

```bash
npm install
python -m pip install -r requirements.txt
```
"""
        with open(os.path.join(temp_dir, "README.md"), "w") as f:
            f.write(readme_content)
        
        # Create Python file with docstrings
        python_content = '''"""
Test Python module for integration testing.

This module demonstrates docstring extraction capabilities.
"""

def calculate_sum(a: int, b: int) -> int:
    """
    Calculate the sum of two integers.
    
    Args:
        a: First integer
        b: Second integer
        
    Returns:
        The sum of a and b
    """
    return a + b

class DataProcessor:
    """
    A class for processing data with various methods.
    
    This class demonstrates class-level documentation.
    """
    
    def __init__(self, name: str):
        """Initialize the processor with a name."""
        self.name = name
    
    async def process_async(self, data: List[str]) -> Dict[str, Any]:
        """
        Process data asynchronously.
        
        Args:
            data: List of strings to process
            
        Returns:
            Processed data as dictionary
        """
        return {"processed": len(data), "name": self.name}
'''
        with open(os.path.join(temp_dir, "src", "processor.py"), "w") as f:
            f.write(python_content)
        
        # Create TypeScript file with JSDoc
        typescript_content = '''
/**
 * User interface definition with comprehensive documentation.
 * @interface User
 * @public
 */
export interface User {
    id: number;
    name: string;
    email: string;
    active: boolean;
}

/**
 * Service class for managing user operations.
 * @class UserService
 * @public
 */
export class UserService {
    private users: User[] = [];
    
    /**
     * Retrieves a user by their ID.
     * @param id - The unique identifier for the user
     * @returns The user object if found, undefined otherwise
     * @example
     * ```typescript
     * const service = new UserService();
     * const user = service.getUser(123);
     * ```
     */
    getUser(id: number): User | undefined {
        return this.users.find(u => u.id === id);
    }
    
    /**
     * Creates a new user in the system.
     * @param userData - The user data to create
     * @returns Promise resolving to the created user
     * @throws {ValidationError} When user data is invalid
     */
    async createUser(userData: Omit<User, 'id'>): Promise<User> {
        const newUser: User = {
            id: Date.now(),
            ...userData
        };
        this.users.push(newUser);
        return newUser;
    }
}

/**
 * Utility function for validating email addresses.
 * @param email - Email address to validate
 * @returns True if email is valid, false otherwise
 */
export function validateEmail(email: string): boolean {
    const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;
    return emailRegex.test(email);
}
'''
        with open(os.path.join(temp_dir, "src", "user-service.ts"), "w") as f:
            f.write(typescript_content)
        
        # Create package.json
        package_json = {
            "name": "test-integration-repo",
            "version": "1.0.0",
            "description": "Test repository for integration testing",
            "main": "index.js",
            "scripts": {
                "start": "node index.js",
                "test": "jest",
                "build": "tsc"
            },
            "dependencies": {
                "express": "^4.18.0",
                "typescript": "^4.9.0"
            },
            "devDependencies": {
                "jest": "^29.0.0",
                "@types/node": "^18.0.0"
            }
        }
        with open(os.path.join(temp_dir, "package.json"), "w") as f:
            json.dump(package_json, f, indent=2)
        
        # Create docker-compose.yml
        docker_compose = """version: '3.8'
services:
  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
    volumes:
      - ./src:/app/src
    depends_on:
      - database
  
  database:
    image: postgres:13
    environment:
      POSTGRES_DB: testdb
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpass
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
"""
        with open(os.path.join(temp_dir, "config", "docker-compose.yml"), "w") as f:
            f.write(docker_compose)
        
        # Create pyproject.toml
        pyproject_content = """[project]
name = "test-python-project"
version = "0.1.0"
description = "Test Python project for integration testing"
authors = [
    {name = "Test Author", email = "test@example.com"}
]
readme = "README.md"
license = {text = "MIT"}

[project.dependencies]
requests = "^2.28.0"
click = "^8.1.0"
pydantic = "^1.10.0"

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "black>=22.0.0",
    "isort>=5.10.0",
    "mypy>=0.991"
]

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
"""
        with open(os.path.join(temp_dir, "pyproject.toml"), "w") as f:
            f.write(pyproject_content)
        
        # Create API documentation
        api_docs = """# API Documentation

## User Management API

### Endpoints

#### GET /users/{id}
Retrieve a user by ID.

**Parameters:**
- `id` (integer): User ID

**Response:**
```json
{
  "id": 123,
  "name": "John Doe",
  "email": "john@example.com",
  "active": true
}
```

#### POST /users
Create a new user.

**Request Body:**
```json
{
  "name": "Jane Smith",
  "email": "jane@example.com",
  "active": true
}
```
"""
        with open(os.path.join(temp_dir, "docs", "api.md"), "w") as f:
            f.write(api_docs)
        
        yield temp_dir
        
        # Cleanup
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_multi_file_complete_workflow(
        self, 
        mock_extractor_class,
        mock_manager_class, 
        mock_add_documents,
        mock_context,
        sample_repo_structure
    ):
        """Test complete workflow with multiple file types."""
        # Setup mocks
        mock_manager = Mock()
        mock_manager.clone_repository.return_value = sample_repo_structure
        mock_manager_class.return_value = mock_manager
        
        mock_extractor = Mock()
        mock_extractor.extract_repo_metadata.return_value = {
            "repo_url": "https://github.com/test/repo",
            "owner": "test",
            "repo_name": "repo",
            "source_type": "github_repository"
        }
        mock_extractor_class.return_value = mock_extractor
        
        mock_add_documents.return_value = None
        
        # Test with multiple file types
        result = await smart_crawl_github(
            ctx=mock_context,
            repo_url="https://github.com/test/repo",
            max_files=50,
            chunk_size=1000,
            max_size_mb=500,
            file_types_to_index=['.md', '.py', '.ts', '.json', '.yml', '.toml']
        )
        
        # Parse result
        result_data = json.loads(result)
        
        # Verify success
        assert result_data["status"] == "success"
        assert result_data["repo_url"] == "https://github.com/test/repo"
        
        # Verify file type processing
        assert "file_types_processed" in result_data
        file_types = result_data["file_types_processed"]
        
        # Should have processed multiple types
        assert file_types["markdown"] > 0  # README.md, api.md
        assert file_types["python"] > 0   # processor.py
        assert file_types["typescript"] > 0  # user-service.ts
        assert file_types["configuration"] > 0  # package.json, docker-compose.yml, pyproject.toml
        
        # Verify chunks were created
        assert result_data["total_chunks"] > 0
        assert result_data["total_files_processed"] >= 6  # Multiple files processed
        
        # Verify documents were added to storage
        mock_add_documents.assert_called_once()
        added_documents = mock_add_documents.call_args[0][0]
        
        # Check document types and metadata
        doc_types = set()
        languages = set()
        for doc in added_documents:
            metadata = doc["metadata"]
            doc_types.add(metadata["type"])
            languages.add(metadata["language"])
        
        # Should have various document types
        expected_types = {"markdown", "module", "function", "class", "interface", "configuration"}
        assert len(doc_types.intersection(expected_types)) >= 4
        
        # Should have various languages
        expected_languages = {"markdown", "python", "typescript", "json", "yaml", "toml"}
        assert len(languages.intersection(expected_languages)) >= 4
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_backward_compatibility_markdown_only(
        self,
        mock_extractor_class,
        mock_manager_class,
        mock_add_documents,
        mock_context,
        sample_repo_structure
    ):
        """Test backward compatibility with default markdown-only behavior."""
        # Setup mocks
        mock_manager = Mock()
        mock_manager.clone_repository.return_value = sample_repo_structure
        mock_manager_class.return_value = mock_manager
        
        mock_extractor = Mock()
        mock_extractor.extract_repo_metadata.return_value = {
            "repo_url": "https://github.com/test/repo",
            "source_type": "github_repository"
        }
        mock_extractor_class.return_value = mock_extractor
        
        mock_add_documents.return_value = None
        
        # Test with default parameters (should only process markdown)
        result = await smart_crawl_github(
            ctx=mock_context,
            repo_url="https://github.com/test/repo"
            # No file_types_to_index parameter - should default to ['.md']
        )
        
        # Parse result
        result_data = json.loads(result)
        
        # Verify success
        assert result_data["status"] == "success"
        
        # Verify only markdown was processed
        file_types = result_data["file_types_processed"]
        assert file_types["markdown"] > 0  # Should have markdown files
        assert file_types.get("python", 0) == 0  # Should not have Python
        assert file_types.get("typescript", 0) == 0  # Should not have TypeScript
        assert file_types.get("configuration", 0) == 0  # Should not have config
        
        # Verify documents were added
        mock_add_documents.assert_called_once()
        added_documents = mock_add_documents.call_args[0][0]
        
        # All documents should be markdown
        for doc in added_documents:
            assert doc["metadata"]["language"] == "markdown"
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_single_file_type_processing(
        self,
        mock_extractor_class,
        mock_manager_class,
        mock_add_documents,
        mock_context,
        sample_repo_structure
    ):
        """Test processing single specific file type."""
        # Setup mocks
        mock_manager = Mock()
        mock_manager.clone_repository.return_value = sample_repo_structure
        mock_manager_class.return_value = mock_manager
        
        mock_extractor = Mock()
        mock_extractor.extract_repo_metadata.return_value = {
            "repo_url": "https://github.com/test/repo",
            "source_type": "github_repository"
        }
        mock_extractor_class.return_value = mock_extractor
        
        mock_add_documents.return_value = None
        
        # Test with only Python files
        result = await smart_crawl_github(
            ctx=mock_context,
            repo_url="https://github.com/test/repo",
            file_types_to_index=['.py']
        )
        
        result_data = json.loads(result)
        
        # Should only have Python processing
        file_types = result_data["file_types_processed"]
        assert file_types["python"] > 0
        assert file_types.get("markdown", 0) == 0
        assert file_types.get("typescript", 0) == 0
        
        # Verify Python-specific content
        mock_add_documents.assert_called_once()
        added_documents = mock_add_documents.call_args[0][0]
        
        python_docs = [doc for doc in added_documents if doc["metadata"]["language"] == "python"]
        assert len(python_docs) > 0
        
        # Should have various Python elements
        doc_types = {doc["metadata"]["type"] for doc in python_docs}
        assert "module" in doc_types  # Module docstring
        assert "function" in doc_types  # Function docstrings
        assert "class" in doc_types  # Class docstrings
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_error_handling_invalid_repo(
        self,
        mock_extractor_class,
        mock_manager_class,
        mock_context
    ):
        """Test error handling with invalid repository."""
        # Setup mock to raise error
        mock_manager = Mock()
        mock_manager.clone_repository.side_effect = ValueError("Invalid GitHub repository URL")
        mock_manager_class.return_value = mock_manager
        
        # Test with invalid repo URL
        result = await smart_crawl_github(
            ctx=mock_context,
            repo_url="https://invalid-url.com/not/github",
            file_types_to_index=['.md', '.py']
        )
        
        result_data = json.loads(result)
        
        # Should return error status
        assert result_data["status"] == "error"
        assert "Invalid GitHub repository URL" in result_data["error"]
        assert result_data["total_chunks"] == 0
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_mixed_file_processing_with_errors(
        self,
        mock_extractor_class,
        mock_manager_class,
        mock_add_documents,
        mock_context,
        sample_repo_structure
    ):
        """Test processing with mixed valid and problematic files."""
        # Create problematic files in the sample repo
        
        # Add invalid Python file (syntax error)
        invalid_python = '''
def broken_function(
    # Missing closing parenthesis and invalid syntax
    return "this will fail"
'''
        with open(os.path.join(sample_repo_structure, "src", "broken.py"), "w") as f:
            f.write(invalid_python)
        
        # Add very large file (should be skipped)
        large_content = "x" * 2_000_000  # 2MB content
        with open(os.path.join(sample_repo_structure, "src", "large.py"), "w") as f:
            f.write(large_content)
        
        # Add binary-like file
        with open(os.path.join(sample_repo_structure, "binary.json"), "wb") as f:
            f.write(b"{\x00binary\x00content\x00}")
        
        # Setup mocks
        mock_manager = Mock()
        mock_manager.clone_repository.return_value = sample_repo_structure
        mock_manager_class.return_value = mock_manager
        
        mock_extractor = Mock()
        mock_extractor.extract_repo_metadata.return_value = {
            "repo_url": "https://github.com/test/repo",
            "source_type": "github_repository"
        }
        mock_extractor_class.return_value = mock_extractor
        
        mock_add_documents.return_value = None
        
        # Test processing - should continue despite errors
        result = await smart_crawl_github(
            ctx=mock_context,
            repo_url="https://github.com/test/repo",
            file_types_to_index=['.md', '.py', '.ts', '.json']
        )
        
        result_data = json.loads(result)
        
        # Should still succeed overall
        assert result_data["status"] == "success"
        
        # Should have processed valid files
        assert result_data["total_chunks"] > 0
        file_types = result_data["file_types_processed"]
        assert file_types["markdown"] > 0  # Valid markdown files
        assert file_types["python"] > 0   # Valid Python files (excluding broken ones)
        assert file_types["typescript"] > 0  # Valid TypeScript files
        
        # Documents should be added (only valid ones)
        mock_add_documents.assert_called_once()
        added_documents = mock_add_documents.call_args[0][0]
        
        # Should not contain content from problematic files
        all_content = " ".join(doc["content"] for doc in added_documents)
        assert "broken_function" not in all_content  # Syntax error file skipped
        assert len(all_content) < 1_500_000  # Large file not included
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_metadata_structure_validation(
        self,
        mock_extractor_class,
        mock_manager_class,
        mock_add_documents,
        mock_context,
        sample_repo_structure
    ):
        """Test that generated metadata follows expected structure."""
        # Setup mocks
        mock_manager = Mock()
        mock_manager.clone_repository.return_value = sample_repo_structure
        mock_manager_class.return_value = mock_manager
        
        mock_extractor = Mock()
        mock_extractor.extract_repo_metadata.return_value = {
            "repo_url": "https://github.com/test/repo",
            "owner": "test",
            "repo_name": "repo",
            "full_name": "test/repo",
            "source_type": "github_repository"
        }
        mock_extractor_class.return_value = mock_extractor
        
        mock_add_documents.return_value = None
        
        # Test processing
        result = await smart_crawl_github(
            ctx=mock_context,
            repo_url="https://github.com/test/repo",
            file_types_to_index=['.md', '.py', '.ts', '.json', '.yml', '.toml']
        )
        
        result_data = json.loads(result)
        assert result_data["status"] == "success"
        
        # Verify documents were added with correct metadata structure
        mock_add_documents.assert_called_once()
        added_documents = mock_add_documents.call_args[0][0]
        
        # Validate metadata structure for each document
        required_fields = [
            "file_path", "type", "name", "language", "repo_url", "source_type"
        ]
        
        for doc in added_documents:
            assert "content" in doc
            assert "metadata" in doc
            
            metadata = doc["metadata"]
            
            # Check required fields
            for field in required_fields:
                assert field in metadata, f"Missing field {field} in metadata"
            
            # Validate field types and values
            assert isinstance(metadata["file_path"], str)
            assert isinstance(metadata["type"], str)
            assert isinstance(metadata["name"], str)
            assert isinstance(metadata["language"], str)
            assert metadata["repo_url"] == "https://github.com/test/repo"
            assert metadata["source_type"] == "github_repository"
            
            # Language-specific validation
            if metadata["language"] == "python":
                assert metadata["type"] in ["module", "function", "class"]
                if metadata["type"] == "function":
                    assert "signature" in metadata
                    assert "line_number" in metadata
            
            elif metadata["language"] == "typescript":
                assert metadata["type"] in ["function", "class", "interface", "method"]
                if metadata["type"] in ["function", "method"]:
                    assert "signature" in metadata
            
            elif metadata["language"] == "markdown":
                assert metadata["type"] == "markdown"
            
            elif metadata["language"] in ["json", "yaml", "toml"]:
                assert metadata["type"] == "configuration"
    
    @pytest.mark.asyncio
    @patch('crawl4ai_mcp.add_documents_to_supabase')
    @patch('crawl4ai_mcp.GitHubRepoManager')
    @patch('crawl4ai_mcp.GitHubMetadataExtractor')
    async def test_chunking_integration(
        self,
        mock_extractor_class,
        mock_manager_class,
        mock_add_documents,
        mock_context,
        sample_repo_structure
    ):
        """Test that chunking works correctly with multi-file content."""
        # Setup mocks
        mock_manager = Mock()
        mock_manager.clone_repository.return_value = sample_repo_structure
        mock_manager_class.return_value = mock_manager
        
        mock_extractor = Mock()
        mock_extractor.extract_repo_metadata.return_value = {
            "repo_url": "https://github.com/test/repo",
            "source_type": "github_repository"
        }
        mock_extractor_class.return_value = mock_extractor
        
        mock_add_documents.return_value = None
        
        # Test with small chunk size to force chunking
        result = await smart_crawl_github(
            ctx=mock_context,
            repo_url="https://github.com/test/repo",
            chunk_size=500,  # Small chunks
            file_types_to_index=['.md', '.py', '.ts']
        )
        
        result_data = json.loads(result)
        assert result_data["status"] == "success"
        
        # Verify chunking occurred
        mock_add_documents.assert_called_once()
        added_documents = mock_add_documents.call_args[0][0]
        
        # Should have multiple chunks due to small chunk size
        assert len(added_documents) > 3
        
        # Verify chunk metadata
        chunk_counts = {}
        for doc in added_documents:
            metadata = doc["metadata"]
            assert "chunk_index" in metadata
            assert "total_chunks" in metadata
            
            # Track chunks per document type
            doc_key = f"{metadata['file_path']}:{metadata['name']}"
            if doc_key not in chunk_counts:
                chunk_counts[doc_key] = 0
            chunk_counts[doc_key] += 1
        
        # Some documents should have been chunked
        multi_chunk_docs = [count for count in chunk_counts.values() if count > 1]
        assert len(multi_chunk_docs) > 0  # At least some documents were chunked
        
        # Verify chunk ordering
        for doc in added_documents:
            metadata = doc["metadata"]
            assert metadata["chunk_index"] >= 0
            assert metadata["chunk_index"] < metadata["total_chunks"]


if __name__ == "__main__":
    pytest.main([__file__, "-v"])


================================================
FILE: tests/test_utils_integration.py
================================================
"""
Integration tests for utils.py with Qdrant wrapper.

Tests the integration between utils functions and QdrantClientWrapper.
"""
import pytest
import sys
from pathlib import Path
from unittest.mock import Mock, patch

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from utils import (
    get_supabase_client, search_documents, search_code_examples,
    add_documents_to_supabase, add_code_examples_to_supabase,
    create_embedding, create_embeddings_batch,
    extract_code_blocks, update_source_info
)
from embedding_config import get_embedding_dimensions
from qdrant_wrapper import QdrantClientWrapper


class TestUtilsIntegration:
    """Test integration between utils and Qdrant wrapper."""

    @patch('utils.get_qdrant_client')
    def test_get_supabase_client_returns_qdrant(self, mock_get_qdrant):
        """Test that get_supabase_client returns Qdrant client (legacy compatibility)."""
        # Setup
        mock_client = Mock(spec=QdrantClientWrapper)
        mock_get_qdrant.return_value = mock_client
        
        # Test
        client = get_supabase_client()
        
        # Verify
        assert client == mock_client
        mock_get_qdrant.assert_called_once()

    @patch('utils.create_embedding')
    @patch('utils.QdrantClientWrapper')
    def test_search_documents_integration(self, mock_wrapper_class, mock_create_embedding):
        """Test document search integration with Qdrant."""
        # Setup mocks
        mock_client = Mock()
        mock_client.search_documents.return_value = [
            {"id": "doc1", "similarity": 0.9, "content": "test content"}
        ]
        mock_wrapper_class.return_value = mock_client
        
        mock_create_embedding.return_value = [0.1] * get_embedding_dimensions()
        
        # Test
        results = search_documents(mock_client, "test query", match_count=5)
        
        # Verify
        assert len(results) == 1
        assert results[0]["id"] == "doc1"
        mock_create_embedding.assert_called_once_with("test query")
        mock_client.search_documents.assert_called_once()

    @patch('utils.create_embedding')
    @patch('utils.QdrantClientWrapper')
    def test_search_code_examples_integration(self, mock_wrapper_class, mock_create_embedding):
        """Test code examples search integration with Qdrant."""
        # Setup mocks
        mock_client = Mock()
        mock_client.search_code_examples.return_value = [
            {"id": "code1", "similarity": 0.85, "content": "def test(): pass"}
        ]
        mock_wrapper_class.return_value = mock_client
        
        mock_create_embedding.return_value = [0.1] * get_embedding_dimensions()
        
        # Test
        results = search_code_examples(mock_client, "function definition", match_count=3)
        
        # Verify
        assert len(results) == 1
        assert results[0]["id"] == "code1"
        mock_create_embedding.assert_called_once()
        # Enhanced query should be used
        call_args = mock_create_embedding.call_args[0][0]
        assert "Code example for" in call_args

    @patch('utils.create_embeddings_batch')
    @patch('utils.QdrantClientWrapper')
    def test_add_documents_integration(self, mock_wrapper_class, mock_create_embeddings):
        """Test document addition integration with Qdrant."""
        # Setup mocks
        mock_client = Mock()
        mock_client.add_documents_to_qdrant.return_value = [
            [{"id": "doc1", "payload": {"content": "test"}, "content": "test"}]
        ]
        mock_client.upsert_points.return_value = None
        mock_wrapper_class.return_value = mock_client
        
        mock_create_embeddings.return_value = [[0.1] * get_embedding_dimensions()]
        
        # Test data
        urls = ["https://example.com"]
        chunk_numbers = [1]
        contents = ["test content"]
        metadatas = [{"category": "test"}]
        url_to_full_document = {"https://example.com": "full document"}
        
        # Test
        add_documents_to_supabase(
            mock_client, urls, chunk_numbers, contents, metadatas, url_to_full_document
        )
        
        # Verify
        mock_client.add_documents_to_qdrant.assert_called_once()
        mock_create_embeddings.assert_called_once()
        mock_client.upsert_points.assert_called_once()

    @patch('utils.create_embeddings_batch')
    @patch('utils.QdrantClientWrapper')
    def test_add_code_examples_integration(self, mock_wrapper_class, mock_create_embeddings):
        """Test code examples addition integration with Qdrant."""
        # Setup mocks
        mock_client = Mock()
        mock_client.add_code_examples_to_qdrant.return_value = [
            [{"id": "code1", "payload": {"content": "code"}, "combined_text": "code\n\nSummary: test"}]
        ]
        mock_client.upsert_points.return_value = None
        mock_wrapper_class.return_value = mock_client
        
        mock_create_embeddings.return_value = [[0.1] * get_embedding_dimensions()]
        
        # Test data
        urls = ["https://example.com"]
        chunk_numbers = [1]
        code_examples = ["def test(): pass"]
        summaries = ["Test function"]
        metadatas = [{"language": "python"}]
        
        # Test
        add_code_examples_to_supabase(
            mock_client, urls, chunk_numbers, code_examples, summaries, metadatas
        )
        
        # Verify
        mock_client.add_code_examples_to_qdrant.assert_called_once()
        mock_create_embeddings.assert_called_once()
        mock_client.upsert_points.assert_called_once()

    @patch('utils.QdrantClientWrapper')
    def test_update_source_info_integration(self, mock_wrapper_class):
        """Test source info update integration."""
        # Setup mock
        mock_client = Mock()
        mock_client.update_source_info.return_value = None
        mock_wrapper_class.return_value = mock_client
        
        # Test
        update_source_info(mock_client, "example.com", "Test site", 1000)
        
        # Verify
        mock_client.update_source_info.assert_called_once_with("example.com", "Test site", 1000)


class TestEmbeddingFunctions:
    """Test embedding creation functions."""

    @patch('utils.openai.embeddings.create')
    def test_create_embeddings_batch_success(self, mock_openai_create):
        """Test successful batch embedding creation."""
        # Setup mock
        mock_response = Mock()
        mock_response.data = [
            Mock(embedding=[0.1] * get_embedding_dimensions()),
            Mock(embedding=[0.2] * get_embedding_dimensions())
        ]
        mock_openai_create.return_value = mock_response
        
        # Test
        texts = ["text1", "text2"]
        embeddings = create_embeddings_batch(texts)
        
        # Verify
        assert len(embeddings) == 2
        assert len(embeddings[0]) == get_embedding_dimensions()
        assert len(embeddings[1]) == get_embedding_dimensions()
        mock_openai_create.assert_called_once_with(
            model="text-embedding-3-small",
            input=texts
        )

    @patch('utils.time.sleep')  # Mock sleep to speed up test
    @patch('utils.openai.embeddings.create')
    def test_create_embeddings_batch_failure_with_fallback(self, mock_openai_create, mock_sleep):
        """Test batch embedding creation with fallback to individual."""
        # Setup mock to fail on all batch attempts (3 retries), then succeed on individual calls
        mock_openai_create.side_effect = [
            Exception("Batch failed"),  # 1st batch attempt fails
            Exception("Batch failed"),  # 2nd batch attempt fails  
            Exception("Batch failed"),  # 3rd batch attempt fails
            Mock(data=[Mock(embedding=[0.1] * get_embedding_dimensions())]),  # 1st individual call succeeds
            Mock(data=[Mock(embedding=[0.2] * get_embedding_dimensions())])   # 2nd individual call succeeds
        ]
        
        # Test
        texts = ["text1", "text2"]
        embeddings = create_embeddings_batch(texts)
        
        # Verify
        assert len(embeddings) == 2
        assert len(embeddings[0]) == get_embedding_dimensions()
        assert len(embeddings[1]) == get_embedding_dimensions()
        assert mock_openai_create.call_count == 5  # 3 batch attempts + 2 individual
        assert mock_sleep.call_count == 2  # Called for retry delays

    @patch('utils.create_embeddings_batch')
    def test_create_embedding_single(self, mock_batch):
        """Test single embedding creation."""
        # Setup mock
        mock_batch.return_value = [[0.1] * get_embedding_dimensions()]
        
        # Test
        embedding = create_embedding("test text")
        
        # Verify
        assert len(embedding) == get_embedding_dimensions()
        mock_batch.assert_called_once_with(["test text"])

    @patch('utils.create_embeddings_batch')
    def test_create_embedding_failure(self, mock_batch):
        """Test single embedding creation failure."""
        # Setup mock to fail
        mock_batch.side_effect = Exception("Failed")
        
        # Test
        embedding = create_embedding("test text")
        
        # Verify fallback to zero embedding
        assert len(embedding) == get_embedding_dimensions()
        assert all(v == 0.0 for v in embedding)


class TestCodeExtraction:
    """Test code block extraction."""

    def test_extract_code_blocks_basic(self):
        """Test basic code block extraction."""
        markdown = """
        Some text before
        
        ```python
        def hello():
            print("world")
        ```
        
        Some text after
        """
        
        blocks = extract_code_blocks(markdown, min_length=10)
        
        assert len(blocks) == 1
        block = blocks[0]
        assert block["language"] == "python"
        assert 'def hello():' in block["code"]
        assert "Some text before" in block["context_before"]
        assert "Some text after" in block["context_after"]

    def test_extract_code_blocks_no_language(self):
        """Test code block extraction without language specifier."""
        markdown = """
        ```
        function test() {
            console.log("test");
        }
        ```
        """
        
        blocks = extract_code_blocks(markdown, min_length=10)
        
        assert len(blocks) == 1
        assert blocks[0]["language"] == ""
        assert "function test()" in blocks[0]["code"]

    def test_extract_code_blocks_min_length_filter(self):
        """Test that short code blocks are filtered out."""
        markdown = """
        ```python
        x = 1
        ```
        """
        
        blocks = extract_code_blocks(markdown, min_length=1000)
        
        assert len(blocks) == 0  # Block too short


if __name__ == "__main__":
    pytest.main([__file__])


================================================
FILE: .gemini/commands/execute-prp.toml
================================================
description = "Implement a feature using a PRP file"

prompt = """
# Execute BASE PRP

Implement a feature using the PRP file.

## PRP File: {{args}}

## Execution Process

1. **Load PRP**
   - Read the specified PRP file
   - Understand all context and requirements
   - Follow all instructions in the PRP and extend the research if needed
   - Ensure you have all needed context to implement the PRP fully
   - Do more web searches and codebase exploration as needed

2. **ULTRATHINK**
   - Think hard before you execute the plan. Create a comprehensive plan addressing all requirements.
   - Break down complex tasks into smaller, manageable steps.
   - Identify implementation patterns from existing code to follow.

3. **Execute the plan**
   - Execute the PRP
   - Implement all the code

4. **Validate**
   - Run each validation command
   - Fix any failures
   - Re-run until all pass

5. **Complete**
   - Ensure all checklist items done
   - Run final validation suite
   - Report completion status
   - Read the PRP again to ensure you have implemented everything

6. **Reference the PRP**
   - You can always reference the PRP again if needed

Note: If validation fails, use error patterns in PRP to fix and retry.
"""


================================================
FILE: .gemini/commands/generate-prp.toml
================================================
description = "Generate a complete PRP for general feature implementation with thorough research"

prompt = """
# Create PRP

## Feature file: {{args}}

Generate a complete PRP for general feature implementation with thorough research. Ensure context is passed to the AI agent to enable self-validation and iterative refinement. Read the feature file first to understand what needs to be created, how the examples provided help, and any other considerations.

The AI agent only gets the context you are appending to the PRP and training data. Assume the AI agent has access to the codebase and the same knowledge cutoff as you, so it's important that your research findings are included or referenced in the PRP. The Agent has Websearch capabilities, so pass urls to documentation and examples.

## Research Process

1. **Codebase Analysis**
   - Search for similar features/patterns in the codebase
   - Identify files to reference in PRP
   - Note existing conventions to follow
   - Check test patterns for validation approach

2. **External Research**
   - Search for similar features/patterns online
   - Library documentation (include specific URLs)
   - Implementation examples (GitHub/StackOverflow/blogs)
   - Best practices and common pitfalls

3. **User Clarification** (if needed)
   - Specific patterns to mirror and where to find them?
   - Integration requirements and where to find them?

## PRP Generation

Using PRPs/templates/prp_base.md as template:

### Critical Context to Include and pass to the AI agent as part of the PRP
- **Documentation**: URLs with specific sections
- **Code Examples**: Real snippets from codebase
- **Gotchas**: Library quirks, version issues
- **Patterns**: Existing approaches to follow

### Implementation Blueprint
- Start with pseudocode showing approach
- Reference real files for patterns
- Include error handling strategy
- List tasks to be completed to fulfill the PRP in the order they should be completed

### Validation Gates (Must be Executable) eg for python
```bash
# Syntax/Style
ruff check --fix && mypy .

# Unit Tests
uv run pytest tests/ -v
```

*** CRITICAL AFTER YOU ARE DONE RESEARCHING AND EXPLORING THE CODEBASE BEFORE YOU START WRITING THE PRP ***

*** ULTRATHINK ABOUT THE PRP AND PLAN YOUR APPROACH THEN START WRITING THE PRP ***

## Output
Save as: `PRPs/{feature-name}.md`

## Quality Checklist
- [ ] All necessary context included
- [ ] Validation gates are executable by AI
- [ ] References existing patterns
- [ ] Clear implementation path
- [ ] Error handling documented

Score the PRP on a scale of 1-10 (confidence level to succeed in one-pass implementation using gemini cli)

Remember: The goal is one-pass implementation success through comprehensive context.
"""

